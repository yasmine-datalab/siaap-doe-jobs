# Default values for siaap-helm.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 1

image:
  repository: nginx
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: ""

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Automatically mount a ServiceAccount's API credentials?
  automount: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

podAnnotations: {}
podLabels: {}

podSecurityContext: {}
  # fsGroup: 2000

securityContext: {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

service:
  type: ClusterIP
  port: 80

ingress:
  enabled: false
  className: ""
  annotations: {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
  hosts:
    - host: chart-example.local
      paths:
        - path: /
          pathType: ImplementationSpecific
  tls: []
  #  - secretName: chart-example-tls
  #    hosts:
  #      - chart-example.local

resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

livenessProbe:
  httpGet:
    path: /
    port: http
readinessProbe:
  httpGet:
    path: /
    port: http

autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 100
  targetCPUUtilizationPercentage: 80
  # targetMemoryUtilizationPercentage: 80

# Additional volumes on the output Deployment definition.
volumes: []
# - name: foo
#   secret:
#     secretName: mysecret
#     optional: false

# Additional volumeMounts on the output Deployment definition.
volumeMounts: []
# - name: foo
#   mountPath: "/etc/foo"
#   readOnly: true

nodeSelector: {}

tolerations: []

affinity: {}

minio:
  ## Provide a name in place of minio for `app:` labels
  ##
  nameOverride: ""

  ## Provide a name to substitute for the full names of resources
  ##
  fullnameOverride: ""

  ## set kubernetes cluster domain where minio is running
  ##
  clusterDomain: cluster.local

  ## Set default image, imageTag, and imagePullPolicy. mode is used to indicate the
  ##
  image:
    repository: quay.io/minio/minio
    tag: RELEASE.2024-04-18T19-09-19Z
    pullPolicy: IfNotPresent

  imagePullSecrets: []
  # - name: "image-pull-secret"

  ## Set default image, imageTag, and imagePullPolicy for the `mc` (the minio
  ## client used to create a default bucket).
  ##
  mcImage:
    repository: quay.io/minio/mc
    tag: RELEASE.2024-04-18T16-45-29Z
    pullPolicy: IfNotPresent

  ## minio mode, i.e. standalone or distributed
  mode: distributed ## other supported values are "standalone"

  ## Additional labels to include with deployment or statefulset
  additionalLabels: {}

  ## Additional annotations to include with deployment or statefulset
  additionalAnnotations: {}

  ## Typically the deployment/statefulset includes checksums of secrets/config,
  ## So that when these change on a subsequent helm install, the deployment/statefulset
  ## is restarted. This can result in unnecessary restarts under GitOps tooling such as
  ## flux, so set to "true" to disable this behaviour.
  ignoreChartChecksums: false

  ## Additional arguments to pass to minio binary
  extraArgs: []
  # example for enabling FTP:
  #   - --ftp=\"address=:8021\"
  #   - --ftp=\"passive-port-range=10000-10010\"

  ## Additional volumes to minio container
  extraVolumes: []

  ## Additional volumeMounts to minio container
  extraVolumeMounts: []

  ## Additional sidecar containers
  extraContainers: []

  ## Internal port number for MinIO S3 API container
  ## Change service.port to change external port number
  minioAPIPort: "9000"

  ## Internal port number for MinIO Browser Console container
  ## Change consoleService.port to change external port number
  minioConsolePort: "9001"

  ## Update strategy for Deployments
  deploymentUpdate:
    type: RollingUpdate
    maxUnavailable: 0
    maxSurge: 100%

  ## Update strategy for StatefulSets
  statefulSetUpdate:
    updateStrategy: RollingUpdate

  ## Pod priority settings
  ## ref: https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/
  ##
  priorityClassName: ""

  ## Pod runtime class name
  ## ref https://kubernetes.io/docs/concepts/containers/runtime-class/
  ##
  runtimeClassName: ""

  ## Set default rootUser, rootPassword
  ## rootUser and rootPassword is generated when not set
  ## Distributed MinIO ref: https://min.io/docs/minio/linux/operations/install-deploy-manage/deploy-minio-multi-node-multi-drive.html
  ##
  rootUser: "rootUser"
  rootPassword: "rootPassword"

  ## Use existing Secret that store following variables:
  ##
  ## | Chart var             | .data.<key> in Secret    |
  ## |:----------------------|:-------------------------|
  ## | rootUser              | rootUser                 |
  ## | rootPassword          | rootPassword             |
  ##
  ## All mentioned variables will be ignored in values file.
  ## .data.rootUser and .data.rootPassword are mandatory,
  ## others depend on enabled status of corresponding sections.
  existingSecret: ""

  ## Directory on the MinIO pof
  certsPath: "/etc/minio/certs/"
  configPathmc: "/etc/minio/mc/"

  ## Path where PV would be mounted on the MinIO Pod
  mountPath: "/export"
  ## Override the root directory which the minio server should serve from.
  ## If left empty, it defaults to the value of {{ .Values.mountPath }}
  ## If defined, it must be a sub-directory of the path specified in {{ .Values.mountPath }}
  ##
  bucketRoot: ""

  # Number of drives attached to a node
  drivesPerNode: 1
  # Number of MinIO containers running
  replicas: 3
  # Number of expanded MinIO clusters
  pools: 1

  ## TLS Settings for MinIO
  tls:
    enabled: false
    ## Create a secret with private.key and public.crt files and pass that here. Ref: https://github.com/minio/minio/tree/master/docs/tls/kubernetes#2-create-kubernetes-secret
    certSecret: ""
    publicCrt: public.crt
    privateKey: private.key

  ## Trusted Certificates Settings for MinIO. Ref: https://min.io/docs/minio/linux/operations/network-encryption.html#third-party-certificate-authorities
  ## Bundle multiple trusted certificates into one secret and pass that here. Ref: https://github.com/minio/minio/tree/master/docs/tls/kubernetes#2-create-kubernetes-secret
  ## When using self-signed certificates, remember to include MinIO's own certificate in the bundle with key public.crt.
  ## If certSecret is left empty and tls is enabled, this chart installs the public certificate from .Values.tls.certSecret.
  trustedCertsSecret: ""

  ## Enable persistence using Persistent Volume Claims
  ## ref: http://kubernetes.io/docs/user-guide/persistent-volumes/
  ##
  persistence:
    enabled: true
    annotations: {}

    ## A manually managed Persistent Volume and Claim
    ## Requires persistence.enabled: true
    ## If defined, PVC must be created manually before volume will be bound
    existingClaim: ""

    ## minio data Persistent Volume Storage Class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    ## Storage class of PV to bind. By default it looks for standard storage class.
    ## If the PV uses a different storage class, specify that here.
    storageClass: csi-cinder-high-speed
    volumeName: ""
    accessMode: ReadWriteOnce
    size: 100Gi

    ## If subPath is set mount a sub folder of a volume instead of the root of the volume.
    ## This is especially handy for volume plugins that don't natively support sub mounting (like glusterfs).
    ##
    subPath: ""

  ## Expose the MinIO service to be accessed from outside the cluster (LoadBalancer service).
  ## or access it from within the cluster (ClusterIP service). Set the service type and the port to serve it.
  ## ref: http://kubernetes.io/docs/user-guide/services/
  ##
  service:
    type: ClusterIP
    clusterIP: ~
    port: "9000"
    nodePort: 32000
    loadBalancerIP: ~
    externalIPs: []
    annotations: {}

    ## service.loadBalancerSourceRanges Addresses that are allowed when service is LoadBalancer
    ## https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service
    ##
    #loadBalancerSourceRanges:
    #   - 10.10.10.0/24
    loadBalancerSourceRanges: []

    ## service.externalTrafficPolicy minio service external traffic policy
    ## ref http://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip
    ##
    externalTrafficPolicy: Cluster

  ## Configure Ingress based on the documentation here: https://kubernetes.io/docs/concepts/services-networking/ingress/
  ##

  ingress:
    enabled: false
    ingressClassName: ~
    labels: {}
      # node-role.kubernetes.io/ingress: platform
    annotations: {}
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
      # kubernetes.io/ingress.allow-http: "false"
      # kubernetes.io/ingress.global-static-ip-name: ""
      # nginx.ingress.kubernetes.io/secure-backends: "true"
      # nginx.ingress.kubernetes.io/backend-protocol: "HTTPS"
      # nginx.ingress.kubernetes.io/whitelist-source-range: 0.0.0.0/0
    path: /
    hosts:
      - minio-example.local
    tls: []
    #  - secretName: chart-example-tls
    #    hosts:
    #      - chart-example.local

  consoleService:
    type: ClusterIP
    clusterIP: ~
    port: "9001"
    nodePort: 32001
    loadBalancerIP: ~
    externalIPs: []
    annotations: {}
    ## consoleService.loadBalancerSourceRanges Addresses that are allowed when service is LoadBalancer
    ## https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service
    ##
    #loadBalancerSourceRanges:
    #   - 10.10.10.0/24
    loadBalancerSourceRanges: []

    ## servconsoleServiceice.externalTrafficPolicy minio service external traffic policy
    ## ref http://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip
    ##
    externalTrafficPolicy: Cluster

  consoleIngress:
    enabled: false
    ingressClassName: ~
    labels: {}
      # node-role.kubernetes.io/ingress: platform
    annotations: {}
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
      # kubernetes.io/ingress.allow-http: "false"
      # kubernetes.io/ingress.global-static-ip-name: ""
      # nginx.ingress.kubernetes.io/secure-backends: "true"
      # nginx.ingress.kubernetes.io/backend-protocol: "HTTPS"
      # nginx.ingress.kubernetes.io/whitelist-source-range: 0.0.0.0/0
    path: /
    hosts:
      - console.minio-example.local
    tls: []
    #  - secretName: chart-example-tls
    #    hosts:
    #      - chart-example.local

  ## Node labels for pod assignment
  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/
  ##
  nodeSelector: {}
  tolerations: []
  affinity: {}
  topologySpreadConstraints: []

  ## Add stateful containers to have security context, if enabled MinIO will run as this
  ## user and group NOTE: securityContext is only enabled if persistence.enabled=true
  securityContext:
    enabled: true
    runAsUser: 1000
    runAsGroup: 1000
    fsGroup: 1000
    fsGroupChangePolicy: "OnRootMismatch"
    readOnlyRootFilesystem: false

  # Additational pod annotations
  podAnnotations: {}

  # Additional pod labels
  podLabels: {}

  ## Configure resource requests and limits
  ## ref: http://kubernetes.io/docs/user-guide/compute-resources/
  ##
  resources:
    requests:
      memory: 3Gi

  ## List of policies to be created after minio install
  ##
  ## In addition to default policies [readonly|readwrite|writeonly|consoleAdmin|diagnostics]
  ## you can define additional policies with custom supported actions and resources
  policies: []
  ## writeexamplepolicy policy grants creation or deletion of buckets with name
  ## starting with example. In addition, grants objects write permissions on buckets starting with
  ## example.
  # - name: writeexamplepolicy
  #   statements:
  #     - effect: Allow  # this is the default
  #       resources:
  #         - 'arn:aws:s3:::example*/*'
  #       actions:
  #         - "s3:AbortMultipartUpload"
  #         - "s3:GetObject"
  #         - "s3:DeleteObject"
  #         - "s3:PutObject"
  #         - "s3:ListMultipartUploadParts"
  #     - resources:
  #         - 'arn:aws:s3:::example*'
  #       actions:
  #         - "s3:CreateBucket"
  #         - "s3:DeleteBucket"
  #         - "s3:GetBucketLocation"
  #         - "s3:ListBucket"
  #         - "s3:ListBucketMultipartUploads"
  ## readonlyexamplepolicy policy grants access to buckets with name starting with example.
  ## In addition, grants objects read permissions on buckets starting with example.
  # - name: readonlyexamplepolicy
  #   statements:
  #     - resources:
  #         - 'arn:aws:s3:::example*/*'
  #       actions:
  #         - "s3:GetObject"
  #     - resources:
  #         - 'arn:aws:s3:::example*'
  #       actions:
  #         - "s3:GetBucketLocation"
  #         - "s3:ListBucket"
  #         - "s3:ListBucketMultipartUploads"
  ## conditionsexample policy creates all access to example bucket with aws:username="johndoe" and source ip range 10.0.0.0/8 and 192.168.0.0/24 only
  # - name: conditionsexample
  #   statements:
  #     - resources:
  #       - 'arn:aws:s3:::example/*'
  #       actions:
  #       - 's3:*'
  #       conditions:
  #         - StringEquals: '"aws:username": "johndoe"'
  #         - IpAddress: |
  #             "aws:SourceIp": [
  #               "10.0.0.0/8",
  #               "192.168.0.0/24"
  #             ]
  #
  ## Additional Annotations for the Kubernetes Job makePolicyJob
  makePolicyJob:
    securityContext:
      enabled: false
      runAsUser: 1000
      runAsGroup: 1000
    resources:
      requests:
        memory: 128Mi
    # Command to run after the main command on exit
    exitCommand: ""

  ## List of users to be created after minio install
  ##
  users: 
    ## Username, password and policy to be assigned to the user
    ## Default policies are [readonly|readwrite|writeonly|consoleAdmin|diagnostics]
    ## Add new policies as explained here https://min.io/docs/minio/kubernetes/upstream/administration/identity-access-management.html#access-management
    ## NOTE: this will fail if LDAP is enabled in your MinIO deployment
    ## make sure to disable this if you are using LDAP.
    - accessKey: rootaccesskay
      secretKey: rootsecretkey
      policy: consoleAdmin
    # Or you can refer to specific secret
    #- accessKey: externalSecret
    #  existingSecret: my-secret
    #  existingSecretKey: password
    #  policy: readonly

  ## Additional Annotations for the Kubernetes Job makeUserJob
  makeUserJob:
    securityContext:
      enabled: false
      runAsUser: 1000
      runAsGroup: 1000
    resources:
      requests:
        memory: 128Mi
    # Command to run after the main command on exit
    exitCommand: ""

  ## List of service accounts to be created after minio install
  ##
  svcaccts: []
    ## accessKey, secretKey and parent user to be assigned to the service accounts
    ## Add new service accounts as explained here https://min.io/docs/minio/kubernetes/upstream/administration/identity-access-management/minio-user-management.html#service-accounts
    # - accessKey: console-svcacct
    #   secretKey: console123
    #   user: console
    ## Or you can refer to specific secret
    # - accessKey: externalSecret
    #   existingSecret: my-secret
    #   existingSecretKey: password
    #   user: console
    ## You also can pass custom policy
    # - accessKey: console-svcacct
    #   secretKey: console123
    #   user: console
    #   policy:
    #     statements:
    #       - resources:
    #           - 'arn:aws:s3:::example*/*'
    #         actions:
    #           - "s3:AbortMultipartUpload"
    #           - "s3:GetObject"
    #           - "s3:DeleteObject"
    #           - "s3:PutObject"
    #           - "s3:ListMultipartUploadParts"

  makeServiceAccountJob:
    securityContext:
      enabled: false
      runAsUser: 1000
      runAsGroup: 1000
    resources:
      requests:
        memory: 128Mi
    # Command to run after the main command on exit
    exitCommand: ""

  ## List of buckets to be created after minio install
  ##
  buckets: []
    #   # Name of the bucket
    # - name: bucket1
    #   # Policy to be set on the
    #   # bucket [none|download|upload|public]
    #   policy: none
    #   # Purge if bucket exists already
    #   purge: false
    #   # set versioning for
    #   # bucket [true|false]
    #   versioning: false # remove this key if you do not want versioning feature
    #   # set objectlocking for
    #   # bucket [true|false] NOTE: versioning is enabled by default if you use locking
    #   objectlocking: false
    # - name: bucket2
    #   policy: none
    #   purge: false
    #   versioning: true
    #   # set objectlocking for
    #   # bucket [true|false] NOTE: versioning is enabled by default if you use locking
    #   objectlocking: false

  ## Additional Annotations for the Kubernetes Job makeBucketJob
  makeBucketJob:
    securityContext:
      enabled: false
      runAsUser: 1000
      runAsGroup: 1000
    resources:
      requests:
        memory: 128Mi
    # Command to run after the main command on exit
    exitCommand: ""

  ## List of command to run after minio install
  ## NOTE: the mc command TARGET is always "myminio"
  customCommands:
    # - command: "admin policy attach myminio consoleAdmin --group='cn=ops,cn=groups,dc=example,dc=com'"

  ## Additional Annotations for the Kubernetes Job customCommandJob
  customCommandJob:
    securityContext:
      enabled: false
      runAsUser: 1000
      runAsGroup: 1000
    resources:
      requests:
        memory: 128Mi
    # Command to run after the main command on exit
    exitCommand: ""

  ## Merge jobs
  postJob:
    podAnnotations: {}
    annotations: {}
    securityContext:
      enabled: false
      runAsUser: 1000
      runAsGroup: 1000
      fsGroup: 1000
    nodeSelector: {}
    tolerations: []
    affinity: {}

  ## Use this field to add environment variables relevant to MinIO server. These fields will be passed on to MinIO container(s)
  ## when Chart is deployed
  environment:
    ## Please refer for comprehensive list https://min.io/docs/minio/linux/reference/minio-server/minio-server.html
    ## MINIO_SUBNET_LICENSE: "License key obtained from https://subnet.min.io"
    ## MINIO_BROWSER: "off"

  ## The name of a secret in the same kubernetes namespace which contain secret values
  ## This can be useful for LDAP password, etc
  ## The key in the secret must be 'config.env'
  ##
  extraSecret: ~

  ## OpenID Identity Management
  ## The following section documents environment variables for enabling external identity management using an OpenID Connect (OIDC)-compatible provider.
  ## See https://min.io/docs/minio/linux/operations/external-iam/configure-openid-external-identity-management.html for a tutorial on using these variables.
  oidc:
    enabled: false
    configUrl: "https://identity-provider-url/.well-known/openid-configuration"
    clientId: "minio"
    clientSecret: ""
    # Provide existing client secret from the Kubernetes Secret resource, existing secret will have priority over `clientId` and/or `clientSecret``
    existingClientSecretName: ""
    existingClientIdKey: ""
    existingClientSecretKey: ""
    claimName: "policy"
    scopes: "openid,profile,email"
    redirectUri: "https://console-endpoint-url/oauth_callback"
    # Can leave empty
    claimPrefix: ""
    comment: ""
    displayName: ""

  networkPolicy:
    enabled: false
    # Specifies whether the policies created will be standard Network Policies (flavor: kubernetes)
    # or Cilium Network Policies (flavor: cilium)
    flavor: kubernetes
    allowExternal: true
    # only when using flavor: cilium
    egressEntities:
    - kube-apiserver

  ## PodDisruptionBudget settings
  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/disruptions/
  ##
  podDisruptionBudget:
    enabled: false
    maxUnavailable: 1

  ## Specify the service account to use for the MinIO pods. If 'create' is set to 'false'
  ## and 'name' is left unspecified, the account 'default' will be used.
  serviceAccount:
    create: true
    ## The name of the service account to use. If 'create' is 'true', a service account with that name
    ## will be created.
    name: "minio-sa"

  metrics:
    serviceMonitor:
      enabled: false
      # scrape each node/pod individually for additional metrics
      includeNode: false
      public: true
      additionalLabels: {}
      annotations: {}
      # for node metrics
      relabelConfigs: {}
      # for cluster metrics
      relabelConfigsCluster: {}
        # metricRelabelings:
        #   - regex: (server|pod)
        #     action: labeldrop
      namespace: ~
      # Scrape interval, for example `interval: 30s`
      interval: ~
      # Scrape timeout, for example `scrapeTimeout: 10s`
      scrapeTimeout: ~

  ## ETCD settings: https://github.com/minio/minio/blob/master/docs/sts/etcd.md
  ## Define endpoints to enable this section.
  etcd:
    endpoints: []
    pathPrefix: ""
    corednsPathPrefix: ""
    clientCert: ""
    clientCertKey: ""

redis:
  name: redis-stack-server
  redis_stack_server:
    image: "redis/redis-stack-server"
    tag: "7.2.0-v11"
    port: 6379
    replicas: 1
    storage_class: csi-cinder-high-speed
    storage: 100Gi
    affinity: {}

neo4j:
  enabled: true
  nameOverride: "neo4j"
  neo4j:
    # Name of your cluster
    name: "neo4j"
    # If password is not set or empty a random password will be generated during installation.
    # Ignored if `neo4j.passwordFromSecret` is provided
    password: "my-admin-neo"

    # Existing secret to use for initial database password
    passwordFromSecret: ""

    # Neo4j Edition to use (community|enterprise)
    edition: "community"

    # Minimum number of machines initially required to form a clustered database. The StatefulSet will not reach the ready state
    # until at least this many members have discovered each other. The default is 1 (standalone)
  #  minimumClusterSize: 1
  #

    # (Clustering only feature)
    # Neo4j operations allows you to enable servers (part of cluster) which are added outside the minimumClusterSize
    # When the enableServer flag is set to true , an operations pod is created which performs the following functions
      # fetch neo4j creds from the k8s secret (provided by user or created via helm chart)
      # Use the cluster ip created as part of the respective release to connect to Neo4j via Go Driver
      # Execute the ENABLE SERVER query and enable the server
    # The operations pod ends successfully if the server is enabled, or it was already enabled
    operations:
        enableServer: false
        image: "neo4j/helm-charts-operations:5.21.2"
        # protocol can be "neo4j or "neo4j+ssc" or "neo4j+s". Default set to neo4j
        # Note: Do not specify bolt protocol here...it will FAIL.
        protocol: "neo4j"
        labels: {}

    # set edition: "enterprise" to use Neo4j Enterprise Edition
    #
    # To use Neo4j Enterprise Edition you must have a Neo4j license agreement.
    #
    # More information is also available at: https://neo4j.com/licensing/
    # Email inquiries can be directed to: licensing@neo4j.com
    #
    # Set acceptLicenseAgreement: "yes" to confirm that you have a Neo4j license agreement.
    acceptLicenseAgreement: "no"
    #
    # set offlineMaintenanceModeEnabled: true to restart the StatefulSet without the Neo4j process running
    # this can be used to perform tasks that cannot be performed when Neo4j is running such as `neo4j-admin dump`
    offlineMaintenanceModeEnabled: false
    #
    # set resources for the Neo4j Container. The values set will be used for both "requests" and "limit".
    resources:
      cpu: "1"
      memory: "2Gi"

    #add labels if required
    labels:

  # Volumes for Neo4j
  volumes:
    data:

      #labels for data pvc on creation (Valid only when mode set to selector | defaultStorageClass | dynamic | volumeClaimTemplate)
      labels: {}

      # Set it to true when you do not want to use the subPathExpr
      disableSubPathExpr: false

      # REQUIRED: specify a volume mode to use for data
      # Valid values are share|selector|defaultStorageClass|volume|volumeClaimTemplate|dynamic
      # To get up-and-running quickly, for development or testing, use "defaultStorageClass" for a dynamically provisioned volume of the default storage class.
      mode: "dynamic"

      # Only used if mode is set to "selector"
      # Will attach to existing volumes that match the selector
      selector:
        storageClassName: "manual"
        accessModes:
          - ReadWriteOnce
        requests:
          storage: 100Gi
        # A helm template to generate a label selector to match existing volumes n.b. both storageClassName and label selector must match existing volumes
        selectorTemplate:
          matchLabels:
            app: "{{ .Values.neo4j.name }}"
            helm.neo4j.com/volume-role: "data"

      # Only used if mode is set to "defaultStorageClass"
      # Dynamic provisioning using the default storageClass
      defaultStorageClass:
        accessModes:
          - ReadWriteOnce
        requests:
          storage: 10Gi

      # Only used if mode is set to "dynamic"
      # Dynamic provisioning using the provided storageClass
      dynamic:
        storageClassName: "csi-cinder-high-speed"
        accessModes:
          - ReadWriteOnce
        requests:
          storage: 100Gi

      # Only used if mode is set to "volume"
      # Provide an explicit volume to use
      volume:
        # If set an init container (running as root) will be added that runs:
        #   `chown -R <securityContext.fsUser>:<securityContext.fsGroup>` AND `chmod -R g+rwx`
        # on the volume. This is useful for some filesystems (e.g. NFS) where Kubernetes fsUser or fsGroup settings are not respected
        setOwnerAndGroupWritableFilePermissions: false

        # Example (using a specific Persistent Volume Claim)
        # persistentVolumeClaim:
        #   claimName: my-neo4j-pvc

      # Only used if mode is set to "volumeClaimTemplate"
      # Provide an explicit volumeClaimTemplate to use
      volumeClaimTemplate: {}

    # provide a volume to use for backups
    # n.b. backups will be written to /backups on the volume
    # any of the volume modes shown above for data can be used for backups
    backups:
      #labels for backups pvc on creation (Valid only when mode set to selector | defaultStorageClass | dynamic | volumeClaimTemplate)
      labels: {}
      disableSubPathExpr: false
      mode: "share" # share an existing volume (e.g. the data volume)
      share:
        name: "data"

    # provide a volume to use for logs
    # n.b. logs will be written to /logs/$(POD_NAME) on the volume
    # any of the volume modes shown above for data can be used for logs
    logs:
      #labels for logs pvc on creation (Valid only when mode set to selector | defaultStorageClass | dynamic | volumeClaimTemplate)
      labels: {}
      disableSubPathExpr: false
      mode: "share" # share an existing volume (e.g. the data volume)
      share:
        name: "data"

    # provide a volume to use for csv metrics (csv metrics are only available in Neo4j Enterprise Edition)
    # n.b. metrics will be written to /metrics/$(POD_NAME) on the volume
    # any of the volume modes shown above for data can be used for metrics
    metrics:
      #labels for metrics pvc on creation (Valid only when mode set to selector | defaultStorageClass | dynamic | volumeClaimTemplate)
      labels: {}
      disableSubPathExpr: false
      mode: "share" # share an existing volume (e.g. the data volume)
      share:
        name: "data"

    # provide a volume to use for import storage
    # n.b. import will be mounted to /import on the underlying volume
    # any of the volume modes shown above for data can be used for import
    import:
      #labels for import pvc on creation (Valid only when mode set to selector | defaultStorageClass | dynamic | volumeClaimTemplate)
      labels: {}
      disableSubPathExpr: false
      mode: "share" # share an existing volume (e.g. the data volume)
      share:
        name: "data"

    # provide a volume to use for licenses
    # n.b. licenses will be mounted to /licenses on the underlying volume
    # any of the volume modes shown above for data can be used for licenses
    licenses:
      #labels for licenses pvc on creation (Valid only when mode set to selector | defaultStorageClass | dynamic | volumeClaimTemplate)
      labels: {}
      disableSubPathExpr: false
      mode: "share" # share an existing volume (e.g. the data volume)
      share:
        name: "data"

  # add additional volumes and their respective mounts
  additionalVolumes: []
  #  - name: neo4j1-conf
  #    emptyDir: {}
  additionalVolumeMounts: []
  #  - mountPath: "/config/neo4j1.conf"
  #    name: neo4j1-conf

  # ldapPasswordFromSecret defines the secret which holds the password for ldap system account
  # Secret key name must be LDAP_PASS
  # This secret is accessible by Neo4j at the path defined in ldapPasswordMountPath
  ldapPasswordFromSecret: ""

  # The above secret gets mounted to the path mentioned here
  ldapPasswordMountPath: ""

  # nodeSelector labels
  # please ensure the respective labels are present on one of the cluster nodes or else helm charts will throw an error
  nodeSelector: {}
  #  label1: "value1"
  #  label2: "value2"

  # Services for Neo4j
  services:
    # A ClusterIP service with the same name as the Helm Release name should be used for Neo4j Driver connections originating inside the
    # Kubernetes cluster.
    default:
      # Annotations for the K8s Service object
      annotations: { }

    # A LoadBalancer Service for external Neo4j driver applications and Neo4j Browser
    neo4j:
      enabled: false

      # Annotations for the K8s Service object
      annotations: {}

      spec:
        # Type of service.
        type: LoadBalancer

        # in most cloud environments LoadBalancer type will receive an ephemeral public IP address automatically. If you need to specify a static ip here use:
        # loadBalancerIP: ...

      # ports to include in neo4j service
      ports:
        http:
          enabled: true # Set this to false to remove HTTP from this service (this does not affect whether http is enabled for the neo4j process)
          # uncomment to publish http on port 80 (neo4j default is 7474)
          #port: 80
          #targetPort: 7474
          #name: http
          #nodePort: <your-nodeport>, enabled only when type set to NodePort
        https:
          enabled: true # Set this to false to remove HTTPS from this service (this does not affect whether https is enabled for the neo4j process)
          # uncomment to publish http on port 443 (neo4j default is 7473)
          #port: 443
          #targetPort: 7473
          #name: https
          #nodePort: <your-nodeport>, enabled only when type set to NodePort
        bolt:
          enabled: true # Set this to false to remove BOLT from this service (this does not affect whether https is enabled for the neo4j process)
          # Uncomment to explicitly specify the port to publish Neo4j Bolt (7687 is the default)
          #port: 7687
          #targetPort: 7687
          #name: tcp-bolt
          #nodePort: <your-nodeport>, enabled only when type set to NodePort
        backup:
          enabled: false # Set this to true to expose backup port externally (n.b. this could have security implications. Backup is not authenticated by default)
          # Uncomment to explicitly specify the port to publish Neo4j Backup (6362 is the default)
          #port: 6362
          #targetPort: 6362
          #name: tcp-backup
          #nodePort: <your-nodeport>, enabled only when type set to NodePort

      selector:
        "helm.neo4j.com/neo4j.loadbalancer": "include"
        # By default the load balancer will match all Neo4j instance types.
        # When Neo4j drivers connect from outside K8s using the load balancer they will not fetch a routing table.
        # In this case drivers can only use instances included in the load balancer.
        # To only include Neo4j Core instances uncomment the setting below.
        # To only route to Neo4j Read Replicas uncomment the setting and change the value to "READ_REPLICA"
        # "helm.neo4j.com/clustering": "false"

      # This flag allows you to open internal neo4j ports necessary in multi zone /region neo4j cluster scenario
      multiCluster: false

      # The neo4j LoadBalancer service is shared between all servers in the cluster. Because of this, the `helm.sh/resource-policy: keep`
      # annotation is used to avoid helm ownership conflicts when another release attempts to update the service.
      # To prevent the service being orphaned when uninstalling a release, a pre-delete helm hook is provided by the template `delete-loadbalancer-hook.yaml`
      # This is enabled by default, and will create a Job, Service Account, Role and Role Binding that will run a kubectl image and delete the service
      # If enabled: is set to false, the LoadBalancer will be orphaned and will have to manually deleted post uninstall and the hook job will not be created
      cleanup:
        enabled: true
        image:
          registry: docker.io
          repository: bitnami/kubectl
          # Will default to use the Kubernetes server version where the chart is deployed, eg 1.22
          tag: ""
          digest: ""
          imagePullPolicy: IfNotPresent

    # A service for admin/ops tasks including taking backups
    # This service is available even if the deployment is not "ready"
    admin:
      enabled: true
      # Annotations for the admin service
      annotations: { }
      spec:
        type: ClusterIP
      # n.b. there is no ports object for this service. Ports are autogenerated based on the neo4j configuration

    # A ClusterIP service for admin/ops and Neo4j cluster-internal communications
    # This is no longer a headless service as headless service have been seen to introduce latency whenever a cluster member restarts
    # This service is available even if the deployment is not "ready"
    internals:
      enabled: false

      # Annotations for the internals service
      annotations: { }
      spec:
        type: ClusterIP
      # n.b. there is no ports object for this service. Ports are autogenerated based on the neo4j configuration


  # Neo4j Configuration (yaml format)
  config:
    server.config.strict_validation.enabled: "false"
    #dbms.cluster.minimum_initial_system_primaries_count: "3"
    # The amount of memory to use for mapping the store files.
    # The default page cache memory assumes the machine is dedicated to running
    # Neo4j, and is heuristically set to 50% of RAM minus the Java heap size.
    # server.memory.pagecache.size: "74m"

    #The number of Cypher query execution plans that are cached.
    # server.db.query_cache_size: "10"

    # Java Heap Size: by default the Java heap size is dynamically calculated based
    # on available system resources. Uncomment these lines to set specific initial
    # and maximum heap size.
    # server.memory.heap.initial_size: "317m"
    # server.memory.heap.max_size: "317m"

  apoc_config: {}
  #  apoc.trigger.enabled: "true"
  #  apoc.import.file.enabled: "true"

  #apoc_credentials allow you to set configs like apoc.jdbc.<aliasname>.url and apoc.es.<aliasname>.url via a kubernetes secret mounted on the provided path
  #ensure the secret exists beforehand or else an error will be thrown by the helm chart
  #aliasName , secretName and secretMountPath are compulsory fields. Empty fields will result in error
  #please ensure you are using the compatible apoc-extended plugin jar while using apoc_credentials
  #please ensure the secret is created with the key named as "URL"
    #Ex: kubectl create secret generic jdbcsecret --from-literal=URL="jdbc:mysql://30.0.0.0:3306/Northwind?user=root&password=password"
  apoc_credentials: {}
  #   jdbc:
  #    aliasName: "jdbc"
  #    secretName: "jdbcsecret"
  #    secretMountPath: "/secret/jdbcCred"
  #
  #   elasticsearch:
  #     aliasName: "es"
  #     secretName: "essecret"
  #     secretMountPath: "/secret/esCred"



  # securityContext defines privilege and access control settings for a Pod. Making sure that we dont run Neo4j as root user.
  securityContext:
    runAsNonRoot: true
    runAsUser: 7474
    runAsGroup: 7474
    fsGroup: 7474
    fsGroupChangePolicy: "Always"

  # securityContext defines privilege and access control settings for a Container. Making sure that we dont run Neo4j as root user.
  containerSecurityContext:
    runAsNonRoot: true
    runAsUser: 7474
    runAsGroup: 7474
    capabilities:
      drop: [ "ALL" ]

  # Readiness probes are set to know when a container is ready to be used.
  # Because Neo4j uses Java these values are large to distinguish between long Garbage Collection pauses (which don't require a restart) and an actual failure.
  # These values should mark Neo4j as not ready after at most 5 minutes of problems (20 attempts * max 15 seconds between probes)
  readinessProbe:
    failureThreshold: 20
    timeoutSeconds: 10
    periodSeconds: 5

  # Liveness probes are set to know when to restart a container.
  # Because Neo4j uses Java these values are large to distinguish between long Garbage Collection pauses (which don't require a restart) and an actual failure.
  # These values should trigger a restart after at most 10 minutes of problems (40 attempts * max 15 seconds between probes)
  livenessProbe:
    failureThreshold: 40
    timeoutSeconds: 10
    periodSeconds: 5

  # Startup probes are used to know when a container application has started.
  # If such a probe is configured, it disables liveness and readiness checks until it succeeds
  # When restoring Neo4j from a backup it's important that startup probe gives time for Neo4j to recover and/or upgrade store files
  # When using Neo4j clusters it's important that startup probe give the Neo4j cluster time to form
  startupProbe:
    failureThreshold: 1000
    periodSeconds: 5

  # top level setting called ssl to match the "ssl" from "dbms.ssl.policy"
  ssl:
    # setting per "connector" matching neo4j config
    bolt:
      privateKey:
        secretName:  # we set up the template to grab `private.key` from this secret
        subPath:  # we specify the privateKey value name to get from the secret
      publicCertificate:
        secretName:  # we set up the template to grab `public.crt` from this secret
        subPath:  # we specify the publicCertificate value name to get from the secret
      trustedCerts:
        sources: [ ] # a sources array for a projected volume - this allows someone to (relatively) easily mount multiple public certs from multiple secrets for example.
      revokedCerts:
        sources: [ ]  # a sources array for a projected volume
    https:
      privateKey:
        secretName:
        subPath:
      publicCertificate:
        secretName:
        subPath:
      trustedCerts:
        sources: [ ]
      revokedCerts:
        sources: [ ]
    cluster:
      privateKey:
        secretName:
        subPath:
      publicCertificate:
        secretName:
        subPath:
      trustedCerts:
        sources: [ ]
      revokedCerts:
        sources: [ ]

  # Kubernetes cluster domain suffix
  clusterDomain: "cluster.local"

  # Override image settings in Neo4j pod
  image:
    imagePullPolicy: IfNotPresent
    # set a customImage if you want to use your own docker image
  #  customImage: eu.gcr.io/neo4j-helm/neo4j:v5

    #imagePullSecrets list
  #  imagePullSecrets:
  #    - "demo"

    #imageCredentials list for which secret of type docker-registry will be created automatically using the details provided
    # password, name are compulsory fields for an imageCredential , without these fields helm chart will throw an error
    # registry ,username and email are optional fields
    # imageCredential name should be part of the imagePullSecrets list or else the respective imageCredential will be ignored and no secret creation will be done
    # In case of a secret already pre-existing you don't need to mention the imageCredential , just add the pre-existing secretName to the imagePullSecret list
    # and that will be used as an imagePullSecret
  #  imageCredentials:
  #    - registry: ""
  #      username: ""
  #      password: ""
  #      email: ""
  #      name: ""

  statefulset:
    metadata:
      #Annotations for Neo4j StatefulSet
      annotations:
  #      imageregistry: "https://hub.docker.com/"
  #      demo: alpha

  # additional environment variables for the Neo4j Container
  env: {}

  # Other K8s configuration to apply to the Neo4j pod
  podSpec:

    #Annotations for Neo4j pod
    annotations: {}
  #   imageregistry: "https://hub.docker.com/"
  #   demo: alpha

    nodeAffinity: {}
  #    requiredDuringSchedulingIgnoredDuringExecution:
  #      nodeSelectorTerms:
  #        - matchExpressions:
  #            - key: topology.kubernetes.io/zone
  #              operator: In
  #              values:
  #                - antarctica-east1
  #                - antarctica-west1
  #    preferredDuringSchedulingIgnoredDuringExecution:
  #      - weight: 1
  #        preference:
  #          matchExpressions:
  #            - key: another-node-label-key
  #              operator: In
  #              values:
  #                - another-node-label-value

    # Anti Affinity
    # If set to true then an anti-affinity rule is applied to prevent database pods with the same `neo4j.name` running on a single Kubernetes node.
    # If set to false then no anti-affinity rules are applied
    # If set to an object then that object is used for the Neo4j podAntiAffinity
    podAntiAffinity: true
  #    requiredDuringSchedulingIgnoredDuringExecution:
  #      - labelSelector:
  #          matchLabels:
  #            app: "demo"
  #            helm.neo4j.com/pod_category: "neo4j-instance"
  #        topologyKey: kubernetes.io/hostname

    #Add tolerations to the Neo4j pod
    tolerations: []
  #    - key: "key1"
  #      operator: "Equal"
  #      value: "value1"
  #      effect: "NoSchedule"
  #    - key: "key2"
  #      operator: "Equal"
  #      value: "value2"
  #      effect: "NoSchedule"

    # topologySpreadConstraints fields for Neo4j Pod
    # https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/
    topologySpreadConstraints: []
  #    - maxSkew: 1
  #      topologyKey: kubernetes.io/hostname
  #      whenUnsatisfiable: DoNotSchedule
  #      labelSelector:
  #        matchLabels:
  #          app: foo
  #      matchLabelKeys:
  #        - pod-template-hash

    #Priority indicates the importance of a Pod relative to other Pods.
    # More Information : https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/
    priorityClassName: ""

    #This indicates that the neo4j instance be included to the loadbalancer. Can be set to exclude to not add the stateful set to loadbalancer
    loadbalancer: "exclude"

    # set pod's dns policy. ClusterFirst by default
    # https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#pod-s-dns-policy
    dnsPolicy: "ClusterFirst"

    # Name of service account to use for the Neo4j Pod (optional)
    # this is useful if you want to use Workload Identity to grant permissions to access cloud resources e.g. cloud object storage (AWS S3 etc.)
    # For clusters, please ensure that it has the appropriate roles and role-bindings to be able to query kubernetes services
    serviceAccountName: ""

    # How long the Neo4j pod is permitted to keep running after it has been signalled by Kubernetes to stop. Once this timeout elapses the Neo4j process is forcibly terminated.
    # A large value is used because Neo4j takes time to flush in-memory data to disk on shutdown.
    terminationGracePeriodSeconds: 3600

    # initContainers for the Neo4j pod
    initContainers: [ ]

    # additional runtime containers for the Neo4j pod
    containers: []
     


  # print the neo4j user password set during install to the `helm install` log
  logInitialPassword: true

  # Jvm configuration for Neo4j
  jvm:
    # If true any additional arguments are added after the Neo4j default jvm arguments.
    # If false Neo4j default jvm arguments are not used.
    useNeo4jDefaultJvmArguments: true
    # additionalJvmArguments is a list of strings. Each jvm argument should be a separate element:
    additionalJvmArguments: []
  #   - "-XX:+HeapDumpOnOutOfMemoryError"
  #   - "-XX:HeapDumpPath=/logs/neo4j.hprof"
  #   - "-XX:MaxMetaspaceSize=180m"
  #   - "-XX:ReservedCodeCacheSize=40m"

  logging:
    serverLogsXml: |-
  #    <?xml version="1.0" encoding="UTF-8"?>
  #    <!-- Example JSON logging configuration -->
  #    <Configuration status="ERROR" monitorInterval="30" packages="org.neo4j.logging.log4j">
  #        <Appenders>
  #            <!-- Default debug.log, please keep -->
  #            <RollingRandomAccessFile name="DebugLog" fileName="${config:server.directories.logs}/debug.log"
  #                                     filePattern="$${config:server.directories.logs}/debug.log.%02i">
  #                <JsonTemplateLayout eventTemplateUri="classpath:org/neo4j/logging/StructuredLayoutWithMessage.json"/>
  #                <Policies>
  #                    <SizeBasedTriggeringPolicy size="20 MB"/>
  #                </Policies>
  #                <DefaultRolloverStrategy fileIndex="min" max="7"/>
  #            </RollingRandomAccessFile>
  #
  #            <RollingRandomAccessFile name="HttpLog" fileName="${config:server.directories.logs}/http.log"
  #                                     filePattern="$${config:server.directories.logs}/http.log.%02i">
  #                <JsonTemplateLayout eventTemplateUri="classpath:org/neo4j/logging/StructuredLayoutWithMessage.json"/>
  #                <Policies>
  #                    <SizeBasedTriggeringPolicy size="20 MB"/>
  #                </Policies>
  #                <DefaultRolloverStrategy fileIndex="min" max="5"/>
  #            </RollingRandomAccessFile>
  #
  #            <RollingRandomAccessFile name="QueryLog" fileName="${config:server.directories.logs}/query.log"
  #                                     filePattern="$${config:server.directories.logs}/query.log.%02i">
  #                <JsonTemplateLayout eventTemplateUri="classpath:org/neo4j/logging/QueryLogJsonLayout.json"/>
  #                <Policies>
  #                    <SizeBasedTriggeringPolicy size="20 MB"/>
  #                </Policies>
  #                <DefaultRolloverStrategy fileIndex="min" max="7"/>
  #            </RollingRandomAccessFile>
  #
  #            <RollingRandomAccessFile name="SecurityLog" fileName="${config:server.directories.logs}/security.log"
  #                                     filePattern="$${config:server.directories.logs}/security.log.%02i">
  #                <JsonTemplateLayout eventTemplateUri="classpath:org/neo4j/logging/StructuredLayoutWithMessage.json"/>
  #                <Policies>
  #                    <SizeBasedTriggeringPolicy size="20 MB"/>
  #                </Policies>
  #                <DefaultRolloverStrategy fileIndex="min" max="7"/>
  #            </RollingRandomAccessFile>
  #        </Appenders>
  #
  #        <Loggers>
  #            <!-- Log levels. One of DEBUG, INFO, WARN, ERROR or OFF -->
  #
  #            <!-- The debug log is used as the root logger to catch everything -->
  #            <Root level="INFO">
  #                <AppenderRef ref="DebugLog"/> <!-- Keep this -->
  #            </Root>
  #            <!-- The query log, must be named "QueryLogger" -->
  #            <Logger name="QueryLogger" level="INFO" additivity="false">
  #                <AppenderRef ref="QueryLog"/>
  #            </Logger>
  #            <!-- The http request log, must be named "HttpLogger" -->
  #            <Logger name="HttpLogger" level="INFO" additivity="false">
  #                <AppenderRef ref="HttpLog"/>
  #            </Logger>
  #            <!-- The security log, must be named "SecurityLogger" -->
  #            <Logger name="SecurityLogger" level="INFO" additivity="false">
  #                <AppenderRef ref="SecurityLog"/>
  #            </Logger>
  #        </Loggers>
  #    </Configuration>
    userLogsXml: |-
  #    <?xml version="1.0" encoding="UTF-8"?>
  #    <!-- Example JSON logging configuration -->
  #    <Configuration status="ERROR" monitorInterval="30" packages="org.neo4j.logging.log4j">
  #    <Appenders>
  #        <RollingRandomAccessFile name="Neo4jLog" fileName="${config:server.directories.logs}/neo4j.log"
  #                                 filePattern="$${config:server.directories.logs}/neo4j.log.%02i">
  #            <JsonTemplateLayout eventTemplateUri="classpath:org/neo4j/logging/StructuredLayoutWithMessage.json"/>
  #            <Policies>
  #                <SizeBasedTriggeringPolicy size="20 MB"/>
  #            </Policies>
  #            <DefaultRolloverStrategy fileIndex="min" max="7"/>
  #        </RollingRandomAccessFile>
  #        <!-- Only used by "neo4j console", will be ignored otherwise -->
  #        <Console name="ConsoleAppender" target="SYSTEM_OUT">
  #            <PatternLayout pattern="%d{yyyy-MM-dd HH:mm:ss.SSSZ}{GMT+0} %-5p %m%n"/>
  #        </Console>
  #    </Appenders>
  #    <Loggers>
  #        <!-- Log level for the neo4j log. One of DEBUG, INFO, WARN, ERROR or OFF -->
  #        <Root level="INFO">
  #            <AppenderRef ref="Neo4jLog"/>
  #            <AppenderRef ref="ConsoleAppender"/>
  #        </Root>
  #    </Loggers>
  #    </Configuration>

  # define your podDisruptionBudget details here
  podDisruptionBudget:
    enabled: false
    matchLabels: {}
  #    "demo": "neo4j"
    matchExpressions: []
  #    - key: "demo"
  #      operator: "Equals"
  #      value: "neo4j"
    labels: {}
  #    "name": "neo4j"
    minAvailable: ""
    maxUnavailable: ""

  # Service Monitor for prometheus
  # Please ensure prometheus operator or the service monitor CRD is present in your cluster before using service monitor config
  serviceMonitor:
    enabled: false
    labels: {}
  #    "demo": "value"
    jobLabel: ""
    interval: ""
    port: ""
    path: ""
    namespaceSelector: {}
  #    any: false
  #    matchNames:
  #      - default
    targetLabels: []
  #    - "demo"
  #    - "value"
    selector: {}
  #    matchLabels:
  #      helm.neo4j.com/service: "admin"


  # this section is to be used only when setting up (1 primary + n secondary neo4j instances scenario)
  # Disabled by default.
  analytics:
    # This flag will enable the internal ports and certain configs necessary to allow 1 primary + n secondary neo4j instances scenario
    enabled: false
    type:
      # values can be primary or secondary
      # this field denotes the neo4j instance type either primary or secondary
      name: primary




airflow:
  enabled: true
  # Licensed to the Apache Software Foundation (ASF) under one
  # or more contributor license agreements.  See the NOTICE file
  # distributed with this work for additional information
  # regarding copyright ownership.  The ASF licenses this file
  # to you under the Apache License, Version 2.0 (the
  # "License"); you may not use this file except in compliance
  # with the License.  You may obtain a copy of the License at
  #
  #   http://www.apache.org/licenses/LICENSE-2.0
  #
  # Unless required by applicable law or agreed to in writing,
  # software distributed under the License is distributed on an
  # "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
  # KIND, either express or implied.  See the License for the
  # specific language governing permissions and limitations
  # under the License.
  # Default values for airflow.
  # This is a YAML-formatted file.
  # Declare variables to be passed into your templates.

  # Provide a name to substitute for the full names of resources
  fullnameOverride: ""

  # Provide a name to substitute for the name of the chart
  nameOverride: ""

  # Use standard naming for all resources using airflow.fullname template
  # Consider removing this later and default it to true
  # to make this chart follow standard naming conventions using the fullname template.
  # For now this is an opt-in switch for backwards compatibility to leverage the standard naming convention
  # and being able to use fully fullnameOverride and nameOverride in all resources
  # For new installations - it is recommended to set it to True to follow standard naming conventions
  # For existing installations, this will rename and redeploy your resources with the new names. Be aware that
  # this will recreate your deployment/statefulsets along with their persistent volume claims and data storage
  # migration may be needed to keep your old data
  #
  # Note:fernet-key,redis-password and broker-url secrets don't use this logic yet,
  # as this may break existing installations due to how they get installed via pre-install hook.
  useStandardNaming: false

  # Max number of old replicasets to retain. Can be overridden by each deployment's revisionHistoryLimit
  revisionHistoryLimit: ~

  # User and group of airflow user
  uid: 50000
  gid: 0

  # Default security context for airflow (deprecated, use `securityContexts` instead)
  securityContext: {}
  #  runAsUser: 50000
  #  fsGroup: 0
  #  runAsGroup: 0

  # Detailed default security context for airflow deployments
  securityContexts:
    pod: {}
    containers: {}

  # Global container lifecycle hooks for airflow containers
  containerLifecycleHooks: {}

  # Airflow home directory
  # Used for mount paths
  airflowHome: /opt/airflow

  # Default airflow repository -- overridden by all the specific images below
  defaultAirflowRepository: apache/airflow

  # Default airflow tag to deploy
  defaultAirflowTag: "2.9.3"

  # Default airflow digest. If specified, it takes precedence over tag
  defaultAirflowDigest: ~

  # Airflow version (Used to make some decisions based on Airflow Version being deployed)
  airflowVersion: "2.9.3"

  # Images
  images:
    airflow:
      repository: l8l54140.c1.gra9.container-registry.ovh.net/library/airflow
      tag: RELEASE-10
      # Specifying digest takes precedence over tag.
      digest: sha256:9d40e21dadbfc052438b4fdc4d1a3de2c4662586c92145b66860d37272ba0d14
      pullPolicy: IfNotPresent
    # To avoid images with user code, you can turn this to 'true' and
    # all the 'run-airflow-migrations' and 'wait-for-airflow-migrations' containers/jobs
    # will use the images from 'defaultAirflowRepository:defaultAirflowTag' values
    # to run and wait for DB migrations .
    useDefaultImageForMigration: false
    # timeout (in seconds) for airflow-migrations to complete
    migrationsWaitTimeout: 60
    pod_template:
      # Note that `images.pod_template.repository` and `images.pod_template.tag` parameters
      # can be overridden in `config.kubernetes` section. So for these parameters to have effect
      # `config.kubernetes.worker_container_repository` and `config.kubernetes.worker_container_tag`
      # must be not set .
      repository: ~
      tag: ~
      pullPolicy: IfNotPresent
    flower:
      repository: ~
      tag: ~
      pullPolicy: IfNotPresent
    statsd:
      repository: quay.io/prometheus/statsd-exporter
      tag: v0.26.1
      pullPolicy: IfNotPresent
    redis:
      repository: redis
      # Redis is limited to 7.2-bookworm due to licencing change
      # https://redis.io/blog/redis-adopts-dual-source-available-licensing/
      tag: 7.2-bookworm
      pullPolicy: IfNotPresent
    pgbouncer:
      repository: apache/airflow
      tag: airflow-pgbouncer-2024.01.19-1.21.0
      pullPolicy: IfNotPresent
    pgbouncerExporter:
      repository: apache/airflow
      tag: airflow-pgbouncer-exporter-2024.06.18-0.17.0
      pullPolicy: IfNotPresent
    gitSync:
      repository: registry.k8s.io/git-sync/git-sync
      tag: v4.1.0
      pullPolicy: IfNotPresent

  # Select certain nodes for airflow pods.
  nodeSelector: {}
  affinity: {}
  tolerations: []
  topologySpreadConstraints: []
  schedulerName: ~

  # Add common labels to all objects and pods defined in this chart.
  labels: {}

  # Ingress configuration
  ingress:
    # Enable all ingress resources (deprecated - use ingress.web.enabled and ingress.flower.enabled)
    enabled: ~

    # Configs for the Ingress of the web Service
    web:
      # Enable web ingress resource
      enabled: false

      # Annotations for the web Ingress
      annotations: {}

      # The path for the web Ingress
      path: "/"

      # The pathType for the above path (used only with Kubernetes v1.19 and above)
      pathType: "ImplementationSpecific"

      # The hostname for the web Ingress (Deprecated - renamed to `ingress.web.hosts`)
      host: ""

      # The hostnames or hosts configuration for the web Ingress
      hosts: []
      #   # The hostname for the web Ingress (can be templated)
      # - name: ""
      #   # configs for web Ingress TLS
      #   tls:
      #     # Enable TLS termination for the web Ingress
      #     enabled: false
      #     # the name of a pre-created Secret containing a TLS private key and certificate
      #     secretName: ""

      # The Ingress Class for the web Ingress (used only with Kubernetes v1.19 and above)
      ingressClassName: ""

      # configs for web Ingress TLS (Deprecated - renamed to `ingress.web.hosts[*].tls`)
      tls:
        # Enable TLS termination for the web Ingress
        enabled: false
        # the name of a pre-created Secret containing a TLS private key and certificate
        secretName: ""

      # HTTP paths to add to the web Ingress before the default path
      precedingPaths: []

      # Http paths to add to the web Ingress after the default path
      succeedingPaths: []

    # Configs for the Ingress of the flower Service
    flower:
      # Enable web ingress resource
      enabled: false

      # Annotations for the flower Ingress
      annotations: {}

      # The path for the flower Ingress
      path: "/"

      # The pathType for the above path (used only with Kubernetes v1.19 and above)
      pathType: "ImplementationSpecific"

      # The hostname for the flower Ingress (Deprecated - renamed to `ingress.flower.hosts`)
      host: ""

      # The hostnames or hosts configuration for the flower Ingress
      hosts: []
      #   # The hostname for the flower Ingress (can be templated)
      # - name: ""
      #   tls:
      #     # Enable TLS termination for the flower Ingress
      #     enabled: false
      #     # the name of a pre-created Secret containing a TLS private key and certificate
      #     secretName: ""

      # The Ingress Class for the flower Ingress (used only with Kubernetes v1.19 and above)
      ingressClassName: ""

      # configs for flower Ingress TLS (Deprecated - renamed to `ingress.flower.hosts[*].tls`)
      tls:
        # Enable TLS termination for the flower Ingress
        enabled: false
        # the name of a pre-created Secret containing a TLS private key and certificate
        secretName: ""

  # Network policy configuration
  networkPolicies:
    # Enabled network policies
    enabled: false

  # Extra annotations to apply to all
  # Airflow pods
  airflowPodAnnotations: {}

  # Extra annotations to apply to
  # main Airflow configmap
  airflowConfigAnnotations: {}

  # `airflow_local_settings` file as a string (can be templated).
  airflowLocalSettings: |-
    {{- if semverCompare ">=2.2.0" .Values.airflowVersion }}
    {{- if not (or .Values.webserverSecretKey .Values.webserverSecretKeySecretName) }}
    from airflow.www.utils import UIAlert

    DASHBOARD_UIALERTS = [
      UIAlert(
        'Usage of a dynamic webserver secret key detected. We recommend a static webserver secret key instead.'
        ' See the <a href='
        '"https://airflow.apache.org/docs/helm-chart/stable/production-guide.html#webserver-secret-key" '
        'target="_blank" rel="noopener noreferrer">'
        'Helm Chart Production Guide</a> for more details.',
        category="warning",
        roles=["Admin"],
        html=True,
      )
    ]
    {{- end }}
    {{- end }}

  # Enable RBAC (default on most clusters these days)
  rbac:
    # Specifies whether RBAC resources should be created
    create: true
    createSCCRoleBinding: false

  # Airflow executor
  # One of: LocalExecutor, LocalKubernetesExecutor, CeleryExecutor, KubernetesExecutor, CeleryKubernetesExecutor
  executor: "CeleryExecutor"

  # If this is true and using LocalExecutor/KubernetesExecutor/CeleryKubernetesExecutor, the scheduler's
  # service account will have access to communicate with the api-server and launch pods.
  # If this is true and using CeleryExecutor/KubernetesExecutor/CeleryKubernetesExecutor, the workers
  # will be able to launch pods.
  allowPodLaunching: true

  # Environment variables for all airflow containers
  env: []
  # - name: ""
  #   value: ""

  # Volumes for all airflow containers
  volumes: []

  # VolumeMounts for all airflow containers
  volumeMounts: []

  # Secrets for all airflow containers
  secret: []
  # - envName: ""
  #   secretName: ""
  #   secretKey: ""

  # Enables selected built-in secrets that are set via environment variables by default.
  # Those secrets are provided by the Helm Chart secrets by default but in some cases you
  # might want to provide some of those variables with _CMD or _SECRET variable, and you should
  # in this case disable setting of those variables by setting the relevant configuration to false.
  enableBuiltInSecretEnvVars:
    AIRFLOW__CORE__FERNET_KEY: true
    # For Airflow <2.3, backward compatibility; moved to [database] in 2.3
    AIRFLOW__CORE__SQL_ALCHEMY_CONN: true
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: true
    AIRFLOW_CONN_AIRFLOW_DB: true
    AIRFLOW__WEBSERVER__SECRET_KEY: true
    AIRFLOW__CELERY__CELERY_RESULT_BACKEND: true
    AIRFLOW__CELERY__RESULT_BACKEND: true
    AIRFLOW__CELERY__BROKER_URL: true
    AIRFLOW__ELASTICSEARCH__HOST: true
    AIRFLOW__ELASTICSEARCH__ELASTICSEARCH_HOST: true

  # Priority Classes that will be installed by charts.
  # Ideally, there should be an entry for dagProcessor, flower,
  #   pgbouncer, scheduler, statsd, triggerer, webserver, worker.
  # The format for priorityClasses is an array with each element having:
  #   * name is the name of the priorityClass. Ensure the same name is given to the respective section as well
  #   * preemptionPolicy for the priorityClass
  #   * value is the preemption value for the priorityClass
  priorityClasses: []
  #  - name: class1 (if this is for dagProcessor, ensure overriding .Values.dagProcessor.priorityClass too)
  #    preemptionPolicy: PreemptLowerPriority
  #    value: 10000
  #  - name: class2
  #    preemptionPolicy: Never
  #    value: 100000

  # Extra secrets that will be managed by the chart
  # (You can use them with extraEnv or extraEnvFrom or some of the extraVolumes values).
  # The format for secret data is "key/value" where
  #    * key (can be templated) is the name of the secret that will be created
  #    * value: an object with the standard 'data' or 'stringData' key (or both).
  #          The value associated with those keys must be a string (can be templated)
  extraSecrets: {}
  # eg:
  # extraSecrets:
  #   '{{ .Release.Name }}-airflow-connections':
  #     type: 'Opaque'
  #     labels:
  #       my.custom.label/v1: my_custom_label_value_1
  #     data: |
  #       AIRFLOW_CONN_GCP: 'base64_encoded_gcp_conn_string'
  #       AIRFLOW_CONN_AWS: 'base64_encoded_aws_conn_string'
  #     stringData: |
  #       AIRFLOW_CONN_OTHER: 'other_conn'
  #   '{{ .Release.Name }}-other-secret-name-suffix':
  #     data: |
  #        ...

  # Extra ConfigMaps that will be managed by the chart
  # (You can use them with extraEnv or extraEnvFrom or some of the extraVolumes values).
  # The format for configmap data is "key/value" where
  #    * key (can be templated) is the name of the configmap that will be created
  #    * value: an object with the standard 'data' key.
  #          The value associated with this keys must be a string (can be templated)
  extraConfigMaps: {}
  # eg:
  # extraConfigMaps:
  #   '{{ .Release.Name }}-airflow-variables':
  #     labels:
  #       my.custom.label/v2: my_custom_label_value_2
  #     data: |
  #       AIRFLOW_VAR_HELLO_MESSAGE: "Hi!"
  #       AIRFLOW_VAR_KUBERNETES_NAMESPACE: "{{ .Release.Namespace }}"

  # Extra env 'items' that will be added to the definition of airflow containers
  # a string is expected (can be templated).
  # TODO: difference from `env`? This is a templated string. Probably should template `env` and remove this.
  extraEnv: ~
  # eg:
  # extraEnv: |
  #   - name: AIRFLOW__CORE__LOAD_EXAMPLES
  #     value: 'True'

  # Extra envFrom 'items' that will be added to the definition of airflow containers
  # A string is expected (can be templated).
  extraEnvFrom: ~
  # eg:
  # extraEnvFrom: |
  #   - secretRef:
  #       name: '{{ .Release.Name }}-airflow-connections'
  #   - configMapRef:
  #       name: '{{ .Release.Name }}-airflow-variables'

  # Airflow database & redis config
  data:
    # If secret names are provided, use those secrets
    # These secrets must be created manually, eg:
    #
    # kind: Secret
    # apiVersion: v1
    # metadata:
    #   name: custom-airflow-metadata-secret
    # type: Opaque
    # data:
    #   connection: base64_encoded_connection_string

    metadataSecretName: ~
    # When providing secret names and using the same database for metadata and
    # result backend, for Airflow < 2.4.0 it is necessary to create a separate
    # secret for result backend but with a db+ scheme prefix.
    # For Airflow >= 2.4.0 it is possible to not specify the secret again,
    # as Airflow will use sql_alchemy_conn with a db+ scheme prefix by default.
    resultBackendSecretName: ~
    brokerUrlSecretName: ~

    # Otherwise pass connection values in
    metadataConnection:
      user: postgres
      pass: postgres
      protocol: postgresql
      host: ~
      port: 5432
      db: postgres
      sslmode: disable
    # resultBackendConnection defaults to the same database as metadataConnection
    resultBackendConnection: ~
    # or, you can use a different database
    # resultBackendConnection:
    #   user: postgres
    #   pass: postgres
    #   protocol: postgresql
    #   host: ~
    #   port: 5432
    #   db: postgres
    #   sslmode: disable
    # Note: brokerUrl can only be set during install, not upgrade
    brokerUrl: ~

  # Fernet key settings
  # Note: fernetKey can only be set during install, not upgrade
  fernetKey: ~
  fernetKeySecretName: ~

  # Flask secret key for Airflow Webserver: `[webserver] secret_key` in airflow.cfg
  webserverSecretKey: ~
  webserverSecretKeySecretName: ~

  # In order to use kerberos you need to create secret containing the keytab file
  # The secret name should follow naming convention of the application where resources are
  # name {{ .Release-name }}-<POSTFIX>. In case of the keytab file, the postfix is "kerberos-keytab"
  # So if your release is named "my-release" the name of the secret should be "my-release-kerberos-keytab"
  #
  # The Keytab content should be available in the "kerberos.keytab" key of the secret.
  #
  #  apiVersion: v1
  #  kind: Secret
  #  data:
  #    kerberos.keytab: <base64_encoded keytab file content>
  #  type: Opaque
  #
  #
  #  If you have such keytab file you can do it with similar
  #
  #  kubectl create secret generic {{ .Release.name }}-kerberos-keytab --from-file=kerberos.keytab
  #
  #
  #  Alternatively, instead of manually creating the secret, it is possible to specify
  #  kerberos.keytabBase64Content parameter. This parameter should contain base64 encoded keytab.
  #

  kerberos:
    enabled: false
    ccacheMountPath: /var/kerberos-ccache
    ccacheFileName: cache
    configPath: /etc/krb5.conf
    keytabBase64Content: ~
    keytabPath: /etc/airflow.keytab
    principal: airflow@FOO.COM
    reinitFrequency: 3600
    config: |
      # This is an example config showing how you can use templating and how "example" config
      # might look like. It works with the test kerberos server that we are using during integration
      # testing at Apache Airflow (see `scripts/ci/docker-compose/integration-kerberos.yml` but in
      # order to make it production-ready you must replace it with your own configuration that
      # Matches your kerberos deployment. Administrators of your Kerberos instance should
      # provide the right configuration.

      [logging]
      default = "FILE:{{ template "airflow_logs_no_quote" . }}/kerberos_libs.log"
      kdc = "FILE:{{ template "airflow_logs_no_quote" . }}/kerberos_kdc.log"
      admin_server = "FILE:{{ template "airflow_logs_no_quote" . }}/kadmind.log"

      [libdefaults]
      default_realm = FOO.COM
      ticket_lifetime = 10h
      renew_lifetime = 7d
      forwardable = true

      [realms]
      FOO.COM = {
        kdc = kdc-server.foo.com
        admin_server = admin_server.foo.com
      }

  # Airflow Worker Config
  workers:
    # Number of airflow celery workers in StatefulSet
    replicas: 2
    # Max number of old replicasets to retain
    revisionHistoryLimit: ~

    # Command to use when running Airflow workers (templated).
    command: ~
    # Args to use when running Airflow workers (templated).
    args:
      - "bash"
      - "-c"
      # The format below is necessary to get `helm lint` happy
      - |-
        exec \
        airflow {{ semverCompare ">=2.0.0" .Values.airflowVersion | ternary "celery worker" "worker" }}

    # If the worker stops responding for 5 minutes (5*60s) kill the
    # worker and let Kubernetes restart it
    livenessProbe:
      enabled: true
      initialDelaySeconds: 10
      timeoutSeconds: 20
      failureThreshold: 5
      periodSeconds: 60
      command: ~

    # Update Strategy when worker is deployed as a StatefulSet
    updateStrategy: ~
    # Update Strategy when worker is deployed as a Deployment
    strategy:
      rollingUpdate:
        maxSurge: "100%"
        maxUnavailable: "50%"

    # When not set, the values defined in the global securityContext will be used
    securityContext: {}
    #  runAsUser: 50000
    #  fsGroup: 0
    #  runAsGroup: 0

    # Detailed default security context for worker deployments for container and pod level
    securityContexts:
      pod: {}
      container: {}

    # container level lifecycle hooks
    containerLifecycleHooks: {}

    # Create ServiceAccount
    serviceAccount:
      # default value is true
      # ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
      automountServiceAccountToken: true
      # Specifies whether a ServiceAccount should be created
      create: true
      # The name of the ServiceAccount to use.
      # If not set and create is true, a name is generated using the release name
      name: ~

      # Annotations to add to worker kubernetes service account.
      annotations: {}

    # Allow KEDA autoscaling.
    keda:
      enabled: false
      namespaceLabels: {}

      # How often KEDA polls the airflow DB to report new scale requests to the HPA
      pollingInterval: 5

      # How many seconds KEDA will wait before scaling to zero.
      # Note that HPA has a separate cooldown period for scale-downs
      cooldownPeriod: 30

      # Minimum number of workers created by keda
      minReplicaCount: 0

      # Maximum number of workers created by keda
      maxReplicaCount: 10

      # Specify HPA related options
      advanced: {}
      # horizontalPodAutoscalerConfig:
      #   behavior:
      #     scaleDown:
      #       stabilizationWindowSeconds: 300
      #       policies:
      #         - type: Percent
      #           value: 100
      #           periodSeconds: 15

      # Query to use for KEDA autoscaling. Must return a single integer.
      query: >-
        SELECT ceil(COUNT(*)::decimal / {{ .Values.config.celery.worker_concurrency }})
        FROM task_instance
        WHERE (state='running' OR state='queued')
        {{- if eq .Values.executor "CeleryKubernetesExecutor" }}
        AND queue != '{{ .Values.config.celery_kubernetes_executor.kubernetes_queue }}'
        {{- end }}

      # Weather to use PGBouncer to connect to the database or not when it is enabled
      # This configuration will be ignored if PGBouncer is not enabled
      usePgbouncer: true

    # Allow HPA (KEDA must be disabled).
    hpa:
      enabled: false

      # Minimum number of workers created by HPA
      minReplicaCount: 0

      # Maximum number of workers created by HPA
      maxReplicaCount: 5

      # Specifications for which to use to calculate the desired replica count
      metrics:
        - type: Resource
          resource:
            name: cpu
            target:
              type: Utilization
              averageUtilization: 80

      # Scaling behavior of the target in both Up and Down directions
      behavior: {}

    persistence:
      # Enable persistent volumes
      enabled: true
      # This policy determines whether PVCs should be deleted when StatefulSet is scaled down or removed.
      # persistentVolumeClaimRetentionPolicy: ~
      # persistentVolumeClaimRetentionPolicy:
      #   whenDeleted: Delete
      #   whenScaled: Delete
      # Volume size for worker StatefulSet
      size: 20Gi
      # If using a custom storageClass, pass name ref to all statefulSets here
      storageClassName:
      # Execute init container to chown log directory.
      # This is currently only needed in kind, due to usage
      # of local-path provisioner.
      fixPermissions: false
      # Annotations to add to worker volumes
      annotations: {}
      # Detailed default security context for persistence for container level
      securityContexts:
        container: {}
      # container level lifecycle hooks
      containerLifecycleHooks: {}

    kerberosSidecar:
      # Enable kerberos sidecar
      enabled: false
      resources: {}
      #  limits:
      #   cpu: 100m
      #   memory: 128Mi
      #  requests:
      #   cpu: 100m
      #   memory: 128Mi
      # Detailed default security context for kerberosSidecar for container level
      securityContexts:
        container: {}
      # container level lifecycle hooks
      containerLifecycleHooks: {}

    kerberosInitContainer:
      # Enable kerberos init container
      enabled: false
      resources: {}
      #  limits:
      #   cpu: 100m
      #   memory: 128Mi
      #  requests:
      #   cpu: 100m
      #   memory: 128Mi


    resources: 
     limits:
      cpu: 1046m
      memory: 4.5Gi
     requests:
      cpu: 1046m
      memory: 4.5Gi

    # Grace period for tasks to finish after SIGTERM is sent from kubernetes
    terminationGracePeriodSeconds: 600

    # This setting tells kubernetes that its ok to evict
    # when it wants to scale a node down.
    safeToEvict: false

    # Launch additional containers into worker (templated).
    # Note: If used with KubernetesExecutor, you are responsible for signaling sidecars to exit when the main
    # container finishes so Airflow can continue the worker shutdown process!
    extraContainers: []
    # Add additional init containers into workers (templated).
    extraInitContainers: []

    # Mount additional volumes into worker. It can be templated like in the following example:
    #   extraVolumes:
    #     - name: my-templated-extra-volume
    #       secret:
    #          secretName: '{{ include "my_secret_template" . }}'
    #          defaultMode: 0640
    #          optional: true
    #
    #   extraVolumeMounts:
    #     - name: my-templated-extra-volume
    #       mountPath: "{{ .Values.my_custom_path }}"
    #       readOnly: true
    extraVolumes: []
    extraVolumeMounts: []

    # Select certain nodes for airflow worker pods.
    nodeSelector: {}
    runtimeClassName: ~
    priorityClassName: ~
    affinity: {}
    # default worker affinity is:
    #  podAntiAffinity:
    #    preferredDuringSchedulingIgnoredDuringExecution:
    #    - podAffinityTerm:
    #        labelSelector:
    #          matchLabels:
    #            component: worker
    #        topologyKey: kubernetes.io/hostname
    #      weight: 100
    tolerations: []
    topologySpreadConstraints: []
    # hostAliases to use in worker pods.
    # See:
    # https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/
    hostAliases: []
    # - ip: "127.0.0.2"
    #   hostnames:
    #   - "test.hostname.one"
    # - ip: "127.0.0.3"
    #   hostnames:
    #   - "test.hostname.two"

    # annotations for the worker resource
    annotations: {}

    podAnnotations: {}

    # Labels specific to workers objects and pods
    labels: {}

    logGroomerSidecar:
      # Whether to deploy the Airflow worker log groomer sidecar.
      enabled: true
      # Command to use when running the Airflow worker log groomer sidecar (templated).
      command: ~
      # Args to use when running the Airflow worker log groomer sidecar (templated).
      args: ["bash", "/clean-logs"]
      # Number of days to retain logs
      retentionDays: 15
      resources: {}
      #  limits:
      #   cpu: 100m
      #   memory: 128Mi
      #  requests:
      #   cpu: 100m
      #   memory: 128Mi
      # Detailed default security context for logGroomerSidecar for container level
      securityContexts:
        container: {}

    waitForMigrations:
      # Whether to create init container to wait for db migrations
      enabled: true
      env: []
      # Detailed default security context for waitForMigrations for container level
      securityContexts:
        container: {}

    env: []

    volumeClaimTemplates: []
    # Additional volumeClaimTemplates needed.
    # Comment out the above and uncomment the section below to enable it.
    # Add more as needed
    # Make sure to mount it under extraVolumeMounts.
    # volumeClaimTemplates:
    #   - metadata:
    #       name: data-volume-1
    #     spec:
    #       storageClassName: "storage-class-1"
    #       accessModes:
    #         - "ReadWriteOnce"
    #       resources:
    #         requests:
    #           storage: "10Gi"
    #   - metadata:
    #       name: data-volume-2
    #     spec:
    #       storageClassName: "storage-class-2"
    #       accessModes:
    #         - "ReadWriteOnce"
    #       resources:
    #         requests:
    #           storage: "20Gi"

  # Airflow scheduler settings
  scheduler:
    enabled: true
    #  hostAliases for the scheduler pod
    hostAliases: []
    #  - ip: "127.0.0.1"
    #    hostnames:
    #      - "foo.local"
    #  - ip: "10.1.2.3"
    #    hostnames:
    #      - "foo.remote"

    # If the scheduler stops heartbeating for 5 minutes (5*60s) kill the
    # scheduler and let Kubernetes restart it
    livenessProbe:
      initialDelaySeconds: 10
      timeoutSeconds: 20
      failureThreshold: 5
      periodSeconds: 60
      command: ~

    # Wait for at most 1 minute (6*10s) for the scheduler container to startup.
    # livenessProbe kicks in after the first successful startupProbe
    startupProbe:
      failureThreshold: 6
      periodSeconds: 10
      timeoutSeconds: 20
      command: ~

    # Airflow 2.0 allows users to run multiple schedulers,
    # However this feature is only recommended for MySQL 8+ and Postgres
    replicas: 1
    # Max number of old replicasets to retain
    revisionHistoryLimit: ~

    # Command to use when running the Airflow scheduler (templated).
    command: ~
    # Args to use when running the Airflow scheduler (templated).
    args: ["bash", "-c", "exec airflow scheduler"]

    # Update Strategy when scheduler is deployed as a StatefulSet
    # (when using LocalExecutor and workers.persistence)
    updateStrategy: ~
    # Update Strategy when scheduler is deployed as a Deployment
    # (when not using LocalExecutor and workers.persistence)
    strategy: ~

    # When not set, the values defined in the global securityContext will be used
    # (deprecated, use `securityContexts` instead)
    securityContext: {}
    #  runAsUser: 50000
    #  fsGroup: 0
    #  runAsGroup: 0

    # Detailed default security context for scheduler deployments for container and pod level
    securityContexts:
      pod: {}
      container: {}

    # container level lifecycle hooks
    containerLifecycleHooks: {}

    # Create ServiceAccount
    serviceAccount:
      # default value is true
      # ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
      automountServiceAccountToken: true
      # Specifies whether a ServiceAccount should be created
      create: true
      # The name of the ServiceAccount to use.
      # If not set and create is true, a name is generated using the release name
      name: ~

      # Annotations to add to scheduler kubernetes service account.
      annotations: {}

    # Scheduler pod disruption budget
    podDisruptionBudget:
      enabled: false

      # PDB configuration
      config:
        # minAvailable and maxUnavailable are mutually exclusive
        maxUnavailable: 1
        # minAvailable: 1

    resources: {}
    #  limits:
    #   cpu: 100m
    #   memory: 128Mi
    #  requests:
    #   cpu: 100m
    #   memory: 128Mi

    # This setting tells kubernetes that its ok to evict
    # when it wants to scale a node down.
    safeToEvict: true

    # Launch additional containers into scheduler (templated).
    extraContainers: []
    # Add additional init containers into scheduler (templated).
    extraInitContainers: []

    # Mount additional volumes into scheduler. It can be templated like in the following example:
    #   extraVolumes:
    #     - name: my-templated-extra-volume
    #       secret:
    #          secretName: '{{ include "my_secret_template" . }}'
    #          defaultMode: 0640
    #          optional: true
    #
    #   extraVolumeMounts:
    #     - name: my-templated-extra-volume
    #       mountPath: "{{ .Values.my_custom_path }}"
    #       readOnly: true
    extraVolumes: []
    extraVolumeMounts: []

    # Select certain nodes for airflow scheduler pods.
    nodeSelector: {}
    affinity: {}
    # default scheduler affinity is:
    #  podAntiAffinity:
    #    preferredDuringSchedulingIgnoredDuringExecution:
    #    - podAffinityTerm:
    #        labelSelector:
    #          matchLabels:
    #            component: scheduler
    #        topologyKey: kubernetes.io/hostname
    #      weight: 100
    tolerations: []
    topologySpreadConstraints: []

    priorityClassName: ~

    # annotations for scheduler deployment
    annotations: {}

    podAnnotations: {}

    # Labels specific to scheduler objects and pods
    labels: {}

    logGroomerSidecar:
      # Whether to deploy the Airflow scheduler log groomer sidecar.
      enabled: true
      # Command to use when running the Airflow scheduler log groomer sidecar (templated).
      command: ~
      # Args to use when running the Airflow scheduler log groomer sidecar (templated).
      args: ["bash", "/clean-logs"]
      # Number of days to retain logs
      retentionDays: 15
      resources: {}
      #  limits:
      #   cpu: 100m
      #   memory: 128Mi
      #  requests:
      #   cpu: 100m
      #   memory: 128Mi
      # Detailed default security context for logGroomerSidecar for container level
      securityContexts:
        container: {}
      # container level lifecycle hooks
      containerLifecycleHooks: {}

    waitForMigrations:
      # Whether to create init container to wait for db migrations
      enabled: true
      env: []
      # Detailed default security context for waitForMigrations for container level
      securityContexts:
        container: {}

    env: []

  # Airflow create user job settings
  createUserJob:
    # Limit the lifetime of the job object after it finished execution.
    ttlSecondsAfterFinished: 300
    # Command to use when running the create user job (templated).
    command: ~
    # Args to use when running the create user job (templated).
    args:
      - "bash"
      - "-c"
      # The format below is necessary to get `helm lint` happy
      - |-
        exec \
        airflow {{ semverCompare ">=2.0.0" .Values.airflowVersion | ternary "users create" "create_user" }} "$@"
      - --
      - "-r"
      - "{{ .Values.webserver.defaultUser.role }}"
      - "-u"
      - "{{ .Values.webserver.defaultUser.username }}"
      - "-e"
      - "{{ .Values.webserver.defaultUser.email }}"
      - "-f"
      - "{{ .Values.webserver.defaultUser.firstName }}"
      - "-l"
      - "{{ .Values.webserver.defaultUser.lastName }}"
      - "-p"
      - "{{ .Values.webserver.defaultUser.password }}"

    # Annotations on the create user job pod
    annotations: {}
    # jobAnnotations are annotations on the create user job
    jobAnnotations: {}

    # Labels specific to createUserJob objects and pods
    labels: {}

    # When not set, the values defined in the global securityContext will be used
    securityContext: {}
    #  runAsUser: 50000
    #  fsGroup: 0
    #  runAsGroup: 0

    # Detailed default security context for createUserJob for container and pod level
    securityContexts:
      pod: {}
      container: {}

    # container level lifecycle hooks
    containerLifecycleHooks: {}

    # Create ServiceAccount
    serviceAccount:
      # default value is true
      # ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
      automountServiceAccountToken: true
      # Specifies whether a ServiceAccount should be created
      create: true
      # The name of the ServiceAccount to use.
      # If not set and create is true, a name is generated using the release name
      name: ~

      # Annotations to add to create user kubernetes service account.
      annotations: {}

    # Launch additional containers into user creation job
    extraContainers: []

    # Add additional init containers into user creation job (templated).
    # extraInitContainers: []

    # Mount additional volumes into user creation job. It can be templated like in the following example:
    #   extraVolumes:
    #     - name: my-templated-extra-volume
    #       secret:
    #          secretName: '{{ include "my_secret_template" . }}'
    #          defaultMode: 0640
    #          optional: true
    #
    #   extraVolumeMounts:
    #     - name: my-templated-extra-volume
    #       mountPath: "{{ .Values.my_custom_path }}"
    #       readOnly: true
    extraVolumes: []
    extraVolumeMounts: []

    nodeSelector: {}
    affinity: {}
    tolerations: []
    topologySpreadConstraints: []
    priorityClassName: ~
    # In case you need to disable the helm hooks that create the jobs after install.
    # Disable this if you are using ArgoCD for example
    useHelmHooks: true
    applyCustomEnv: true

    env: []

    resources: {}
    #  limits:
    #   cpu: 100m
    #   memory: 128Mi
    #  requests:
    #   cpu: 100m
    #   memory: 128Mi

  # Airflow database migration job settings
  migrateDatabaseJob:
    enabled: true
    # Limit the lifetime of the job object after it finished execution.
    ttlSecondsAfterFinished: 300
    # Command to use when running the migrate database job (templated).
    command: ~
    # Args to use when running the migrate database job (templated).
    args:
      - "bash"
      - "-c"
      - >-
        exec \

        airflow {{ semverCompare ">=2.7.0" .Values.airflowVersion
        | ternary "db migrate" (semverCompare ">=2.0.0" .Values.airflowVersion
        | ternary "db upgrade" "upgradedb") }}

    # Annotations on the database migration pod
    annotations: {}
    # jobAnnotations are annotations on the database migration job
    jobAnnotations: {}

    # Labels specific to migrate database job objects and pods
    labels: {}

    # When not set, the values defined in the global securityContext will be used
    securityContext: {}
    #  runAsUser: 50000
    #  fsGroup: 0
    #  runAsGroup: 0

    # Detailed default security context for migrateDatabaseJob for container and pod level
    securityContexts:
      pod: {}
      container: {}

    # container level lifecycle hooks
    containerLifecycleHooks: {}

    # Create ServiceAccount
    serviceAccount:
      # default value is true
      # ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
      automountServiceAccountToken: true
      # Specifies whether a ServiceAccount should be created
      create: true
      # The name of the ServiceAccount to use.
      # If not set and create is true, a name is generated using the release name
      name: ~

      # Annotations to add to migrate database job kubernetes service account.
      annotations: {}

    resources: {}
    #  limits:
    #   cpu: 100m
    #   memory: 128Mi
    #  requests:
    #   cpu: 100m
    #   memory: 128Mi

    # Launch additional containers into database migration job
    extraContainers: []

    # Add additional init containers into migrate database job (templated).
    # extraInitContainers: []

    # Mount additional volumes into database migration job. It can be templated like in the following example:
    #   extraVolumes:
    #     - name: my-templated-extra-volume
    #       secret:
    #          secretName: '{{ include "my_secret_template" . }}'
    #          defaultMode: 0640
    #          optional: true
    #
    #   extraVolumeMounts:
    #     - name: my-templated-extra-volume
    #       mountPath: "{{ .Values.my_custom_path }}"
    #       readOnly: true
    extraVolumes: []
    extraVolumeMounts: []

    nodeSelector: {}
    affinity: {}
    tolerations: []
    topologySpreadConstraints: []
    priorityClassName: ~
    # In case you need to disable the helm hooks that create the jobs after install.
    # Disable this if you are using ArgoCD for example
    useHelmHooks: true
    applyCustomEnv: true

  # rpcServer support is experimental / dev purpose only and will later be renamed
  _rpcServer:
    enabled: false

    # Labels specific to workers objects and pods
    labels: {}

    # Command to use when running the Airflow rpc server (templated).
    command:
      - "bash"
    # Args to use when running the Airflow rpc server (templated).
    args: ["-c", "exec airflow internal-api"]
    env: []
    serviceAccount:
      # default value is true
      # ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
      automountServiceAccountToken: true
      # Specifies whether a ServiceAccount should be created
      create: true
      # The name of the ServiceAccount to use.
      # If not set and create is true, a name is generated using the release name
      name: ~

      # Annotations to add to webserver kubernetes service account.
      annotations: {}
    service:
      type: ClusterIP
      ## service annotations
      annotations: {}
      ports:
        - name: rpc-server
          port: "{{ .Values.ports._rpcServer }}"

      loadBalancerIP: ~
      ## Limit load balancer source ips to list of CIDRs
      # loadBalancerSourceRanges:
      #   - "10.123.0.0/16"
      loadBalancerSourceRanges: []

    podDisruptionBudget:
      enabled: false

      # PDB configuration
      config:
        # minAvailable and maxUnavailable are mutually exclusive
        maxUnavailable: 1
        # minAvailable: 1

    # Detailed default security contexts for webserver deployments for container and pod level
    securityContexts:
      pod: {}
      container: {}

    waitForMigrations:
      # Whether to create init container to wait for db migrations
      enabled: true
      env: []
      # Detailed default security context for waitForMigrations for container level
      securityContexts:
        container: {}

    # Launch additional containers into the flower pods.
    extraContainers: []

    # Additional network policies as needed (Deprecated - renamed to `webserver.networkPolicy.ingress.from`)
    extraNetworkPolicies: []
    networkPolicy:
      ingress:
        # Peers for webserver NetworkPolicy ingress
        from: []
        # Ports for webserver NetworkPolicy ingress (if `from` is set)
        ports:
          - port: "{{ .Values.ports._rpcServer }}"

    resources: {}
    #   limits:
    #     cpu: 100m
    #     memory: 128Mi
    #   requests:
    #     cpu: 100m
    #     memory: 128Mi

    livenessProbe:
      initialDelaySeconds: 15
      timeoutSeconds: 5
      failureThreshold: 5
      periodSeconds: 10
      scheme: HTTP

    readinessProbe:
      initialDelaySeconds: 15
      timeoutSeconds: 5
      failureThreshold: 5
      periodSeconds: 10
      scheme: HTTP

    # Wait for at most 1 minute (6*10s) for the RPC server container to startup.
    # livenessProbe kicks in after the first successful startupProbe
    startupProbe:
      timeoutSeconds: 20
      failureThreshold: 6
      periodSeconds: 10
      scheme: HTTP

  # Airflow webserver settings
  webserver:
    enabled: true
    # Add custom annotations to the webserver configmap
    configMapAnnotations: {}
    #  hostAliases for the webserver pod
    hostAliases: []
    #  - ip: "127.0.0.1"
    #    hostnames:
    #      - "foo.local"
    #  - ip: "10.1.2.3"
    #    hostnames:
    #      - "foo.remote"
    allowPodLogReading: true
    livenessProbe:
      initialDelaySeconds: 15
      timeoutSeconds: 5
      failureThreshold: 5
      periodSeconds: 10
      scheme: HTTP

    readinessProbe:
      initialDelaySeconds: 15
      timeoutSeconds: 5
      failureThreshold: 5
      periodSeconds: 10
      scheme: HTTP

    # Wait for at most 1 minute (6*10s) for the webserver container to startup.
    # livenessProbe kicks in after the first successful startupProbe
    startupProbe:
      timeoutSeconds: 20
      failureThreshold: 6
      periodSeconds: 10
      scheme: HTTP

    # Number of webservers
    replicas: 1
    # Max number of old replicasets to retain
    revisionHistoryLimit: ~

    # Command to use when running the Airflow webserver (templated).
    command: ~
    # Args to use when running the Airflow webserver (templated).
    args: ["bash", "-c", "exec airflow webserver"]

    # Create ServiceAccount
    serviceAccount:
      # default value is true
      # ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
      automountServiceAccountToken: true
      # Specifies whether a ServiceAccount should be created
      create: true
      # The name of the ServiceAccount to use.
      # If not set and create is true, a name is generated using the release name
      name: ~

      # Annotations to add to webserver kubernetes service account.
      annotations: {}

    # Webserver pod disruption budget
    podDisruptionBudget:
      enabled: false

      # PDB configuration
      config:
        # minAvailable and maxUnavailable are mutually exclusive
        maxUnavailable: 1
        # minAvailable: 1

    # Allow overriding Update Strategy for Webserver
    strategy: ~

    # When not set, the values defined in the global securityContext will be used
    # (deprecated, use `securityContexts` instead)
    securityContext: {}
    #  runAsUser: 50000
    #  fsGroup: 0
    #  runAsGroup: 0

    # Detailed default security contexts for webserver deployments for container and pod level
    securityContexts:
      pod: {}
      container: {}

    # container level lifecycle hooks
    containerLifecycleHooks: {}

    # Additional network policies as needed (Deprecated - renamed to `webserver.networkPolicy.ingress.from`)
    extraNetworkPolicies: []
    networkPolicy:
      ingress:
        # Peers for webserver NetworkPolicy ingress
        from: []
        # Ports for webserver NetworkPolicy ingress (if `from` is set)
        ports:
          - port: "{{ .Values.ports.airflowUI }}"

    resources: {}
    #   limits:
    #     cpu: 100m
    #     memory: 128Mi
    #   requests:
    #     cpu: 100m
    #     memory: 128Mi

    # Create initial user.
    defaultUser:
      enabled: true
      role: Admin
      username: admin
      email: admin@example.com
      firstName: admin
      lastName: user
      password: admin

    # Launch additional containers into webserver (templated).
    extraContainers: []
    # Add additional init containers into webserver (templated).
    extraInitContainers: []

    # Mount additional volumes into webserver. It can be templated like in the following example:
    #   extraVolumes:
    #     - name: my-templated-extra-volume
    #       secret:
    #          secretName: '{{ include "my_secret_template" . }}'
    #          defaultMode: 0640
    #          optional: true
    #
    #   extraVolumeMounts:
    #     - name: my-templated-extra-volume
    #       mountPath: "{{ .Values.my_custom_path }}"
    #       readOnly: true
    extraVolumes: []
    extraVolumeMounts: []

    # This string (can be templated) will be mounted into the Airflow Webserver
    # as a custom webserver_config.py. You can bake a webserver_config.py in to
    # your image instead or specify a configmap containing the
    # webserver_config.py.
    webserverConfig: ~
    # webserverConfig: |
    #   from airflow import configuration as conf

    #   # The SQLAlchemy connection string.
    #   SQLALCHEMY_DATABASE_URI = conf.get('database', 'SQL_ALCHEMY_CONN')

    #   # Flask-WTF flag for CSRF
    #   CSRF_ENABLED = True
    webserverConfigConfigMapName: ~

    service:
      type: ClusterIP
      ## service annotations
      annotations: {}
      ports:
        - name: airflow-ui
          port: "{{ .Values.ports.airflowUI }}"
      # To change the port used to access the webserver:
      # ports:
      #   - name: airflow-ui
      #     port: 80
      #     targetPort: airflow-ui
      # To only expose a sidecar, not the webserver directly:
      # ports:
      #   - name: only_sidecar
      #     port: 80
      #     targetPort: 8888
      # If you have a public IP, set NodePort to set an external port.
      # Service type must be 'NodePort':
      # ports:
      #   - name: airflow-ui
      #     port: 8080
      #     targetPort: 8080
      #     nodePort: 31151
      loadBalancerIP: ~
      ## Limit load balancer source ips to list of CIDRs
      # loadBalancerSourceRanges:
      #   - "10.123.0.0/16"
      loadBalancerSourceRanges: []

    # Select certain nodes for airflow webserver pods.
    nodeSelector: {}
    priorityClassName: ~
    affinity: {}
    # default webserver affinity is:
    #  podAntiAffinity:
    #    preferredDuringSchedulingIgnoredDuringExecution:
    #    - podAffinityTerm:
    #        labelSelector:
    #          matchLabels:
    #            component: webserver
    #        topologyKey: kubernetes.io/hostname
    #      weight: 100
    tolerations: []
    topologySpreadConstraints: []

    # annotations for webserver deployment
    annotations: {}

    podAnnotations: {}

    # Labels specific webserver app
    labels: {}

    waitForMigrations:
      # Whether to create init container to wait for db migrations
      enabled: true
      env: []
      # Detailed default security context for waitForMigrations for container level
      securityContexts:
        container: {}

    env: []

  # Airflow Triggerer Config
  triggerer:
    enabled: true
    # Number of airflow triggerers in the deployment
    replicas: 1
    # Max number of old replicasets to retain
    revisionHistoryLimit: ~

    # Command to use when running Airflow triggerers (templated).
    command: ~
    # Args to use when running Airflow triggerer (templated).
    args: ["bash", "-c", "exec airflow triggerer"]

    # Update Strategy when triggerer is deployed as a StatefulSet
    updateStrategy: ~
    # Update Strategy when triggerer is deployed as a Deployment
    strategy:
      rollingUpdate:
        maxSurge: "100%"
        maxUnavailable: "50%"

    # If the triggerer stops heartbeating for 5 minutes (5*60s) kill the
    # triggerer and let Kubernetes restart it
    livenessProbe:
      initialDelaySeconds: 10
      timeoutSeconds: 20
      failureThreshold: 5
      periodSeconds: 60
      command: ~

    # Create ServiceAccount
    serviceAccount:
      # default value is true
      # ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
      automountServiceAccountToken: true
      # Specifies whether a ServiceAccount should be created
      create: true
      # The name of the ServiceAccount to use.
      # If not set and create is true, a name is generated using the release name
      name: ~

      # Annotations to add to triggerer kubernetes service account.
      annotations: {}

    # When not set, the values defined in the global securityContext will be used
    securityContext: {}
    #  runAsUser: 50000
    #  fsGroup: 0
    #  runAsGroup: 0

    # Detailed default security context for triggerer for container and pod level
    securityContexts:
      pod: {}
      container: {}

    # container level lifecycle hooks
    containerLifecycleHooks: {}

    persistence:
      # Enable persistent volumes
      enabled: true
      # This policy determines whether PVCs should be deleted when StatefulSet is scaled down or removed.
      # persistentVolumeClaimRetentionPolicy: ~
      # Volume size for triggerer StatefulSet
      size: 20Gi
      # If using a custom storageClass, pass name ref to all statefulSets here
      storageClassName:
      # Execute init container to chown log directory.
      # This is currently only needed in kind, due to usage
      # of local-path provisioner.
      fixPermissions: false
      # Annotations to add to triggerer volumes
      annotations: {}

    resources: {}
    #  limits:
    #   cpu: 100m
    #   memory: 128Mi
    #  requests:
    #   cpu: 100m
    #   memory: 128Mi

    # Grace period for triggerer to finish after SIGTERM is sent from kubernetes
    terminationGracePeriodSeconds: 60

    # This setting tells kubernetes that its ok to evict
    # when it wants to scale a node down.
    safeToEvict: true

    # Launch additional containers into triggerer (templated).
    extraContainers: []
    # Add additional init containers into triggerers (templated).
    extraInitContainers: []

    # Mount additional volumes into triggerer. It can be templated like in the following example:
    #   extraVolumes:
    #     - name: my-templated-extra-volume
    #       secret:
    #          secretName: '{{ include "my_secret_template" . }}'
    #          defaultMode: 0640
    #          optional: true
    #
    #   extraVolumeMounts:
    #     - name: my-templated-extra-volume
    #       mountPath: "{{ .Values.my_custom_path }}"
    #       readOnly: true
    extraVolumes: []
    extraVolumeMounts: []

    # Select certain nodes for airflow triggerer pods.
    nodeSelector: {}
    affinity: {}
    # default triggerer affinity is:
    #  podAntiAffinity:
    #    preferredDuringSchedulingIgnoredDuringExecution:
    #    - podAffinityTerm:
    #        labelSelector:
    #          matchLabels:
    #            component: triggerer
    #        topologyKey: kubernetes.io/hostname
    #      weight: 100
    tolerations: []
    topologySpreadConstraints: []

    priorityClassName: ~

    # annotations for the triggerer deployment
    annotations: {}

    podAnnotations: {}

    # Labels specific to triggerer objects and pods
    labels: {}

    logGroomerSidecar:
      # Whether to deploy the Airflow triggerer log groomer sidecar.
      enabled: true
      # Command to use when running the Airflow triggerer log groomer sidecar (templated).
      command: ~
      # Args to use when running the Airflow triggerer log groomer sidecar (templated).
      args: ["bash", "/clean-logs"]
      # Number of days to retain logs
      retentionDays: 15
      resources: {}
      #  limits:
      #   cpu: 100m
      #   memory: 128Mi
      #  requests:
      #   cpu: 100m
      #   memory: 128Mi
      # Detailed default security context for logGroomerSidecar for container level
      securityContexts:
        container: {}

      # container level lifecycle hooks
      containerLifecycleHooks: {}

    waitForMigrations:
      # Whether to create init container to wait for db migrations
      enabled: true
      env: []
      # Detailed default security context for waitForMigrations for container level
      securityContexts:
        container: {}

    env: []

    # Allow KEDA autoscaling.
    keda:
      enabled: false
      namespaceLabels: {}

      # How often KEDA polls the airflow DB to report new scale requests to the HPA
      pollingInterval: 5

      # How many seconds KEDA will wait before scaling to zero.
      # Note that HPA has a separate cooldown period for scale-downs
      cooldownPeriod: 30

      # Minimum number of triggerers created by keda
      minReplicaCount: 0

      # Maximum number of triggerers created by keda
      maxReplicaCount: 10

      # Specify HPA related options
      advanced: {}
      # horizontalPodAutoscalerConfig:
      #   behavior:
      #     scaleDown:
      #       stabilizationWindowSeconds: 300
      #       policies:
      #         - type: Percent
      #           value: 100
      #           periodSeconds: 15

      # Query to use for KEDA autoscaling. Must return a single integer.
      query: >-
        SELECT ceil(COUNT(*)::decimal / {{ .Values.config.triggerer.default_capacity }})
        FROM trigger

      # Whether to use PGBouncer to connect to the database or not when it is enabled
      # This configuration will be ignored if PGBouncer is not enabled
      # usePgbouncer: false

  # Airflow Dag Processor Config
  dagProcessor:
    enabled: false
    # Number of airflow dag processors in the deployment
    replicas: 1
    # Max number of old replicasets to retain
    revisionHistoryLimit: ~

    # Command to use when running Airflow dag processors (templated).
    command: ~
    # Args to use when running Airflow dag processor (templated).
    args: ["bash", "-c", "exec airflow dag-processor"]

    # Update Strategy for dag processors
    strategy:
      rollingUpdate:
        maxSurge: "100%"
        maxUnavailable: "50%"

    # If the dag processor stops heartbeating for 5 minutes (5*60s) kill the
    # dag processor and let Kubernetes restart it
    livenessProbe:
      initialDelaySeconds: 10
      timeoutSeconds: 20
      failureThreshold: 5
      periodSeconds: 60
      command: ~

    # Create ServiceAccount
    serviceAccount:
      # default value is true
      # ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
      automountServiceAccountToken: true
      # Specifies whether a ServiceAccount should be created
      create: true
      # The name of the ServiceAccount to use.
      # If not set and create is true, a name is generated using the release name
      name: ~

      # Annotations to add to dag processor kubernetes service account.
      annotations: {}

    # When not set, the values defined in the global securityContext will be used
    securityContext: {}
    #  runAsUser: 50000
    #  fsGroup: 0
    #  runAsGroup: 0

    # Detailed default security context for dagProcessor for container and pod level
    securityContexts:
      pod: {}
      container: {}

    # container level lifecycle hooks
    containerLifecycleHooks: {}

    resources: {}
    #  limits:
    #   cpu: 100m
    #   memory: 128Mi
    #  requests:
    #   cpu: 100m
    #   memory: 128Mi

    # Grace period for dag processor to finish after SIGTERM is sent from kubernetes
    terminationGracePeriodSeconds: 60

    # This setting tells kubernetes that its ok to evict
    # when it wants to scale a node down.
    safeToEvict: true

    # Launch additional containers into dag processor (templated).
    extraContainers: []
    # Add additional init containers into dag processors (templated).
    extraInitContainers: []

    # Mount additional volumes into dag processor. It can be templated like in the following example:
    #   extraVolumes:
    #     - name: my-templated-extra-volume
    #       secret:
    #          secretName: '{{ include "my_secret_template" . }}'
    #          defaultMode: 0640
    #          optional: true
    #
    #   extraVolumeMounts:
    #     - name: my-templated-extra-volume
    #       mountPath: "{{ .Values.my_custom_path }}"
    #       readOnly: true
    extraVolumes: []
    extraVolumeMounts: []

    # Select certain nodes for airflow dag processor pods.
    nodeSelector: {}
    affinity: {}
    # default dag processor affinity is:
    #  podAntiAffinity:
    #    preferredDuringSchedulingIgnoredDuringExecution:
    #    - podAffinityTerm:
    #        labelSelector:
    #          matchLabels:
    #            component: dag-processor
    #        topologyKey: kubernetes.io/hostname
    #      weight: 100
    tolerations: []
    topologySpreadConstraints: []

    priorityClassName: ~

    # annotations for the dag processor deployment
    annotations: {}

    podAnnotations: {}

    logGroomerSidecar:
      # Whether to deploy the Airflow dag processor log groomer sidecar.
      enabled: true
      # Command to use when running the Airflow dag processor log groomer sidecar (templated).
      command: ~
      # Args to use when running the Airflow dag processor log groomer sidecar (templated).
      args: ["bash", "/clean-logs"]
      # Number of days to retain logs
      retentionDays: 15
      resources: {}
      #  limits:
      #   cpu: 100m
      #   memory: 128Mi
      #  requests:
      #   cpu: 100m
      #   memory: 128Mi
      securityContexts:
        container: {}

    waitForMigrations:
      # Whether to create init container to wait for db migrations
      enabled: true
      env: []
      # Detailed default security context for waitForMigrations for container level
      securityContexts:
        container: {}

    env: []

  # Flower settings
  flower:
    # Enable flower.
    # If True, and using CeleryExecutor/CeleryKubernetesExecutor, will deploy flower app.
    enabled: false

    livenessProbe:
      initialDelaySeconds: 10
      timeoutSeconds: 5
      failureThreshold: 10
      periodSeconds: 5

    readinessProbe:
      initialDelaySeconds: 10
      timeoutSeconds: 5
      failureThreshold: 10
      periodSeconds: 5

    # Max number of old replicasets to retain
    revisionHistoryLimit: ~

    # Command to use when running flower (templated).
    command: ~
    # Args to use when running flower (templated).
    args:
      - "bash"
      - "-c"
      # The format below is necessary to get `helm lint` happy
      - |-
        exec \
        airflow {{ semverCompare ">=2.0.0" .Values.airflowVersion | ternary "celery flower" "flower" }}

    # Additional network policies as needed (Deprecated - renamed to `flower.networkPolicy.ingress.from`)
    extraNetworkPolicies: []
    networkPolicy:
      ingress:
        # Peers for flower NetworkPolicy ingress
        from: []
        # Ports for flower NetworkPolicy ingress (if ingressPeers is set)
        ports:
          - port: "{{ .Values.ports.flowerUI }}"

    resources: {}
    #   limits:
    #     cpu: 100m
    #     memory: 128Mi
    #   requests:
    #     cpu: 100m
    #     memory: 128Mi

    # When not set, the values defined in the global securityContext will be used
    securityContext: {}
    #  runAsUser: 50000
    #  fsGroup: 0
    #  runAsGroup: 0

    # Detailed default security context for flower for container and pod level
    securityContexts:
      pod: {}
      container: {}

    # container level lifecycle hooks
    containerLifecycleHooks: {}

    # Create ServiceAccount
    serviceAccount:
      # default value is true
      # ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
      automountServiceAccountToken: true
      # Specifies whether a ServiceAccount should be created
      create: true
      # The name of the ServiceAccount to use.
      # If not set and create is true, a name is generated using the release name
      name: ~

      # Annotations to add to worker kubernetes service account.
      annotations: {}

    # A secret containing the connection
    secretName: ~

    # Else, if username and password are set, create secret from username and password
    username: ~
    password: ~

    service:
      type: ClusterIP
      ## service annotations
      annotations: {}
      ports:
        - name: flower-ui
          port: "{{ .Values.ports.flowerUI }}"
      # To change the port used to access flower:
      # ports:
      #   - name: flower-ui
      #     port: 8080
      #     targetPort: flower-ui
      loadBalancerIP: ~
      ## Limit load balancer source ips to list of CIDRs
      # loadBalancerSourceRanges:
      #   - "10.123.0.0/16"
      loadBalancerSourceRanges: []

    # Launch additional containers into the flower pods.
    extraContainers: []
    # Mount additional volumes into the flower pods. It can be templated like in the following example:
    #   extraVolumes:
    #     - name: my-templated-extra-volume
    #       secret:
    #          secretName: '{{ include "my_secret_template" . }}'
    #          defaultMode: 0640
    #          optional: true
    #
    #   extraVolumeMounts:
    #     - name: my-templated-extra-volume
    #       mountPath: "{{ .Values.my_custom_path }}"
    #       readOnly: true
    extraVolumes: []
    extraVolumeMounts: []

    # Select certain nodes for airflow flower pods.
    nodeSelector: {}
    affinity: {}
    tolerations: []
    topologySpreadConstraints: []

    priorityClassName: ~

    # annotations for the flower deployment
    annotations: {}

    podAnnotations: {}

    # Labels specific to flower objects and pods
    labels: {}
    env: []

  # StatsD settings
  statsd:
    # Add custom annotations to the statsd configmap
    configMapAnnotations: {}

    enabled: true
    # Max number of old replicasets to retain
    revisionHistoryLimit: ~

    # Arguments for StatsD exporter command.
    args: ["--statsd.mapping-config=/etc/statsd-exporter/mappings.yml"]

    # Annotations to add to the StatsD Deployment.
    annotations: {}

    # Create ServiceAccount
    serviceAccount:
      # default value is true
      # ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
      automountServiceAccountToken: true
      # Specifies whether a ServiceAccount should be created
      create: true
      # The name of the ServiceAccount to use.
      # If not set and create is true, a name is generated using the release name
      name: ~

      # Annotations to add to worker kubernetes service account.
      annotations: {}

    uid: 65534
    # When not set, `statsd.uid` will be used

    # (deprecated, use `securityContexts` instead)
    securityContext: {}
    #  runAsUser: 65534
    #  fsGroup: 0
    #  runAsGroup: 0

    # Detailed default security context for statsd deployments for container and pod level
    securityContexts:
      pod: {}
      container: {}

    # container level lifecycle hooks
    containerLifecycleHooks: {}

    # Additional network policies as needed
    extraNetworkPolicies: []
    resources: {}
    #   limits:
    #     cpu: 100m
    #     memory: 128Mi
    #   requests:
    #     cpu: 100m
    #     memory: 128Mi

    service:
      extraAnnotations: {}

    # Select certain nodes for StatsD pods.
    nodeSelector: {}
    affinity: {}
    tolerations: []
    topologySpreadConstraints: []

    priorityClassName: ~

    # Additional mappings for StatsD exporter.
    # If set, will merge default mapping and extra mappings, default mapping has higher priority.
    # So, if you want to change some default mapping, please use `overrideMappings`
    extraMappings: []

    # Override mappings for StatsD exporter.
    # If set, will ignore setting item in default and `extraMappings`.
    # So, If you use it, ensure all mapping item contains in it.
    overrideMappings: []

    podAnnotations: {}
    env: []

  # PgBouncer settings
  pgbouncer:
    # Enable PgBouncer
    enabled: false
    # Number of PgBouncer replicas to run in Deployment
    replicas: 1
    # Max number of old replicasets to retain
    revisionHistoryLimit: ~
    # Command to use for PgBouncer(templated).
    command: ["pgbouncer", "-u", "nobody", "/etc/pgbouncer/pgbouncer.ini"]
    # Args to use for PgBouncer(templated).
    args: ~
    auth_type: scram-sha-256
    auth_file: /etc/pgbouncer/users.txt

    # annotations to be added to the PgBouncer deployment
    annotations: {}

    podAnnotations: {}

    # Create ServiceAccount
    serviceAccount:
      # default value is true
      # ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
      automountServiceAccountToken: true
      # Specifies whether a ServiceAccount should be created
      create: true
      # The name of the ServiceAccount to use.
      # If not set and create is true, a name is generated using the release name
      name: ~

      # Annotations to add to worker kubernetes service account.
      annotations: {}

    # Additional network policies as needed
    extraNetworkPolicies: []

    # Pool sizes
    metadataPoolSize: 10
    resultBackendPoolSize: 5

    # Maximum clients that can connect to PgBouncer (higher = more file descriptors)
    maxClientConn: 100

    # supply the name of existing secret with pgbouncer.ini and users.txt defined
    # you can load them to a k8s secret like the one below
    #  apiVersion: v1
    #  kind: Secret
    #  metadata:
    #    name: pgbouncer-config-secret
    #  data:
    #     pgbouncer.ini: <base64_encoded pgbouncer.ini file content>
    #     users.txt: <base64_encoded users.txt file content>
    #  type: Opaque
    #
    #  configSecretName: pgbouncer-config-secret
    #
    configSecretName: ~

    # PgBouncer pod disruption budget
    podDisruptionBudget:
      enabled: false

      # PDB configuration
      config:
        # minAvailable and maxUnavailable are mutually exclusive
        maxUnavailable: 1
        # minAvailable: 1

    # Limit the resources to PgBouncer.
    # When you specify the resource request the k8s scheduler uses this information to decide which node to
    # place the Pod on. When you specify a resource limit for a Container, the kubelet enforces those limits so
    # that the running container is not allowed to use more of that resource than the limit you set.
    # See: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    # Example:
    #
    # resource:
    #   limits:
    #     cpu: 100m
    #     memory: 128Mi
    #   requests:
    #     cpu: 100m
    #     memory: 128Mi
    resources: {}

    service:
      extraAnnotations: {}

    # https://www.pgbouncer.org/config.html
    verbose: 0
    logDisconnections: 0
    logConnections: 0

    sslmode: "prefer"
    ciphers: "normal"

    ssl:
      ca: ~
      cert: ~
      key: ~

    # Add extra PgBouncer ini configuration in the databases section:
    # https://www.pgbouncer.org/config.html#section-databases
    extraIniMetadata: ~
    extraIniResultBackend: ~
    # Add extra general PgBouncer ini configuration: https://www.pgbouncer.org/config.html
    extraIni: ~

    # Mount additional volumes into pgbouncer. It can be templated like in the following example:
    #   extraVolumes:
    #     - name: my-templated-extra-volume
    #       secret:
    #          secretName: '{{ include "my_secret_template" . }}'
    #          defaultMode: 0640
    #          optional: true
    #
    #   extraVolumeMounts:
    #     - name: my-templated-extra-volume
    #       mountPath: "{{ .Values.my_custom_path }}"
    #       readOnly: true
    extraVolumes: []
    extraVolumeMounts: []

    # Launch additional containers into pgbouncer.
    extraContainers: []

    # Select certain nodes for PgBouncer pods.
    nodeSelector: {}
    affinity: {}
    tolerations: []
    topologySpreadConstraints: []

    priorityClassName: ~

    uid: 65534

    # Detailed default security context for pgbouncer for container level
    securityContexts:
      pod: {}
      container: {}

    # container level lifecycle hooks
    containerLifecycleHooks:
      preStop:
        exec:
          # Allow existing queries clients to complete within 120 seconds
          command: ["/bin/sh", "-c", "killall -INT pgbouncer && sleep 120"]

    metricsExporterSidecar:
      resources: {}
      #  limits:
      #   cpu: 100m
      #   memory: 128Mi
      #  requests:
      #   cpu: 100m
      #   memory: 128Mi
      sslmode: "disable"

      # supply the name of existing secret with PGBouncer connection URI containing
      # stats user and password.
      # you can load them to a k8s secret like the one below
      #  apiVersion: v1
      #  kind: Secret
      #  metadata:
      #    name: pgbouncer-stats-secret
      #  data:
      #     connection: postgresql://<stats user>:<password>@127.0.0.1:6543/pgbouncer?<connection params>
      #  type: Opaque
      #
      #  statsSecretName: pgbouncer-stats-secret
      #
      statsSecretName: ~

      # Key containing the PGBouncer connection URI, defaults to `connection` if not defined
      statsSecretKey: ~

      # Detailed default security context for metricsExporterSidecar for container level
      securityContexts:
        container: {}

      # container level lifecycle hooks
      containerLifecycleHooks: {}

      livenessProbe:
        initialDelaySeconds: 10
        periodSeconds: 10
        timeoutSeconds: 1

      readinessProbe:
        initialDelaySeconds: 10
        periodSeconds: 10
        timeoutSeconds: 1

    # Environment variables to add to pgbouncer container
    env: []

  # Configuration for the redis provisioned by the chart
  redis:
    enabled: true
    terminationGracePeriodSeconds: 600

    # Annotations for Redis Statefulset
    # annotations: {}

    # Create ServiceAccount
    serviceAccount:
      # default value is true
      # ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
      automountServiceAccountToken: true
      # Specifies whether a ServiceAccount should be created
      create: true
      # The name of the ServiceAccount to use.
      # If not set and create is true, a name is generated using the release name
      name: ~

      # Annotations to add to worker kubernetes service account.
      annotations: {}

    persistence:
      # Enable persistent volumes
      enabled: true
      # Volume size for worker StatefulSet
      size: 1Gi
      # If using a custom storageClass, pass name ref to all statefulSets here
      storageClassName:
      # Annotations to add to redis volumes
      annotations: {}

    # Configuration for empty dir volume (if redis.persistence.enabled == false)
    # emptyDirConfig:
    #   sizeLimit: 1Gi
    #   medium: Memory

    resources: {}
    #  limits:
    #   cpu: 100m
    #   memory: 128Mi
    #  requests:
    #   cpu: 100m
    #   memory: 128Mi

    # If set use as redis secret. Make sure to also set data.brokerUrlSecretName value.
    passwordSecretName: ~

    # Else, if password is set, create secret with it,
    # Otherwise a new password will be generated on install
    # Note: password can only be set during install, not upgrade.
    password: ~

    # This setting tells kubernetes that its ok to evict
    # when it wants to scale a node down.
    safeToEvict: true

    # Select certain nodes for redis pods.
    nodeSelector: {}
    affinity: {}
    tolerations: []
    topologySpreadConstraints: []
    priorityClassName: ~

    # Set to 0 for backwards-compatiblity
    uid: 0
    # If not set, `redis.uid` will be used
    securityContext: {}
    #  runAsUser: 999
    #  runAsGroup: 0

    # Detailed default security context for redis for container and pod level
    securityContexts:
      pod: {}
      container: {}

    # container level lifecycle hooks
    containerLifecycleHooks: {}

    podAnnotations: {}
  # Auth secret for a private registry
  # This is used if pulling airflow images from a private registry
  registry:
    secretName: ~

    # Example:
    # connection:
    #   user: ~
    #   pass: ~
    #   host: ~
    #   email: ~
    connection: {}

  # Elasticsearch logging configuration
  elasticsearch:
    # Enable elasticsearch task logging
    enabled: false
    # A secret containing the connection
    secretName: ~
    # Or an object representing the connection
    # Example:
    # connection:
    #   scheme: ~
    #   user: ~
    #   pass: ~
    #   host: ~
    #   port: ~
    connection: {}

  # All ports used by chart
  ports:
    flowerUI: 5555
    airflowUI: 8080
    workerLogs: 8793
    triggererLogs: 8794
    redisDB: 6379
    statsdIngest: 9125
    statsdScrape: 9102
    pgbouncer: 6543
    pgbouncerScrape: 9127
    # rpcServer support is experimental / dev purpose only and will later be renamed
    _rpcServer: 9080

  # Define any ResourceQuotas for namespace
  quotas: {}

  # Define default/max/min values for pods and containers in namespace
  limits: []

  # This runs as a CronJob to cleanup old pods.
  cleanup:
    enabled: false
    # Run every 15 minutes (templated).
    schedule: "*/15 * * * *"
    # To select a random-ish, deterministic starting minute between 3 and 12 inclusive for each release:
    #     '{{- add 3 (regexFind ".$" (adler32sum .Release.Name)) -}}-59/15 * * * *'
    # To select the last digit of unix epoch time as the starting minute on each deploy:
    #     '{{- now | unixEpoch | trunc -1 -}}-59/* * * * *'

    # Command to use when running the cleanup cronjob (templated).
    command: ~
    # Args to use when running the cleanup cronjob (templated).
    args: ["bash", "-c", "exec airflow kubernetes cleanup-pods --namespace={{ .Release.Namespace }}"]

    # jobAnnotations are annotations on the cleanup CronJob
    jobAnnotations: {}

    # Select certain nodes for airflow cleanup pods.
    nodeSelector: {}
    affinity: {}
    tolerations: []
    topologySpreadConstraints: []
    priorityClassName: ~

    podAnnotations: {}

    # Labels specific to cleanup objects and pods
    labels: {}

    resources: {}
    #  limits:
    #   cpu: 100m
    #   memory: 128Mi
    #  requests:
    #   cpu: 100m
    #   memory: 128Mi

    # Create ServiceAccount
    serviceAccount:
      # default value is true
      # ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
      automountServiceAccountToken: true
      # Specifies whether a ServiceAccount should be created
      create: true
      # The name of the ServiceAccount to use.
      # If not set and create is true, a name is generated using the release name
      name: ~

      # Annotations to add to cleanup cronjob kubernetes service account.
      annotations: {}

    # When not set, the values defined in the global securityContext will be used
    securityContext: {}
    #  runAsUser: 50000
    #  runAsGroup: 0
    env: []

    # Detailed default security context for cleanup for container level
    securityContexts:
      pod: {}
      container: {}

    # container level lifecycle hooks
    containerLifecycleHooks: {}

    # Specify history limit
    # When set, overwrite the default k8s number of successful and failed CronJob executions that are saved.
    failedJobsHistoryLimit: ~
    successfulJobsHistoryLimit: ~

  # Configuration for postgresql subchart
  # Not recommended for production
  postgresql:
    enabled: true
    auth:
      enablePostgresUser: true
      postgresPassword: postgres
      username: ""
      password: ""

  # Config settings to go into the mounted airflow.cfg
  #
  # Please note that these values are passed through the `tpl` function, so are
  # all subject to being rendered as go templates. If you need to include a
  # literal `{{` in a value, it must be expressed like this:
  #
  #    a: '{{ "{{ not a template }}" }}'
  #
  # Do not set config containing secrets via plain text values, use Env Var or k8s secret object
  # yamllint disable rule:line-length
  config:
    core:
      dags_folder: '{{ include "airflow_dags" . }}'
      # This is ignored when used with the official Docker image
      load_examples: 'False'
      executor: '{{ .Values.executor }}'
      # For Airflow 1.10, backward compatibility; moved to [logging] in 2.0
      colored_console_log: 'False'
      remote_logging: '{{- ternary "True" "False" .Values.elasticsearch.enabled }}'
    logging:
      remote_logging: '{{- ternary "True" "False" .Values.elasticsearch.enabled }}'
      colored_console_log: 'False'
    metrics:
      statsd_on: '{{ ternary "True" "False" .Values.statsd.enabled }}'
      statsd_port: 9125
      statsd_prefix: airflow
      statsd_host: '{{ printf "%s-statsd" (include "airflow.fullname" .) }}'
    webserver:
      enable_proxy_fix: 'True'
      # For Airflow 1.10
      rbac: 'True'
    celery:
      flower_url_prefix: '{{ ternary "" .Values.ingress.flower.path (eq .Values.ingress.flower.path "/") }}'
      worker_concurrency: 16
    scheduler:
      standalone_dag_processor: '{{ ternary "True" "False" .Values.dagProcessor.enabled }}'
      # statsd params included for Airflow 1.10 backward compatibility; moved to [metrics] in 2.0
      statsd_on: '{{ ternary "True" "False" .Values.statsd.enabled }}'
      statsd_port: 9125
      statsd_prefix: airflow
      statsd_host: '{{ printf "%s-statsd" (include "airflow.fullname" .) }}'
      # `run_duration` included for Airflow 1.10 backward compatibility; removed in 2.0.
      run_duration: 41460
    elasticsearch:
      json_format: 'True'
      log_id_template: "{dag_id}_{task_id}_{execution_date}_{try_number}"
    elasticsearch_configs:
      max_retries: 3
      timeout: 30
      retry_timeout: 'True'
    kerberos:
      keytab: '{{ .Values.kerberos.keytabPath }}'
      reinit_frequency: '{{ .Values.kerberos.reinitFrequency }}'
      principal: '{{ .Values.kerberos.principal }}'
      ccache: '{{ .Values.kerberos.ccacheMountPath }}/{{ .Values.kerberos.ccacheFileName }}'
    celery_kubernetes_executor:
      kubernetes_queue: 'kubernetes'
    # The `kubernetes` section is deprecated in Airflow >= 2.5.0 due to an airflow.cfg schema change.
    # The `kubernetes` section can be removed once the helm chart no longer supports Airflow < 2.5.0.
    kubernetes:
      namespace: '{{ .Release.Namespace }}'
      # The following `airflow_` entries are for Airflow 1, and can be removed when it is no longer supported.
      airflow_configmap: '{{ include "airflow_config" . }}'
      airflow_local_settings_configmap: '{{ include "airflow_config" . }}'
      pod_template_file: '{{ include "airflow_pod_template_file" . }}/pod_template_file.yaml'
      worker_container_repository: '{{ .Values.images.airflow.repository | default .Values.defaultAirflowRepository }}'
      worker_container_tag: '{{ .Values.images.airflow.tag | default .Values.defaultAirflowTag }}'
      multi_namespace_mode: '{{ ternary "True" "False" .Values.multiNamespaceMode }}'
    # The `kubernetes_executor` section duplicates the `kubernetes` section in Airflow >= 2.5.0 due to an airflow.cfg schema change.
    kubernetes_executor:
      namespace: '{{ .Release.Namespace }}'
      pod_template_file: '{{ include "airflow_pod_template_file" . }}/pod_template_file.yaml'
      worker_container_repository: '{{ .Values.images.airflow.repository | default .Values.defaultAirflowRepository }}'
      worker_container_tag: '{{ .Values.images.airflow.tag | default .Values.defaultAirflowTag }}'
      multi_namespace_mode: '{{ ternary "True" "False" .Values.multiNamespaceMode }}'
    triggerer:
      default_capacity: 1000
  # yamllint enable rule:line-length

  # Whether Airflow can launch workers and/or pods in multiple namespaces
  # If true, it creates ClusterRole/ClusterRolebinding (with access to entire cluster)
  multiNamespaceMode: false

  # `podTemplate` is a templated string containing the contents of `pod_template_file.yaml` used for
  # KubernetesExecutor workers. The default `podTemplate` will use normal `workers` configuration parameters
  # (e.g. `workers.resources`). As such, you normally won't need to override this directly, however,
  # you can still provide a completely custom `pod_template_file.yaml` if desired.
  # If not set, a default one is created using `files/pod-template-file.kubernetes-helm-yaml`.
  podTemplate: ~
  # The following example is NOT functional, but meant to be illustrative of how you can provide a custom
  # `pod_template_file`. You're better off starting with the default in
  # `files/pod-template-file.kubernetes-helm-yaml` and modifying from there.
  # We will set `priorityClassName` in this example:
  # podTemplate: |
  #   apiVersion: v1
  #   kind: Pod
  #   metadata:
  #     name: placeholder-name
  #     labels:
  #       tier: airflow
  #       component: worker
  #       release: {{ .Release.Name }}
  #   spec:
  #     priorityClassName: high-priority
  #     containers:
  #       - name: base
  #         ...

  # Git sync
  dags:
    # Where dags volume will be mounted. Works for both persistence and gitSync.
    # If not specified, dags mount path will be set to $AIRFLOW_HOME/dags
    mountPath: ~
    persistence:
      # Annotations for dags PVC
      annotations: {}
      # Enable persistent volume for storing dags
      enabled: false
      # Volume size for dags
      size: 1Gi
      # If using a custom storageClass, pass name here
      storageClassName:
      # access mode of the persistent volume
      accessMode: ReadWriteOnce
      ## the name of an existing PVC to use
      existingClaim:
      ## optional subpath for dag volume mount
      subPath: ~
    gitSync:
      enabled: false

      # git repo clone url
      # ssh example: git@github.com:apache/airflow.git
      # https example: https://github.com/apache/airflow.git
      repo: https://github.com/apache/airflow.git
      branch: v2-2-stable
      rev: HEAD
      # The git revision (branch, tag, or hash) to check out, v4 only
      ref: v2-2-stable
      depth: 1
      # the number of consecutive failures allowed before aborting
      maxFailures: 0
      # subpath within the repo where dags are located
      # should be "" if dags are at repo root
      subPath: "tests/dags"
      # if your repo needs a user name password
      # you can load them to a k8s secret like the one below
      #   ---
      #   apiVersion: v1
      #   kind: Secret
      #   metadata:
      #     name: git-credentials
      #   data:
      #     # For git-sync v3
      #     GIT_SYNC_USERNAME: <base64_encoded_git_username>
      #     GIT_SYNC_PASSWORD: <base64_encoded_git_password>
      #     # For git-sync v4
      #     GITSYNC_USERNAME: <base64_encoded_git_username>
      #     GITSYNC_PASSWORD: <base64_encoded_git_password>
      # and specify the name of the secret below
      #
      # credentialsSecret: git-credentials
      #
      #
      # If you are using an ssh clone url, you can load
      # the ssh private key to a k8s secret like the one below
      #   ---
      #   apiVersion: v1
      #   kind: Secret
      #   metadata:
      #     name: airflow-ssh-secret
      #   data:
      #     # key needs to be gitSshKey
      #     gitSshKey: <base64_encoded_data>
      # and specify the name of the secret below
      # sshKeySecret: airflow-ssh-secret
      #
      # Or set sshKeySecret with your key
      # sshKey: |-
      #   -----BEGIN {OPENSSH PRIVATE KEY}-----
      #   ...
      #   -----END {OPENSSH PRIVATE KEY}-----
      #
      # If you are using an ssh private key, you can additionally
      # specify the content of your known_hosts file, example:
      #
      # knownHosts: |
      #    <host1>,<ip1> <key1>
      #    <host2>,<ip2> <key2>

      # interval between git sync attempts in seconds
      # high values are more likely to cause DAGs to become out of sync between different components
      # low values cause more traffic to the remote git repository
      # Go-style duration string (e.g. "100ms" or "0.1s" = 100ms).
      # For backwards compatibility, wait will be used if it is specified.
      period: 5s
      wait: ~

      containerName: git-sync
      uid: 65533

      # When not set, the values defined in the global securityContext will be used
      securityContext: {}
      #  runAsUser: 65533
      #  runAsGroup: 0

      securityContexts:
        container: {}

      # container level lifecycle hooks
      containerLifecycleHooks: {}

      # Mount additional volumes into git-sync. It can be templated like in the following example:
      #   extraVolumeMounts:
      #     - name: my-templated-extra-volume
      #       mountPath: "{{ .Values.my_custom_path }}"
      #       readOnly: true
      extraVolumeMounts: []
      env: []
      # Supported env vars for gitsync can be found at https://github.com/kubernetes/git-sync
      # - name: ""
      #   value: ""

      # Configuration for empty dir volume
      # emptyDirConfig:
      #   sizeLimit: 1Gi
      #   medium: Memory

      resources: {}
      #  limits:
      #   cpu: 100m
      #   memory: 128Mi
      #  requests:
      #   cpu: 100m
      #   memory: 128Mi

  logs:
    # Configuration for empty dir volume (if logs.persistence.enabled == false)
    # emptyDirConfig:
    #   sizeLimit: 1Gi
    #   medium: Memory

    persistence:
      # Enable persistent volume for storing logs
      enabled: false
      # Volume size for logs
      size: 100Gi
      # Annotations for the logs PVC
      annotations: {}
      # If using a custom storageClass, pass name here
      storageClassName:
      ## the name of an existing PVC to use
      existingClaim:

mongodb:
  enabled: true
  # Copyright Broadcom, Inc. All Rights Reserved.
  # SPDX-License-Identifier: APACHE-2.0

  ## @section Global parameters
  ## Global Docker image parameters
  ## Please, note that this will override the image parameters, including dependencies, configured to use the global value
  ## Current available global Docker image parameters: imageRegistry, imagePullSecrets and storageClass
  ##

  ## @param global.imageRegistry Global Docker image registry
  ## @param global.imagePullSecrets Global Docker registry secret names as an array
  ## @param global.defaultStorageClass Global default StorageClass for Persistent Volume(s)
  ## @param global.storageClass DEPRECATED: use global.defaultStorageClass instead
  ## @param global.namespaceOverride Override the namespace for resource deployed by the chart, but can itself be overridden by the local namespaceOverride
  ##
  global:
    imageRegistry: ""
    ## E.g.
    ## imagePullSecrets:
    ##   - myRegistryKeySecretName
    ##
    imagePullSecrets: []
    defaultStorageClass: ""
    storageClass: csi-cinder-high-speed
    namespaceOverride: ""
    ## Compatibility adaptations for Kubernetes platforms
    ##
    compatibility:
      ## Compatibility adaptations for Openshift
      ##
      openshift:
        ## @param global.compatibility.openshift.adaptSecurityContext Adapt the securityContext sections of the deployment to make them compatible with Openshift restricted-v2 SCC: remove runAsUser, runAsGroup and fsGroup and let the platform use their allowed default IDs. Possible values: auto (apply if the detected running cluster is Openshift), force (perform the adaptation always), disabled (do not perform adaptation)
        ##
        adaptSecurityContext: auto
  ## @section Common parameters
  ##

  ## @param nameOverride String to partially override mongodb.fullname template (will maintain the release name)
  ##
  nameOverride: mongodb
  ## @param fullnameOverride String to fully override mongodb.fullname template
  ##
  fullnameOverride: ""
  ## @param namespaceOverride String to fully override common.names.namespace
  ##
  namespaceOverride: ""
  ## @param kubeVersion Force target Kubernetes version (using Helm capabilities if not set)
  ##
  kubeVersion: ""
  ## @param clusterDomain Default Kubernetes cluster domain
  ##
  clusterDomain: cluster.local
  ## @param extraDeploy Array of extra objects to deploy with the release
  ## extraDeploy:
  ## This needs to be uncommented and added to 'extraDeploy' in order to use the replicaset 'mongo-labeler' sidecar
  ## for dynamically discovering the mongodb primary pod
  ## suggestion is to use a hard-coded and predictable TCP port for the primary mongodb pod (here is 30001, choose your own)
  ## - apiVersion: v1
  ##   kind: Service
  ##   metadata:
  ##     name: mongodb-primary
  ##     namespace: the-mongodb-namespace
  ##     labels:
  ##       app.kubernetes.io/component: mongodb
  ##       app.kubernetes.io/instance: mongodb
  ##       app.kubernetes.io/managed-by: Helm
  ##       app.kubernetes.io/name: mongodb
  ##   spec:
  ##     type: NodePort
  ##     externalTrafficPolicy: Cluster
  ##     ports:
  ##       - name: mongodb
  ##         port: 30001
  ##         nodePort: 30001
  ##         protocol: TCP
  ##         targetPort: mongodb
  ##     selector:
  ##       app.kubernetes.io/component: mongodb
  ##       app.kubernetes.io/instance: mongodb
  ##       app.kubernetes.io/name: mongodb
  ##       primary: "true"
  ##
  extraDeploy: []
  ## @param commonLabels Add labels to all the deployed resources (sub-charts are not considered). Evaluated as a template
  ##
  commonLabels: {}
  ## @param commonAnnotations Common annotations to add to all Mongo resources (sub-charts are not considered). Evaluated as a template
  ##
  commonAnnotations: {}
  ## @param topologyKey Override common lib default topology key. If empty - "kubernetes.io/hostname" is used
  ## i.e. topologyKey: topology.kubernetes.io/zone
  ##
  topologyKey: ""
  ## @param serviceBindings.enabled Create secret for service binding (Experimental)
  ## Ref: https://servicebinding.io/service-provider/
  ##
  serviceBindings:
    enabled: false
  ## @param enableServiceLinks Whether information about services should be injected into pod's environment variable
  ## The environment variables injected by service links are not used, but can lead to slow boot times or slow running of the scripts when there are many services in the current namespace.
  ## If you experience slow pod startups or slow running of the scripts you probably want to set this to `false`.
  ##
  enableServiceLinks: true
  ## Enable diagnostic mode in the deployment
  ##
  diagnosticMode:
    ## @param diagnosticMode.enabled Enable diagnostic mode (all probes will be disabled and the command will be overridden)
    ##
    enabled: false
    ## @param diagnosticMode.command Command to override all containers in the deployment
    ##
    command:
      - sleep
    ## @param diagnosticMode.args Args to override all containers in the deployment
    ##
    args:
      - infinity
  ## @section MongoDB(&reg;) parameters
  ##

  ## Bitnami MongoDB(&reg;) image
  ## ref: https://hub.docker.com/r/bitnami/mongodb/tags/
  ## @param image.registry [default: REGISTRY_NAME] MongoDB(&reg;) image registry
  ## @param image.repository [default: REPOSITORY_NAME/mongodb] MongoDB(&reg;) image registry
  ## @skip image.tag MongoDB(&reg;) image tag (immutable tags are recommended)
  ## @param image.digest MongoDB(&reg;) image digest in the way sha256:aa.... Please note this parameter, if set, will override the tag
  ## @param image.pullPolicy MongoDB(&reg;) image pull policy
  ## @param image.pullSecrets Specify docker-registry secret names as an array
  ## @param image.debug Set to true if you would like to see extra information on logs
  ##
  image:
    registry: docker.io
    repository: bitnami/mongodb
    tag: 7.0.12-debian-12-r0
    digest: ""
    ## Specify a imagePullPolicy
    ## ref: https://kubernetes.io/docs/concepts/containers/images/#pre-pulled-images
    ##
    pullPolicy: IfNotPresent
    ## Optionally specify an array of imagePullSecrets.
    ## Secrets must be manually created in the namespace.
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
    ## e.g:
    ## pullSecrets:
    ##   - myRegistryKeySecretName
    ##
    pullSecrets: []
    ## Set to true if you would like to see extra information on logs
    ##
    debug: false
  ## @param schedulerName Name of the scheduler (other than default) to dispatch pods
  ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
  ##
  schedulerName: ""
  ## @param architecture MongoDB(&reg;) architecture (`standalone` or `replicaset`)
  ##
  architecture: standalone
  ## @param useStatefulSet Set to true to use a StatefulSet instead of a Deployment (only when `architecture=standalone`)
  ##
  useStatefulSet: true
  ## MongoDB(&reg;) Authentication parameters
  ##
  auth:
    ## @param auth.enabled Enable authentication
    ## ref: https://docs.mongodb.com/manual/tutorial/enable-authentication/
    ##
    enabled: true
    ## @param auth.rootUser MongoDB(&reg;) root user
    ##
    rootUser: root
    ## @param auth.rootPassword MongoDB(&reg;) root password
    ## ref: https://github.com/bitnami/containers/tree/main/bitnami/mongodb#setting-the-root-user-and-password-on-first-run
    ##
    rootPassword: "root"
    ## MongoDB(&reg;) custom users and databases
    ## ref: https://github.com/bitnami/containers/tree/main/bitnami/mongodb#creating-a-user-and-database-on-first-run
    ## @param auth.usernames List of custom users to be created during the initialization
    ## @param auth.passwords List of passwords for the custom users set at `auth.usernames`
    ## @param auth.databases List of custom databases to be created during the initialization
    ##
    usernames: []
    passwords: []
    databases: []
    ## @param auth.username DEPRECATED: use `auth.usernames` instead
    ## @param auth.password DEPRECATED: use `auth.passwords` instead
    ## @param auth.database DEPRECATED: use `auth.databases` instead
    ##
    username: ""
    password: ""
    database: ""
    ## @param auth.replicaSetKey Key used for authentication in the replicaset (only when `architecture=replicaset`)
    ##
    replicaSetKey: ""
    ## @param auth.existingSecret Existing secret with MongoDB(&reg;) credentials (keys: `mongodb-passwords`, `mongodb-root-password`, `mongodb-metrics-password`, `mongodb-replica-set-key`)
    ## NOTE: When it's set the previous parameters are ignored.
    ##
    existingSecret: ""
  tls:
    ## @param tls.enabled Enable MongoDB(&reg;) TLS support between nodes in the cluster as well as between mongo clients and nodes
    ##
    enabled: false
    mTLS:
      ## @param tls.mTLS.enabled IF TLS support is enabled, require clients to provide certificates
      enabled: true
    ## @param tls.autoGenerated Generate a custom CA and self-signed certificates
    ##
    autoGenerated: true
    ## @param tls.existingSecret Existing secret with TLS certificates (keys: `mongodb-ca-cert`, `mongodb-ca-key`)
    ## NOTE: When it's set it will disable secret creation.
    ##
    existingSecret: ""
    ## Add Custom CA certificate
    ## @param tls.caCert Custom CA certificated (base64 encoded)
    ## @param tls.caKey CA certificate private key (base64 encoded)
    ##
    caCert: ""
    caKey: ""
    ## @param tls.pemChainIncluded Flag to denote that the Certificate Authority (CA) certificates are bundled with the endpoint cert.
    ## Certificates must be in proper order, where the top certificate is the leaf and the bottom certificate is the top-most intermediate CA.
    ##
    pemChainIncluded: false
    standalone:
      ## @param tls.standalone.existingSecret Existing secret with TLS certificates (`tls.key`, `tls.crt`, `ca.crt`) or (`tls.key`, `tls.crt`) with tls.pemChainIncluded set as enabled.
      ## NOTE: When it's set it will disable certificate self-generation from existing CA.
      ##
      existingSecret: ""
    replicaset:
      ## @param tls.replicaset.existingSecrets Array of existing secrets with TLS certificates (`tls.key`, `tls.crt`, `ca.crt`) or (`tls.key`, `tls.crt`) with tls.pemChainIncluded set as enabled.
      ## existingSecrets:
      ##  - "mySecret-0"
      ##  - "mySecret-1"
      ## NOTE: When it's set it will disable certificate self-generation from existing CA.
      ##
      existingSecrets: []
    hidden:
      ## @param tls.hidden.existingSecrets Array of existing secrets with TLS certificates (`tls.key`, `tls.crt`, `ca.crt`) or (`tls.key`, `tls.crt`) with tls.pemChainIncluded set as enabled.
      ## existingSecrets:
      ##  - "mySecret-0"
      ##  - "mySecret-1"
      ## NOTE: When it's set it will disable certificate self-generation from existing CA.
      ##
      existingSecrets: []
    arbiter:
      ## @param tls.arbiter.existingSecret Existing secret with TLS certificates (`tls.key`, `tls.crt`, `ca.crt`) or (`tls.key`, `tls.crt`) with tls.pemChainIncluded set as enabled.
      ## NOTE: When it's set it will disable certificate self-generation from existing CA.
      ##
      existingSecret: ""
    ## Bitnami Nginx image
    ## @param tls.image.registry [default: REGISTRY_NAME] Init container TLS certs setup image registry
    ## @param tls.image.repository [default: REPOSITORY_NAME/nginx] Init container TLS certs setup image repository
    ## @skip tls.image.tag Init container TLS certs setup image tag (immutable tags are recommended)
    ## @param tls.image.digest Init container TLS certs setup image digest in the way sha256:aa.... Please note this parameter, if set, will override the tag
    ## @param tls.image.pullPolicy Init container TLS certs setup image pull policy
    ## @param tls.image.pullSecrets Init container TLS certs specify docker-registry secret names as an array
    ## @param tls.extraDnsNames Add extra dns names to the CA, can solve x509 auth issue for pod clients
    ##
    image:
      registry: docker.io
      repository: bitnami/nginx
      tag: 1.27.0-debian-12-r2
      digest: ""
      pullPolicy: IfNotPresent
      ## Optionally specify an array of imagePullSecrets.
      ## Secrets must be manually created in the namespace.
      ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
      ## e.g:
      ## pullSecrets:
      ##   - myRegistryKeySecretName
      ##
      pullSecrets: []
    ## e.g:
    ## extraDnsNames
    ##   "DNS.6": "$my_host"
    ##   "DNS.7": "$test"
    ##
    extraDnsNames: []
    ## @param tls.mode Allows to set the tls mode which should be used when tls is enabled (options: `allowTLS`, `preferTLS`, `requireTLS`)
    ##
    mode: requireTLS
    ## Init Container resource requests and limits
    ## ref: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
    ## We usually recommend not to specify default resources and to leave this as a conscious
    ## choice for the user. This also increases chances charts run on environments with little
    ## resources, such as Minikube. If you do want to specify resources, uncomment the following
    ## lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    ## @param tls.resourcesPreset Set container resources according to one common preset (allowed values: none, nano, micro, small, medium, large, xlarge, 2xlarge). This is ignored if tls.resources is set (tls.resources is recommended for production).
    ## More information: https://github.com/bitnami/charts/blob/main/bitnami/common/templates/_resources.tpl#L15
    ##
    resourcesPreset: "nano"
    ## @param tls.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
    ## Example:
    ## resources:
    ##   requests:
    ##     cpu: 2
    ##     memory: 512Mi
    ##   limits:
    ##     cpu: 3
    ##     memory: 1024Mi
    ##
    resources: {}
    ## Init Container securityContext 
    ## ref: https://kubernetes.io/docs/concepts/security/pod-security-policy/
    ## @param tls.securityContext Init container generate-tls-cert Security context
    ##
    securityContext: {}
    ## Example:
    ## allowPrivilegeEscalation: false
    ## capabilities:
    ##   drop: ["ALL"]
    ##
  ## @param automountServiceAccountToken Mount Service Account token in pod
  ##
  automountServiceAccountToken: false
  ## @param hostAliases Add deployment host aliases
  ## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/
  ##
  hostAliases: []
  ## @param replicaSetName Name of the replica set (only when `architecture=replicaset`)
  ## Ignored when mongodb.architecture=standalone
  ##
  replicaSetName: rs0
  ## @param replicaSetHostnames Enable DNS hostnames in the replicaset config (only when `architecture=replicaset`)
  ## Ignored when mongodb.architecture=standalone
  ## Ignored when externalAccess.enabled=true
  ##
  replicaSetHostnames: true
  ## @param enableIPv6 Switch to enable/disable IPv6 on MongoDB(&reg;)
  ## ref: https://github.com/bitnami/containers/tree/main/bitnami/mongodb#enablingdisabling-ipv6
  ##
  enableIPv6: false
  ## @param directoryPerDB Switch to enable/disable DirectoryPerDB on MongoDB(&reg;)
  ## ref: https://github.com/bitnami/containers/tree/main/bitnami/mongodb#enablingdisabling-directoryperdb
  ##
  directoryPerDB: false
  ## MongoDB(&reg;) System Log configuration
  ## ref: https://github.com/bitnami/containers/tree/main/bitnami/mongodb#configuring-system-log-verbosity-level
  ## @param systemLogVerbosity MongoDB(&reg;) system log verbosity level
  ## @param disableSystemLog Switch to enable/disable MongoDB(&reg;) system log
  ##
  systemLogVerbosity: 0
  disableSystemLog: false
  ## @param disableJavascript Switch to enable/disable MongoDB(&reg;) server-side JavaScript execution
  ## ref: https://docs.mongodb.com/manual/core/server-side-javascript/
  ##
  disableJavascript: false
  ## @param enableJournal Switch to enable/disable MongoDB(&reg;) Journaling
  ## ref: https://docs.mongodb.com/manual/reference/configuration-options/#mongodb-setting-storage.journal.enabled
  ##
  enableJournal: true
  ## @param configuration MongoDB(&reg;) configuration file to be used for Primary and Secondary nodes
  ## For documentation of all options, see: http://docs.mongodb.org/manual/reference/configuration-options/
  ## Example:
  ## configuration: |-
  ##   # where and how to store data.
  ##   storage:
  ##     dbPath: /bitnami/mongodb/data/db
  ##     journal:
  ##       enabled: true
  ##     directoryPerDB: false
  ##   # where to write logging data
  ##   systemLog:
  ##     destination: file
  ##     quiet: false
  ##     logAppend: true
  ##     logRotate: reopen
  ##     path: /opt/bitnami/mongodb/logs/mongodb.log
  ##     verbosity: 0
  ##   # network interfaces
  ##   net:
  ##     port: 27017
  ##     unixDomainSocket:
  ##       enabled: true
  ##       pathPrefix: /opt/bitnami/mongodb/tmp
  ##     ipv6: false
  ##     bindIpAll: true
  ##   # replica set options
  ##   #replication:
  ##     #replSetName: replicaset
  ##     #enableMajorityReadConcern: true
  ##   # process management options
  ##   processManagement:
  ##      fork: false
  ##      pidFilePath: /opt/bitnami/mongodb/tmp/mongodb.pid
  ##   # set parameter options
  ##   setParameter:
  ##      enableLocalhostAuthBypass: true
  ##   # security options
  ##   security:
  ##     authorization: disabled
  ##     #keyFile: /opt/bitnami/mongodb/conf/keyfile
  ##
  configuration: ""
  ## @section replicaSetConfigurationSettings settings applied during runtime (not via configuration file)
  ## If enabled, these are applied by a script which is called within setup.sh
  ## for documentation see https://docs.mongodb.com/manual/reference/replica-configuration/#replica-set-configuration-fields
  ## @param replicaSetConfigurationSettings.enabled Enable MongoDB(&reg;) Switch to enable/disable configuring MongoDB(&reg;) run time rs.conf settings
  ## @param replicaSetConfigurationSettings.configuration run-time rs.conf settings
  ##
  replicaSetConfigurationSettings:
    enabled: false
    configuration: {}
  ## Custom configurations for individual replica set members.
  ## Use the prefix 'members[X].' to apply settings to the member X of the replica set.
  ## Example: 'members[0].priority: 3' sets the priority of the first replica set member to 3.
  ## The index X in 'members[X]' corresponds to the member's position in the replica set.
  ##    members[0].priority: 3
  ##    chainingAllowed : false
  ##    heartbeatTimeoutSecs : 10
  ##    heartbeatIntervalMillis : 2000
  ##    electionTimeoutMillis : 10000
  ##    catchUpTimeoutMillis : 30000
  ## @param existingConfigmap Name of existing ConfigMap with MongoDB(&reg;) configuration for Primary and Secondary nodes
  ## NOTE: When it's set the arbiter.configuration parameter is ignored
  ##
  existingConfigmap: ""
  ## @param initdbScripts Dictionary of initdb scripts
  ## Specify dictionary of scripts to be run at first boot
  ## Example:
  ## initdbScripts:
  ##   my_init_script.sh: |
  ##      #!/bin/bash
  ##      echo "Do something."
  ##
  initdbScripts: {}
  ## @param initdbScriptsConfigMap Existing ConfigMap with custom initdb scripts
  ##
  initdbScriptsConfigMap: ""
  ## Command and args for running the container (set to default if not set). Use array form
  ## @param command Override default container command (useful when using custom images)
  ## @param args Override default container args (useful when using custom images)
  ##
  command: []
  args: []
  ## @param extraFlags MongoDB(&reg;) additional command line flags
  ## Example:
  ## extraFlags:
  ##  - "--wiredTigerCacheSizeGB=2"
  ##
  extraFlags: []
  ## @param extraEnvVars Extra environment variables to add to MongoDB(&reg;) pods
  ## E.g:
  ## extraEnvVars:
  ##   - name: FOO
  ##     value: BAR
  ##
  extraEnvVars: []
  ## @param extraEnvVarsCM Name of existing ConfigMap containing extra env vars
  ##
  extraEnvVarsCM: ""
  ## @param extraEnvVarsSecret Name of existing Secret containing extra env vars (in case of sensitive data)
  ##
  extraEnvVarsSecret: ""
  ## @section MongoDB(&reg;) statefulset parameters
  ##

  ## @param annotations Additional labels to be added to the MongoDB(&reg;) statefulset. Evaluated as a template
  ##
  annotations: {}
  ## @param labels Annotations to be added to the MongoDB(&reg;) statefulset. Evaluated as a template
  ##
  labels: {}
  ## @param replicaCount Number of MongoDB(&reg;) nodes
  ## When `mongodb.architecture=replicaset`, the number of replicas is taken in account
  ## When `mongodb.architecture=standalone`, the number of replicas can only be 0 or 1 (value higher then 1 will not be taken in account)
  ##
  replicaCount: 2
  ## @param updateStrategy.type Strategy to use to replace existing MongoDB(&reg;) pods. When architecture=standalone and useStatefulSet=false,
  ## this parameter will be applied on a deployment object. In other case it will be applied on a statefulset object
  ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies
  ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#strategy
  ## Example:
  ## updateStrategy:
  ##  type: RollingUpdate
  ##  rollingUpdate:
  ##    maxSurge: 25%
  ##    maxUnavailable: 25%
  ##
  updateStrategy:
    type: RollingUpdate
  ## @param podManagementPolicy Pod management policy for MongoDB(&reg;)
  ## Should be initialized one by one when building the replicaset for the first time
  ##
  podManagementPolicy: OrderedReady
  ## @param podAffinityPreset MongoDB(&reg;) Pod affinity preset. Ignored if `affinity` is set. Allowed values: `soft` or `hard`
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  ##
  podAffinityPreset: ""
  ## @param podAntiAffinityPreset MongoDB(&reg;) Pod anti-affinity preset. Ignored if `affinity` is set. Allowed values: `soft` or `hard`
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  ##
  podAntiAffinityPreset: soft
  ## Node affinity preset
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
  ##
  nodeAffinityPreset:
    ## @param nodeAffinityPreset.type MongoDB(&reg;) Node affinity preset type. Ignored if `affinity` is set. Allowed values: `soft` or `hard`
    ##
    type: ""
    ## @param nodeAffinityPreset.key MongoDB(&reg;) Node label key to match Ignored if `affinity` is set.
    ## E.g.
    ## key: "kubernetes.io/e2e-az-name"
    ##
    key: ""
    ## @param nodeAffinityPreset.values MongoDB(&reg;) Node label values to match. Ignored if `affinity` is set.
    ## E.g.
    ## values:
    ##   - e2e-az1
    ##   - e2e-az2
    ##
    values: []
  ## @param affinity MongoDB(&reg;) Affinity for pod assignment
  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  ## Note: podAffinityPreset, podAntiAffinityPreset, and nodeAffinityPreset will be ignored when it's set
  ##
  affinity: {}
  ## @param nodeSelector MongoDB(&reg;) Node labels for pod assignment
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/
  ##
  nodeSelector: {}
  ## @param tolerations MongoDB(&reg;) Tolerations for pod assignment
  ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  ##
  tolerations: []
  ## @param topologySpreadConstraints MongoDB(&reg;) Spread Constraints for Pods
  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/
  ##
  topologySpreadConstraints: []
  ## @param lifecycleHooks LifecycleHook for the MongoDB(&reg;) container(s) to automate configuration before or after startup
  ##
  lifecycleHooks: {}
  ## @param terminationGracePeriodSeconds MongoDB(&reg;) Termination Grace Period
  ##
  terminationGracePeriodSeconds: ""
  ## @param podLabels MongoDB(&reg;) pod labels
  ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
  ##
  podLabels: {}
  ## @param podAnnotations MongoDB(&reg;) Pod annotations
  ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
  ##
  podAnnotations: {}
  ## @param priorityClassName Name of the existing priority class to be used by MongoDB(&reg;) pod(s)
  ## ref: https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/
  ##
  priorityClassName: ""
  ## @param runtimeClassName Name of the runtime class to be used by MongoDB(&reg;) pod(s)
  ## ref: https://kubernetes.io/docs/concepts/containers/runtime-class/
  ##
  runtimeClassName: ""
  ## MongoDB(&reg;) pods' Security Context.
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
  ## @param podSecurityContext.enabled Enable MongoDB(&reg;) pod(s)' Security Context
  ## @param podSecurityContext.fsGroupChangePolicy Set filesystem group change policy
  ## @param podSecurityContext.supplementalGroups Set filesystem extra groups
  ## @param podSecurityContext.fsGroup Group ID for the volumes of the MongoDB(&reg;) pod(s)
  ## @param podSecurityContext.sysctls sysctl settings of the MongoDB(&reg;) pod(s)'
  ##
  podSecurityContext:
    enabled: true
    fsGroupChangePolicy: Always
    supplementalGroups: []
    fsGroup: 1001
    ## sysctl settings
    ## Example:
    ## sysctls:
    ## - name: net.core.somaxconn
    ##   value: "10000"
    ##
    sysctls: []
  ## MongoDB(&reg;) containers' Security Context (main and metrics container).
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container
  ## @param containerSecurityContext.enabled Enabled containers' Security Context
  ## @param containerSecurityContext.seLinuxOptions [object,nullable] Set SELinux options in container
  ## @param containerSecurityContext.runAsUser Set containers' Security Context runAsUser
  ## @param containerSecurityContext.runAsGroup Set containers' Security Context runAsGroup
  ## @param containerSecurityContext.runAsNonRoot Set container's Security Context runAsNonRoot
  ## @param containerSecurityContext.privileged Set container's Security Context privileged
  ## @param containerSecurityContext.readOnlyRootFilesystem Set container's Security Context readOnlyRootFilesystem
  ## @param containerSecurityContext.allowPrivilegeEscalation Set container's Security Context allowPrivilegeEscalation
  ## @param containerSecurityContext.capabilities.drop List of capabilities to be dropped
  ## @param containerSecurityContext.seccompProfile.type Set container's Security Context seccomp profile
  ##
  containerSecurityContext:
    enabled: true
    seLinuxOptions: {}
    runAsUser: 1001
    runAsGroup: 1001
    runAsNonRoot: true
    privileged: false
    readOnlyRootFilesystem: true
    allowPrivilegeEscalation: false
    capabilities:
      drop: ["ALL"]
    seccompProfile:
      type: "RuntimeDefault"
  ## MongoDB(&reg;) containers' resource requests and limits.
  ## ref: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
  ## We usually recommend not to specify default resources and to leave this as a conscious
  ## choice for the user. This also increases chances charts run on environments with little
  ## resources, such as Minikube. If you do want to specify resources, uncomment the following
  ## lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  ## @param resourcesPreset Set container resources according to one common preset (allowed values: none, nano, micro, small, medium, large, xlarge, 2xlarge). This is ignored if resources is set (resources is recommended for production).
  ## More information: https://github.com/bitnami/charts/blob/main/bitnami/common/templates/_resources.tpl#L15
  ##
  resourcesPreset: "small"
  ## @param resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
  ## Example:
  ## resources:
  ##   requests:
  ##     cpu: 2
  ##     memory: 512Mi
  ##   limits:
  ##     cpu: 3
  ##     memory: 1024Mi
  ##
  resources: {}
  ## @param containerPorts.mongodb MongoDB(&reg;) container port
  ##
  containerPorts:
    mongodb: 27017
  ## MongoDB(&reg;) pods' liveness probe. Evaluated as a template.
  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes
  ## @param livenessProbe.enabled Enable livenessProbe
  ## @param livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe
  ## @param livenessProbe.periodSeconds Period seconds for livenessProbe
  ## @param livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
  ## @param livenessProbe.failureThreshold Failure threshold for livenessProbe
  ## @param livenessProbe.successThreshold Success threshold for livenessProbe
  ##
  livenessProbe:
    enabled: true
    initialDelaySeconds: 30
    periodSeconds: 20
    timeoutSeconds: 10
    failureThreshold: 6
    successThreshold: 1
  ## MongoDB(&reg;) pods' readiness probe. Evaluated as a template.
  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes
  ## @param readinessProbe.enabled Enable readinessProbe
  ## @param readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe
  ## @param readinessProbe.periodSeconds Period seconds for readinessProbe
  ## @param readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
  ## @param readinessProbe.failureThreshold Failure threshold for readinessProbe
  ## @param readinessProbe.successThreshold Success threshold for readinessProbe
  ##
  readinessProbe:
    enabled: true
    initialDelaySeconds: 5
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 6
    successThreshold: 1
  ## Slow starting containers can be protected through startup probes
  ## Startup probes are available in Kubernetes version 1.16 and above
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#define-startup-probes
  ## @param startupProbe.enabled Enable startupProbe
  ## @param startupProbe.initialDelaySeconds Initial delay seconds for startupProbe
  ## @param startupProbe.periodSeconds Period seconds for startupProbe
  ## @param startupProbe.timeoutSeconds Timeout seconds for startupProbe
  ## @param startupProbe.failureThreshold Failure threshold for startupProbe
  ## @param startupProbe.successThreshold Success threshold for startupProbe
  ##
  startupProbe:
    enabled: false
    initialDelaySeconds: 5
    periodSeconds: 20
    timeoutSeconds: 10
    successThreshold: 1
    failureThreshold: 30
  ## @param customLivenessProbe Override default liveness probe for MongoDB(&reg;) containers
  ## Ignored when livenessProbe.enabled=true
  ##
  customLivenessProbe: {}
  ## @param customReadinessProbe Override default readiness probe for MongoDB(&reg;) containers
  ## Ignored when readinessProbe.enabled=true
  ##
  customReadinessProbe: {}
  ## @param customStartupProbe Override default startup probe for MongoDB(&reg;) containers
  ## Ignored when startupProbe.enabled=true
  ##
  customStartupProbe: {}
  ## @param initContainers Add additional init containers for the hidden node pod(s)
  ## Example:
  ## initContainers:
  ##   - name: your-image-name
  ##     image: your-image
  ##     imagePullPolicy: Always
  ##     ports:
  ##       - name: portname
  ##         containerPort: 1234
  ##
  initContainers: []
  ## @param sidecars Add additional sidecar containers for the MongoDB(&reg;) pod(s)
  ## Example:
  ## sidecars:
  ##   - name: your-image-name
  ##     image: your-image
  ##     imagePullPolicy: Always
  ##     ports:
  ##       - name: portname
  ##         containerPort: 1234
  ## This is an optional 'mongo-labeler' sidecar container that tracks replica-set for the primary mongodb pod
  ## and labels it dynamically with ' primary: "true" ' in order for an extra-deployed service to always expose
  ## and attach to the primary pod, this needs to be uncommented along with the suggested 'extraDeploy' example
  ## and the suggested rbac example for the pod to be allowed adding labels to mongo replica pods
  ## search 'mongo-labeler' through this file to find the sections that needs to be uncommented to make it work
  ##
  ## - name: mongo-labeler
  ##   image: korenlev/k8s-mongo-labeler-sidecar
  ##   imagePullPolicy: Always
  ##   env:
  ##     - name: LABEL_SELECTOR
  ##       value: "app.kubernetes.io/component=mongodb,app.kubernetes.io/instance=mongodb,app.kubernetes.io/name=mongodb"
  ##     - name: NAMESPACE
  ##       value: "the-mongodb-namespace"
  ##     - name: DEBUG
  ##       value: "true"
  ##
  sidecars: []
  ## @param extraVolumeMounts Optionally specify extra list of additional volumeMounts for the MongoDB(&reg;) container(s)
  ## Examples:
  ## extraVolumeMounts:
  ##   - name: extras
  ##     mountPath: /usr/share/extras
  ##     readOnly: true
  ##
  extraVolumeMounts: []
  ## @param extraVolumes Optionally specify extra list of additional volumes to the MongoDB(&reg;) statefulset
  ## extraVolumes:
  ##   - name: extras
  ##     emptyDir: {}
  ##
  extraVolumes: []
  ## MongoDB(&reg;) Pod Disruption Budget configuration
  ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/
  ##
  pdb:
    ## @param pdb.create Enable/disable a Pod Disruption Budget creation for MongoDB(&reg;) pod(s)
    ##
    create: true
    ## @param pdb.minAvailable Minimum number/percentage of MongoDB(&reg;) pods that must still be available after the eviction
    ##
    minAvailable: ""
    ## @param pdb.maxUnavailable Maximum number/percentage of MongoDB(&reg;) pods that may be made unavailable after the eviction. Defaults to `1` if both `pdb.minAvailable` and `pdb.maxUnavailable` are empty.
    ##
    maxUnavailable: ""
  ## @section Traffic exposure parameters
  ##

  ## Service parameters
  ##
  service:
    ## @param service.nameOverride MongoDB(&reg;) service name
    ##
    nameOverride: ""
    ## @param service.type Kubernetes Service type (only for standalone architecture)
    ##
    type: ClusterIP
    ## @param service.portName MongoDB(&reg;) service port name (only for standalone architecture)
    ##
    portName: mongodb
    ## @param service.ports.mongodb MongoDB(&reg;) service port.
    ##
    ports:
      mongodb: 27017
    ## @param service.nodePorts.mongodb Port to bind to for NodePort and LoadBalancer service types (only for standalone architecture)
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport
    ##
    nodePorts:
      mongodb: ""
    ## @param service.clusterIP MongoDB(&reg;) service cluster IP (only for standalone architecture)
    ## e.g:
    ## clusterIP: None
    ##
    clusterIP: ""
    ## @param service.externalIPs Specify the externalIP value ClusterIP service type (only for standalone architecture)
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#external-ips
    ##
    externalIPs: []
    ## @param service.loadBalancerIP loadBalancerIP for MongoDB(&reg;) Service (only for standalone architecture)
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#loadbalancer
    ##
    loadBalancerIP: ""
    ## @param service.loadBalancerClass loadBalancerClass for MongoDB(&reg;) Service (only for standalone architecture)
    # ref: https://kubernetes.io/docs/concepts/services-networking/service/#load-balancer-class
    loadBalancerClass: ""
    ## @param service.loadBalancerSourceRanges Address(es) that are allowed when service is LoadBalancer (only for standalone architecture)
    ## ref: https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service
    ##
    loadBalancerSourceRanges: []
    ## @param service.allocateLoadBalancerNodePorts Wheter to allocate node ports when service type is LoadBalancer
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#load-balancer-nodeport-allocation
    ##
    allocateLoadBalancerNodePorts: true
    ## @param service.extraPorts Extra ports to expose (normally used with the `sidecar` value)
    ##
    extraPorts: []
    ## @param service.annotations Provide any additional annotations that may be required
    ##
    annotations: {}
    ## @param service.externalTrafficPolicy service external traffic policy (only for standalone architecture)
    ## ref https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip
    ##
    externalTrafficPolicy: Local
    ## @param service.sessionAffinity Control where client requests go, to the same pod or round-robin
    ## Values: ClientIP or None
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/
    ##
    sessionAffinity: None
    ## @param service.sessionAffinityConfig Additional settings for the sessionAffinity
    ## sessionAffinityConfig:
    ##   clientIP:
    ##     timeoutSeconds: 300
    ##
    sessionAffinityConfig: {}
    ## Headless service properties
    ##
    headless:
      ## @param service.headless.annotations Annotations for the headless service.
      ##
      annotations: {}
  ## External Access to MongoDB(&reg;) nodes configuration
  ##
  externalAccess:
    ## @param externalAccess.enabled Enable Kubernetes external cluster access to MongoDB(&reg;) nodes (only for replicaset architecture)
    ##
    enabled: false
    ## External IPs auto-discovery configuration
    ## An init container is used to auto-detect LB IPs or node ports by querying the K8s API
    ## Note: RBAC might be required
    ##
    autoDiscovery:
      ## @param externalAccess.autoDiscovery.enabled Enable using an init container to auto-detect external IPs by querying the K8s API
      ##
      enabled: false
      ## Bitnami Kubectl image
      ## ref: https://hub.docker.com/r/bitnami/kubectl/tags/
      ## @param externalAccess.autoDiscovery.image.registry [default: REGISTRY_NAME] Init container auto-discovery image registry
      ## @param externalAccess.autoDiscovery.image.repository [default: REPOSITORY_NAME/kubectl] Init container auto-discovery image repository
      ## @skip externalAccess.autoDiscovery.image.tag Init container auto-discovery image tag (immutable tags are recommended)
      ## @param externalAccess.autoDiscovery.image.digest Init container auto-discovery image digest in the way sha256:aa.... Please note this parameter, if set, will override the tag
      ## @param externalAccess.autoDiscovery.image.pullPolicy Init container auto-discovery image pull policy
      ## @param externalAccess.autoDiscovery.image.pullSecrets Init container auto-discovery image pull secrets
      ##
      image:
        registry: docker.io
        repository: bitnami/kubectl
        tag: 1.30.2-debian-12-r0
        digest: ""
        ## Specify a imagePullPolicy
        ## Defaults to 'Always' if image tag is 'latest', else set to 'IfNotPresent'
        ## ref: https://kubernetes.io/docs/concepts/containers/images/#pre-pulled-images
        ##
        pullPolicy: IfNotPresent
        ## Optionally specify an array of imagePullSecrets (secrets must be manually created in the namespace)
        ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
        ## Example:
        ## pullSecrets:
        ##   - myRegistryKeySecretName
        ##
        pullSecrets: []
      ## Init Container resource requests and limits
      ## ref: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
      ## We usually recommend not to specify default resources and to leave this as a conscious
      ## choice for the user. This also increases chances charts run on environments with little
      ## resources, such as Minikube. If you do want to specify resources, uncomment the following
      ## lines, adjust them as necessary, and remove the curly braces after 'resources:'.
      ## @param externalAccess.autoDiscovery.resourcesPreset Set container resources according to one common preset (allowed values: none, nano, micro, small, medium, large, xlarge, 2xlarge). This is ignored if externalAccess.autoDiscovery.resources is set (externalAccess.autoDiscovery.resources is recommended for production).
      ## More information: https://github.com/bitnami/charts/blob/main/bitnami/common/templates/_resources.tpl#L15
      ##
      resourcesPreset: "nano"
      ## @param externalAccess.autoDiscovery.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
      ## Example:
      ## resources:
      ##   requests:
      ##     cpu: 2
      ##     memory: 512Mi
      ##   limits:
      ##     cpu: 3
      ##     memory: 1024Mi
      ##
      resources: {}
    ## Init container what mission is ensure public names can be resolved.
    ##
    dnsCheck:
      ## Bitnami os-shell image
      ## ref: https://hub.docker.com/r/bitnami/os-shell/tags/
      ## @param externalAccess.dnsCheck.image.registry [default: REGISTRY_NAME] Init container dns-check image registry
      ## @param externalAccess.dnsCheck.image.repository [default: REPOSITORY_NAME/kubectl] Init container dns-check image repository
      ## @skip externalAccess.dnsCheck.image.tag Init container dns-check image tag (immutable tags are recommended)
      ## @param externalAccess.dnsCheck.image.digest Init container dns-check image digest in the way sha256:aa.... Please note this parameter, if set, will override the tag
      ## @param externalAccess.dnsCheck.image.pullPolicy Init container dns-check image pull policy
      ## @param externalAccess.dnsCheck.image.pullSecrets Init container dns-check image pull secrets
      ##
      image:
        registry: docker.io
        repository: bitnami/os-shell
        tag: 12-debian-12-r23
        digest: ""
        ## Specify a imagePullPolicy
        ## Defaults to 'Always' if image tag is 'latest', else set to 'IfNotPresent'
        ## ref: https://kubernetes.io/docs/concepts/containers/images/#pre-pulled-images
        ##
        pullPolicy: IfNotPresent
        ## Optionally specify an array of imagePullSecrets (secrets must be manually created in the namespace)
        ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
        ## Example:
        ## pullSecrets:
        ##   - myRegistryKeySecretName
        ##
        pullSecrets: []
      ## Init Container resource requests and limits
      ## ref: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
      ## We usually recommend not to specify default resources and to leave this as a conscious
      ## choice for the user. This also increases chances charts run on environments with little
      ## resources, such as Minikube. If you do want to specify resources, uncomment the following
      ## lines, adjust them as necessary, and remove the curly braces after 'resources:'.
      ## @param externalAccess.dnsCheck.resourcesPreset Set container resources according to one common preset (allowed values: none, nano, micro, small, medium, large, xlarge, 2xlarge). This is ignored if externalAccess.autoDiscovery.resources is set (externalAccess.autoDiscovery.resources is recommended for production).
      ## More information: https://github.com/bitnami/charts/blob/main/bitnami/common/templates/_resources.tpl#L15
      ##
      resourcesPreset: "nano"
      ## @param externalAccess.dnsCheck.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
      ## Example:
      ## resources:
      ##   requests:
      ##     cpu: 2
      ##     memory: 512Mi
      ##   limits:
      ##     cpu: 3
      ##     memory: 1024Mi
      ##
      resources: {}
    ## Parameters to configure a set of Pods that connect to an existing MongoDB(&reg;) deployment that lies outside of Kubernetes.
    ## @param externalAccess.externalMaster.enabled Use external master for bootstrapping
    ## @param externalAccess.externalMaster.host External master host to bootstrap from
    ## @param externalAccess.externalMaster.port Port for MongoDB(&reg;) service external master host
    ##
    externalMaster:
      enabled: false
      host: ""
      port: 27017
    ## Parameters to configure K8s service(s) used to externally access MongoDB(&reg;)
    ## A new service per broker will be created
    ##
    service:
      ## @param externalAccess.service.type Kubernetes Service type for external access. Allowed values: NodePort, LoadBalancer or ClusterIP
      ##
      type: LoadBalancer
      ## @param externalAccess.service.portName MongoDB(&reg;) port name used for external access when service type is LoadBalancer
      ##
      portName: "mongodb"
      ## @param externalAccess.service.ports.mongodb MongoDB(&reg;) port used for external access when service type is LoadBalancer
      ##
      ports:
        mongodb: 27017
      ## @param externalAccess.service.loadBalancerIPs Array of load balancer IPs for MongoDB(&reg;) nodes
      ## Example:
      ## loadBalancerIPs:
      ##   - X.X.X.X
      ##   - Y.Y.Y.Y
      ##
      loadBalancerIPs: []
      ## @param externalAccess.service.publicNames Array of public names. The size should be equal to the number of replicas.
      ##
      publicNames: []
      ## @param externalAccess.service.loadBalancerClass loadBalancerClass when service type is LoadBalancer
      # ref: https://kubernetes.io/docs/concepts/services-networking/service/#load-balancer-class
      loadBalancerClass: ""
      ## @param externalAccess.service.loadBalancerSourceRanges Address(es) that are allowed when service is LoadBalancer
      ## ref: https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service
      ## Example:
      ## loadBalancerSourceRanges:
      ## - 10.10.10.0/24
      ##
      loadBalancerSourceRanges: []
      ## @param externalAccess.service.allocateLoadBalancerNodePorts Whether to allocate node ports when service type is LoadBalancer
      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#load-balancer-nodeport-allocation
      ##
      allocateLoadBalancerNodePorts: true
      ## @param externalAccess.service.externalTrafficPolicy MongoDB(&reg;) service external traffic policy
      ## ref https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip
      ##
      externalTrafficPolicy: Local
      ## @param externalAccess.service.nodePorts Array of node ports used to configure MongoDB(&reg;) advertised hostname when service type is NodePort
      ## Example:
      ## nodePorts:
      ##   - 30001
      ##   - 30002
      ##
      nodePorts: []
      ## @param externalAccess.service.domain Domain or external IP used to configure MongoDB(&reg;) advertised hostname when service type is NodePort
      ## If not specified, the container will try to get the kubernetes node external IP
      ## e.g:
      ## domain: mydomain.com
      ##
      domain: ""
      ## @param externalAccess.service.extraPorts Extra ports to expose (normally used with the `sidecar` value)
      ##
      extraPorts: []
      ## @param externalAccess.service.annotations Service annotations for external access. These annotations are common for all services created.
      ##
      annotations: {}
      ## @param externalAccess.service.annotationsList Service annotations for eache external service. This value contains a list allowing different annotations per each external service.
      ## Eg:
      ##   annotationsList:
      ##     - external-dns.alpha.kubernetes.io/hostname: mongodb-0.example.com
      ##     - external-dns.alpha.kubernetes.io/hostname: mongodb-1.example.com
      ##
      annotationsList: []
      ## @param externalAccess.service.sessionAffinity Control where client requests go, to the same pod or round-robin
      ## Values: ClientIP or None
      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/
      ##
      sessionAffinity: None
      ## @param externalAccess.service.sessionAffinityConfig Additional settings for the sessionAffinity
      ## sessionAffinityConfig:
      ##   clientIP:
      ##     timeoutSeconds: 300
      ##
      sessionAffinityConfig: {}
    ## External Access to MongoDB(&reg;) Hidden nodes configuration
    ##
    hidden:
      ## @param externalAccess.hidden.enabled Enable Kubernetes external cluster access to MongoDB(&reg;) hidden nodes
      ##
      enabled: false
      ## Parameters to configure K8s service(s) used to externally access MongoDB(&reg;)
      ## A new service per broker will be created
      ##
      service:
        ## @param externalAccess.hidden.service.type Kubernetes Service type for external access. Allowed values: NodePort or LoadBalancer
        ##
        type: LoadBalancer
        ## @param externalAccess.hidden.service.portName MongoDB(&reg;) port name used for external access when service type is LoadBalancer
        ##
        portName: "mongodb"
        ## @param externalAccess.hidden.service.ports.mongodb MongoDB(&reg;) port used for external access when service type is LoadBalancer
        ##
        ports:
          mongodb: 27017
        ## @param externalAccess.hidden.service.loadBalancerIPs Array of load balancer IPs for MongoDB(&reg;) nodes
        ## Example:
        ## loadBalancerIPs:
        ##   - X.X.X.X
        ##   - Y.Y.Y.Y
        ##
        loadBalancerIPs: []
        ## @param externalAccess.hidden.service.loadBalancerClass loadBalancerClass when service type is LoadBalancer
        # ref: https://kubernetes.io/docs/concepts/services-networking/service/#load-balancer-class
        loadBalancerClass: ""
        ## @param externalAccess.hidden.service.loadBalancerSourceRanges Address(es) that are allowed when service is LoadBalancer
        ## ref: https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service
        ## Example:
        ## loadBalancerSourceRanges:
        ## - 10.10.10.0/24
        ##
        loadBalancerSourceRanges: []
        ## @param externalAccess.hidden.service.allocateLoadBalancerNodePorts Wheter to allocate node ports when service type is LoadBalancer
        ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#load-balancer-nodeport-allocation
        ##
        allocateLoadBalancerNodePorts: true
        ## @param externalAccess.hidden.service.externalTrafficPolicy MongoDB(&reg;) service external traffic policy
        ## ref https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip
        ##
        externalTrafficPolicy: Local
        ## @param externalAccess.hidden.service.nodePorts Array of node ports used to configure MongoDB(&reg;) advertised hostname when service type is NodePort. Length must be the same as replicaCount
        ## Example:
        ## nodePorts:
        ##   - 30001
        ##   - 30002
        ##
        nodePorts: []
        ## @param externalAccess.hidden.service.domain Domain or external IP used to configure MongoDB(&reg;) advertised hostname when service type is NodePort
        ## If not specified, the container will try to get the kubernetes node external IP
        ## e.g:
        ## domain: mydomain.com
        ##
        domain: ""
        ## @param externalAccess.hidden.service.extraPorts Extra ports to expose (normally used with the `sidecar` value)
        ##
        extraPorts: []
        ## @param externalAccess.hidden.service.annotations Service annotations for external access
        ##
        annotations: {}
        ## @param externalAccess.hidden.service.sessionAffinity Control where client requests go, to the same pod or round-robin
        ## Values: ClientIP or None
        ## ref: https://kubernetes.io/docs/concepts/services-networking/service/
        ##
        sessionAffinity: None
        ## @param externalAccess.hidden.service.sessionAffinityConfig Additional settings for the sessionAffinity
        ## sessionAffinityConfig:
        ##   clientIP:
        ##     timeoutSeconds: 300
        ##
        sessionAffinityConfig: {}
  ## @section Network policy parameters
  ##

  ## Network Policies
  ## Ref: https://kubernetes.io/docs/concepts/services-networking/network-policies/
  ##
  networkPolicy:
    ## @param networkPolicy.enabled Specifies whether a NetworkPolicy should be created
    ##
    enabled: true
    ## @param networkPolicy.allowExternal Don't require server label for connections
    ## The Policy model to apply. When set to false, only pods with the correct
    ## server label will have network access to the ports server is listening
    ## on. When true, server will accept connections from any source
    ## (with the correct destination port).
    ##
    allowExternal: true
    ## @param networkPolicy.allowExternalEgress Allow the pod to access any range of port and all destinations.
    ##
    allowExternalEgress: true
    ## @param networkPolicy.addExternalClientAccess Allow access from pods with client label set to "true". Ignored if `networkPolicy.allowExternal` is true.
    ##
    addExternalClientAccess: true
    ## @param networkPolicy.extraIngress [array] Add extra ingress rules to the NetworkPolicy
    ## e.g:
    ## extraIngress:
    ##   - ports:
    ##       - port: 1234
    ##     from:
    ##       - podSelector:
    ##           - matchLabels:
    ##               - role: frontend
    ##       - podSelector:
    ##           - matchExpressions:
    ##               - key: role
    ##                 operator: In
    ##                 values:
    ##                   - frontend
    extraIngress: []
    ## @param networkPolicy.extraEgress [array] Add extra ingress rules to the NetworkPolicy
    ## e.g:
    ## extraEgress:
    ##   - ports:
    ##       - port: 1234
    ##     to:
    ##       - podSelector:
    ##           - matchLabels:
    ##               - role: frontend
    ##       - podSelector:
    ##           - matchExpressions:
    ##               - key: role
    ##                 operator: In
    ##                 values:
    ##                   - frontend
    ##
    extraEgress: []
    ## @param networkPolicy.ingressPodMatchLabels [object] Labels to match to allow traffic from other pods. Ignored if `networkPolicy.allowExternal` is true.
    ## e.g:
    ## ingressPodMatchLabels:
    ##   my-client: "true"
    #
    ingressPodMatchLabels: {}
    ## @param networkPolicy.ingressNSMatchLabels [object] Labels to match to allow traffic from other namespaces. Ignored if `networkPolicy.allowExternal` is true.
    ## @param networkPolicy.ingressNSPodMatchLabels [object] Pod labels to match to allow traffic from other namespaces. Ignored if `networkPolicy.allowExternal` is true.
    ##
    ingressNSMatchLabels: {}
    ingressNSPodMatchLabels: {}
  persistence:
    ## @param persistence.enabled Enable MongoDB(&reg;) data persistence using PVC
    ##
    enabled: true
    ## @param persistence.name Name of the PVC and mounted volume
    ##
    name: "datadir"
    ## @param persistence.medium Provide a medium for `emptyDir` volumes.
    ## Requires persistence.enabled: false
    ##
    medium: ""
    ## @param persistence.existingClaim Provide an existing `PersistentVolumeClaim` (only when `architecture=standalone`)
    ## Requires persistence.enabled: true
    ## If defined, PVC must be created manually before volume will be bound
    ## Ignored when mongodb.architecture=replicaset
    ##
    existingClaim: ""
    ## @param persistence.resourcePolicy Setting it to "keep" to avoid removing PVCs during a helm delete operation. Leaving it empty will delete PVCs after the chart deleted
    ##
    resourcePolicy: ""
    ## @param persistence.storageClass PVC Storage Class for MongoDB(&reg;) data volume
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ## set, choosing the default provisioner.
    ##
    storageClass: ""
    ## @param persistence.accessModes PV Access Mode
    ##
    accessModes:
      - ReadWriteOnce
    ## @param persistence.size PVC Storage Request for MongoDB(&reg;) data volume
    ##
    size: 100Gi
    ## @param persistence.annotations PVC annotations
    ##
    annotations: {}
    ## @param persistence.labels PVC labels
    ##
    labels: {}
    ## @param persistence.mountPath Path to mount the volume at
    ## MongoDB(&reg;) images.
    ##
    mountPath: /bitnami/mongodb
    ## @param persistence.subPath Subdirectory of the volume to mount at
    ## and one PV for multiple services.
    ##
    subPath: ""
    ## Fine tuning for volumeClaimTemplates
    ##
    volumeClaimTemplates:
      ## @param persistence.volumeClaimTemplates.selector A label query over volumes to consider for binding (e.g. when using local volumes)
      ## A label query over volumes to consider for binding (e.g. when using local volumes)
      ## See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.20/#labelselector-v1-meta for more details
      ##
      selector: {}
      ## @param persistence.volumeClaimTemplates.requests Custom PVC requests attributes
      ## Sometime cloud providers use additional requests attributes to provision custom storage instance
      ## See https://cloud.ibm.com/docs/containers?topic=containers-file_storage#file_dynamic_statefulset
      ##
      requests: {}
      ## @param persistence.volumeClaimTemplates.dataSource Add dataSource to the VolumeClaimTemplate
      ##
      dataSource: {}
  ## Persistent Volume Claim Retention Policy
  ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#persistentvolumeclaim-retention
  ##
  persistentVolumeClaimRetentionPolicy:
    ## @param persistentVolumeClaimRetentionPolicy.enabled Enable Persistent volume retention policy for MongoDB(&reg;) Statefulset
    ##
    enabled: false
    ## @param persistentVolumeClaimRetentionPolicy.whenScaled Volume retention behavior when the replica count of the StatefulSet is reduced
    ##
    whenScaled: Retain
    ## @param persistentVolumeClaimRetentionPolicy.whenDeleted Volume retention behavior that applies when the StatefulSet is deleted
    ##
    whenDeleted: Retain
  ## @section Backup parameters
  ## This section implements a trivial logical dump cronjob of the database.
  ## This only comes with the consistency guarantees of the dump program.
  ## This is not a snapshot based roll forward/backward recovery backup.
  ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/
  ##
  backup:
    ## @param backup.enabled Enable the logical dump of the database "regularly"
    ##
    enabled: false
    ## Fine tuning cronjob's config
    ##
    cronjob:
      ## @param backup.cronjob.schedule Set the cronjob parameter schedule
      ##
      schedule: "@daily"
      ## @param backup.cronjob.concurrencyPolicy Set the cronjob parameter concurrencyPolicy
      ##
      concurrencyPolicy: Allow
      ## @param backup.cronjob.failedJobsHistoryLimit Set the cronjob parameter failedJobsHistoryLimit
      ##
      failedJobsHistoryLimit: 1
      ## @param backup.cronjob.successfulJobsHistoryLimit Set the cronjob parameter successfulJobsHistoryLimit
      ##
      successfulJobsHistoryLimit: 3
      ## @param backup.cronjob.startingDeadlineSeconds Set the cronjob parameter startingDeadlineSeconds
      ##
      startingDeadlineSeconds: ""
      ## @param backup.cronjob.ttlSecondsAfterFinished Set the cronjob parameter ttlSecondsAfterFinished
      ##
      ttlSecondsAfterFinished: ""
      ## @param backup.cronjob.restartPolicy Set the cronjob parameter restartPolicy
      ##
      restartPolicy: OnFailure
      ## @param backup.cronjob.backoffLimit Set the cronjob parameter backoffLimit
      backoffLimit: 6
      ## backup container's Security Context
      ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container
      ## @param backup.cronjob.containerSecurityContext.enabled Enabled containers' Security Context
      ## @param backup.cronjob.containerSecurityContext.seLinuxOptions [object,nullable] Set SELinux options in container
      ## @param backup.cronjob.containerSecurityContext.runAsUser Set containers' Security Context runAsUser
      ## @param backup.cronjob.containerSecurityContext.runAsGroup Set containers' Security Context runAsGroup
      ## @param backup.cronjob.containerSecurityContext.runAsNonRoot Set container's Security Context runAsNonRoot
      ## @param backup.cronjob.containerSecurityContext.privileged Set container's Security Context privileged
      ## @param backup.cronjob.containerSecurityContext.readOnlyRootFilesystem Set container's Security Context readOnlyRootFilesystem
      ## @param backup.cronjob.containerSecurityContext.allowPrivilegeEscalation Set container's Security Context allowPrivilegeEscalation
      ## @param backup.cronjob.containerSecurityContext.capabilities.drop List of capabilities to be dropped
      ## @param backup.cronjob.containerSecurityContext.seccompProfile.type Set container's Security Context seccomp profile
      ##
      containerSecurityContext:
        enabled: true
        seLinuxOptions: {}
        runAsUser: 1001
        runAsGroup: 1001
        runAsNonRoot: true
        privileged: false
        readOnlyRootFilesystem: true
        allowPrivilegeEscalation: false
        capabilities:
          drop: ["ALL"]
        seccompProfile:
          type: "RuntimeDefault"
      ## @param backup.cronjob.command Set backup container's command to run
      ##
      command: []
      ## @param backup.cronjob.labels Set the cronjob labels
      ##
      labels: {}
      ## @param backup.cronjob.annotations Set the cronjob annotations
      ##
      annotations: {}
      ## Backup container's
      ##
      storage:
        ## @param backup.cronjob.storage.existingClaim Provide an existing `PersistentVolumeClaim` (only when `architecture=standalone`)
        ## If defined, PVC must be created manually before volume will be bound
        ##
        existingClaim: ""
        ## @param backup.cronjob.storage.resourcePolicy Setting it to "keep" to avoid removing PVCs during a helm delete operation. Leaving it empty will delete PVCs after the chart deleted
        ##
        resourcePolicy: ""
        ## @param backup.cronjob.storage.storageClass PVC Storage Class for the backup data volume
        ## If defined, storageClassName: <storageClass>
        ## If set to "-", storageClassName: "", which disables dynamic provisioning
        ## If undefined (the default) or set to null, no storageClassName spec is
        ## set, choosing the default provisioner.
        ##
        storageClass: ""
        ## @param backup.cronjob.storage.accessModes PV Access Mode
        ##
        accessModes:
          - ReadWriteOnce
        ## @param backup.cronjob.storage.size PVC Storage Request for the backup data volume
        ##
        size: 8Gi
        ## @param backup.cronjob.storage.annotations PVC annotations
        ##
        annotations: {}
        ## @param backup.cronjob.storage.mountPath Path to mount the volume at 
        ##
        mountPath: /backup/mongodb
        ## @param backup.cronjob.storage.subPath Subdirectory of the volume to mount at
        ## and one PV for multiple services.
        ##
        subPath: ""
        ## Fine tuning for volumeClaimTemplates
        ##
        volumeClaimTemplates:
          ## @param backup.cronjob.storage.volumeClaimTemplates.selector A label query over volumes to consider for binding (e.g. when using local volumes)
          ## A label query over volumes to consider for binding (e.g. when using local volumes)
          ## See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.20/#labelselector-v1-meta for more details
          ##
          selector: {}
  ## @section RBAC parameters
  ##

  ## ServiceAccount
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  ##
  serviceAccount:
    ## @param serviceAccount.create Enable creation of ServiceAccount for MongoDB(&reg;) pods
    ##
    create: true
    ## @param serviceAccount.name Name of the created serviceAccount
    ## If not set and create is true, a name is generated using the mongodb.fullname template
    ##
    name: ""
    ## @param serviceAccount.annotations Additional Service Account annotations
    ##
    annotations: {}
    ## @param serviceAccount.automountServiceAccountToken Allows auto mount of ServiceAccountToken on the serviceAccount created
    ## Can be set to false if pods using this serviceAccount do not need to use K8s API
    ##
    automountServiceAccountToken: false
  ## Role Based Access
  ## ref: https://kubernetes.io/docs/admin/authorization/rbac/
  ##
  rbac:
    ## @param rbac.create Whether to create & use RBAC resources or not
    ## binding MongoDB(&reg;) ServiceAccount to a role
    ## that allows MongoDB(&reg;) pods querying the K8s API
    ## this needs to be set to 'true' to enable the mongo-labeler sidecar primary mongodb discovery
    ##
    create: false
    ## @param rbac.rules Custom rules to create following the role specification
    ## The example below needs to be uncommented to use the 'mongo-labeler' sidecar for dynamic discovery of the primary mongodb pod:
    ## rules:
    ##   - apiGroups:
    ##       - ""
    ##     resources:
    ##       - pods
    ##     verbs:
    ##       - get
    ##       - list
    ##       - watch
    ##       - update
    ##
    rules: []
  ## PodSecurityPolicy configuration
  ## Be sure to also set rbac.create to true, otherwise Role and RoleBinding won't be created.
  ## ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  ##
  podSecurityPolicy:
    ## @param podSecurityPolicy.create Whether to create a PodSecurityPolicy. WARNING: PodSecurityPolicy is deprecated in Kubernetes v1.21 or later, unavailable in v1.25 or later
    ##
    create: false
    ## @param podSecurityPolicy.allowPrivilegeEscalation Enable privilege escalation
    ## Either use predefined policy with some adjustments or use `podSecurityPolicy.spec`
    ##
    allowPrivilegeEscalation: false
    ## @param podSecurityPolicy.privileged Allow privileged
    ##
    privileged: false
    ## @param podSecurityPolicy.spec Specify the full spec to use for Pod Security Policy
    ## ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/
    ## Defining a spec ignores the above values.
    ##
    spec: {}
    ## Example:
    ##    allowPrivilegeEscalation: false
    ##    fsGroup:
    ##      rule: 'MustRunAs'
    ##      ranges:
    ##        - min: 1001
    ##          max: 1001
    ##    hostIPC: false
    ##    hostNetwork: false
    ##    hostPID: false
    ##    privileged: false
    ##    readOnlyRootFilesystem: true
    ##    requiredDropCapabilities:
    ##      - ALL
    ##    runAsUser:
    ##      rule: 'MustRunAs'
    ##      ranges:
    ##        - min: 1001
    ##          max: 1001
    ##    seLinux:
    ##      rule: 'RunAsAny'
    ##    supplementalGroups:
    ##      rule: 'MustRunAs'
    ##      ranges:
    ##        - min: 1001
    ##          max: 1001
    ##    volumes:
    ##      - 'configMap'
    ##      - 'secret'
    ##      - 'emptyDir'
    ##      - 'persistentVolumeClaim'
    ##
  ## @section Volume Permissions parameters
  ##
  ## Init Container parameters
  ## Change the owner and group of the persistent volume(s) mountpoint(s) to 'runAsUser:fsGroup' on each component
  ## values from the securityContext section of the component
  ##
  volumePermissions:
    ## @param volumePermissions.enabled Enable init container that changes the owner and group of the persistent volume(s) mountpoint to `runAsUser:fsGroup`
    ##
    enabled: false
    ## @param volumePermissions.image.registry [default: REGISTRY_NAME] Init container volume-permissions image registry
    ## @param volumePermissions.image.repository [default: REPOSITORY_NAME/os-shell] Init container volume-permissions image repository
    ## @skip volumePermissions.image.tag Init container volume-permissions image tag (immutable tags are recommended)
    ## @param volumePermissions.image.digest Init container volume-permissions image digest in the way sha256:aa.... Please note this parameter, if set, will override the tag
    ## @param volumePermissions.image.pullPolicy Init container volume-permissions image pull policy
    ## @param volumePermissions.image.pullSecrets Specify docker-registry secret names as an array
    ##
    image:
      registry: docker.io
      repository: bitnami/os-shell
      tag: 12-debian-12-r23
      digest: ""
      ## Specify a imagePullPolicy
      ## Defaults to 'Always' if image tag is 'latest', else set to 'IfNotPresent'
      ## ref: https://kubernetes.io/docs/concepts/containers/images/#pre-pulled-images
      ##
      pullPolicy: IfNotPresent
      ## Optionally specify an array of imagePullSecrets (secrets must be manually created in the namespace)
      ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
      ## Example:
      ## pullSecrets:
      ##   - myRegistryKeySecretName
      ##
      pullSecrets: []
    ## Init Container resource requests and limits
    ## ref: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
    ## We usually recommend not to specify default resources and to leave this as a conscious
    ## choice for the user. This also increases chances charts run on environments with little
    ## resources, such as Minikube. If you do want to specify resources, uncomment the following
    ## lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    ## @param volumePermissions.resourcesPreset Set container resources according to one common preset (allowed values: none, nano, micro, small, medium, large, xlarge, 2xlarge). This is ignored if volumePermissions.resources is set (volumePermissions.resources is recommended for production).
    ## More information: https://github.com/bitnami/charts/blob/main/bitnami/common/templates/_resources.tpl#L15
    ##
    resourcesPreset: "nano"
    ## @param volumePermissions.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
    ## Example:
    ## resources:
    ##   requests:
    ##     cpu: 2
    ##     memory: 512Mi
    ##   limits:
    ##     cpu: 3
    ##     memory: 1024Mi
    ##
    resources: {}
    ## Init container Security Context
    ## Note: the chown of the data folder is done to containerSecurityContext.runAsUser
    ## and not the below volumePermissions.securityContext.runAsUser
    ## When runAsUser is set to special value "auto", init container will try to chwon the
    ## data folder to autodetermined user&group, using commands: `id -u`:`id -G | cut -d" " -f2`
    ## "auto" is especially useful for OpenShift which has scc with dynamic userids (and 0 is not allowed).
    ## You may want to use this volumePermissions.securityContext.runAsUser="auto" in combination with
    ## podSecurityContext.enabled=false,containerSecurityContext.enabled=false and shmVolume.chmod.enabled=false
    ## @param volumePermissions.securityContext.seLinuxOptions [object,nullable] Set SELinux options in container
    ## @param volumePermissions.securityContext.runAsUser User ID for the volumePermissions container
    ##
    securityContext:
      seLinuxOptions: {}
      runAsUser: 0
  ## @section Arbiter parameters
  ##
  arbiter:
    ## @param arbiter.enabled Enable deploying the arbiter
    ##   https://docs.mongodb.com/manual/tutorial/add-replica-set-arbiter/
    ##
    enabled: true
    ## @param arbiter.automountServiceAccountToken Mount Service Account token in pod
    ##
    automountServiceAccountToken: false
    ## @param arbiter.hostAliases Add deployment host aliases
    ## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/
    ##
    hostAliases: []
    ## @param arbiter.configuration Arbiter configuration file to be used
    ##   http://docs.mongodb.org/manual/reference/configuration-options/
    ##
    configuration: ""
    ## @param arbiter.existingConfigmap Name of existing ConfigMap with Arbiter configuration
    ## NOTE: When it's set the arbiter.configuration parameter is ignored
    ##
    existingConfigmap: ""
    ## Command and args for running the container (set to default if not set). Use array form
    ## @param arbiter.command Override default container command (useful when using custom images)
    ## @param arbiter.args Override default container args (useful when using custom images)
    ##
    command: []
    args: []
    ## @param arbiter.extraFlags Arbiter additional command line flags
    ## Example:
    ## extraFlags:
    ##  - "--wiredTigerCacheSizeGB=2"
    ##
    extraFlags: []
    ## @param arbiter.extraEnvVars Extra environment variables to add to Arbiter pods
    ## E.g:
    ## extraEnvVars:
    ##   - name: FOO
    ##     value: BAR
    ##
    extraEnvVars: []
    ## @param arbiter.extraEnvVarsCM Name of existing ConfigMap containing extra env vars
    ##
    extraEnvVarsCM: ""
    ## @param arbiter.extraEnvVarsSecret Name of existing Secret containing extra env vars (in case of sensitive data)
    ##
    extraEnvVarsSecret: ""
    ## @param arbiter.annotations Additional labels to be added to the Arbiter statefulset
    ##
    annotations: {}
    ## @param arbiter.labels Annotations to be added to the Arbiter statefulset
    ##
    labels: {}
    ## @param arbiter.topologySpreadConstraints MongoDB(&reg;) Spread Constraints for arbiter Pods
    ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/
    ##
    topologySpreadConstraints: []
    ## @param arbiter.lifecycleHooks LifecycleHook for the Arbiter container to automate configuration before or after startup
    ##
    lifecycleHooks: {}
    ## @param arbiter.terminationGracePeriodSeconds Arbiter Termination Grace Period
    ##
    terminationGracePeriodSeconds: ""
    ## @param arbiter.updateStrategy.type Strategy that will be employed to update Pods in the StatefulSet
    ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies
    ## updateStrategy:
    ##  type: RollingUpdate
    ##  rollingUpdate:
    ##    maxSurge: 25%
    ##    maxUnavailable: 25%
    ##
    updateStrategy:
      type: RollingUpdate
    ## @param arbiter.podManagementPolicy Pod management policy for MongoDB(&reg;)
    ## Should be initialized one by one when building the replicaset for the first time
    ##
    podManagementPolicy: OrderedReady
    ## @param arbiter.schedulerName Name of the scheduler (other than default) to dispatch pods
    ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
    ##
    schedulerName: ""
    ## @param arbiter.podAffinityPreset Arbiter Pod affinity preset. Ignored if `affinity` is set. Allowed values: `soft` or `hard`
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
    ##
    podAffinityPreset: ""
    ## @param arbiter.podAntiAffinityPreset Arbiter Pod anti-affinity preset. Ignored if `affinity` is set. Allowed values: `soft` or `hard`
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
    ##
    podAntiAffinityPreset: soft
    ## Node affinity preset
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
    ##
    nodeAffinityPreset:
      ## @param arbiter.nodeAffinityPreset.type Arbiter Node affinity preset type. Ignored if `affinity` is set. Allowed values: `soft` or `hard`
      ##
      type: ""
      ## @param arbiter.nodeAffinityPreset.key Arbiter Node label key to match Ignored if `affinity` is set.
      ## E.g.
      ## key: "kubernetes.io/e2e-az-name"
      ##
      key: ""
      ## @param arbiter.nodeAffinityPreset.values Arbiter Node label values to match. Ignored if `affinity` is set.
      ## E.g.
      ## values:
      ##   - e2e-az1
      ##   - e2e-az2
      ##
      values: []
    ## @param arbiter.affinity Arbiter Affinity for pod assignment
    ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
    ## Note: arbiter.podAffinityPreset, arbiter.podAntiAffinityPreset, and arbiter.nodeAffinityPreset will be ignored when it's set
    ##
    affinity: {}
    ## @param arbiter.nodeSelector Arbiter Node labels for pod assignment
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/
    ##
    nodeSelector: {}
    ## @param arbiter.tolerations Arbiter Tolerations for pod assignment
    ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
    ##
    tolerations: []
    ## @param arbiter.podLabels Arbiter pod labels
    ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
    ##
    podLabels: {}
    ## @param arbiter.podAnnotations Arbiter Pod annotations
    ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
    ##
    podAnnotations: {}
    ## @param arbiter.priorityClassName Name of the existing priority class to be used by Arbiter pod(s)
    ## ref: https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/
    ##
    priorityClassName: ""
    ## @param arbiter.runtimeClassName Name of the runtime class to be used by Arbiter pod(s)
    ## ref: https://kubernetes.io/docs/concepts/containers/runtime-class/
    ##
    runtimeClassName: ""
    ## MongoDB(&reg;) Arbiter pods' Security Context.
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
    ## @param arbiter.podSecurityContext.enabled Enable Arbiter pod(s)' Security Context
    ## @param arbiter.podSecurityContext.fsGroupChangePolicy Set filesystem group change policy
    ## @param arbiter.podSecurityContext.supplementalGroups Set filesystem extra groups
    ## @param arbiter.podSecurityContext.fsGroup Group ID for the volumes of the Arbiter pod(s)
    ## @param arbiter.podSecurityContext.sysctls sysctl settings of the Arbiter pod(s)'
    ##
    podSecurityContext:
      enabled: true
      fsGroupChangePolicy: Always
      supplementalGroups: []
      fsGroup: 1001
      ## sysctl settings
      ## Example:
      ## sysctls:
      ## - name: net.core.somaxconn
      ##   value: "10000"
      ##
      sysctls: []
    ## MongoDB(&reg;) Arbiter containers' Security Context (only main container).
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container
    ## @param arbiter.containerSecurityContext.enabled Enabled containers' Security Context
    ## @param arbiter.containerSecurityContext.seLinuxOptions [object,nullable] Set SELinux options in container
    ## @param arbiter.containerSecurityContext.runAsUser Set containers' Security Context runAsUser
    ## @param arbiter.containerSecurityContext.runAsGroup Set containers' Security Context runAsGroup
    ## @param arbiter.containerSecurityContext.runAsNonRoot Set container's Security Context runAsNonRoot
    ## @param arbiter.containerSecurityContext.privileged Set container's Security Context privileged
    ## @param arbiter.containerSecurityContext.readOnlyRootFilesystem Set container's Security Context readOnlyRootFilesystem
    ## @param arbiter.containerSecurityContext.allowPrivilegeEscalation Set container's Security Context allowPrivilegeEscalation
    ## @param arbiter.containerSecurityContext.capabilities.drop List of capabilities to be dropped
    ## @param arbiter.containerSecurityContext.seccompProfile.type Set container's Security Context seccomp profile
    ##
    containerSecurityContext:
      enabled: true
      seLinuxOptions: {}
      runAsUser: 1001
      runAsGroup: 1001
      runAsNonRoot: true
      privileged: false
      readOnlyRootFilesystem: true
      allowPrivilegeEscalation: false
      capabilities:
        drop: ["ALL"]
      seccompProfile:
        type: "RuntimeDefault"
    ## MongoDB(&reg;) Arbiter containers' resource requests and limits.
    ## ref: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
    ## We usually recommend not to specify default resources and to leave this as a conscious
    ## choice for the user. This also increases chances charts run on environments with little
    ## resources, such as Minikube. If you do want to specify resources, uncomment the following
    ## lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    ## @param arbiter.resourcesPreset Set container resources according to one common preset (allowed values: none, nano, micro, small, medium, large, xlarge, 2xlarge). This is ignored if arbiter.resources is set (arbiter.resources is recommended for production).
    ## More information: https://github.com/bitnami/charts/blob/main/bitnami/common/templates/_resources.tpl#L15
    ##
    resourcesPreset: "small"
    ## @param arbiter.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
    ## Example:
    ## resources:
    ##   requests:
    ##     cpu: 2
    ##     memory: 512Mi
    ##   limits:
    ##     cpu: 3
    ##     memory: 1024Mi
    ##
    resources: {}
    ## @param arbiter.containerPorts.mongodb MongoDB(&reg;) arbiter container port
    ##
    containerPorts:
      mongodb: 27017
    ## MongoDB(&reg;) Arbiter pods' liveness probe. Evaluated as a template.
    ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes
    ## @param arbiter.livenessProbe.enabled Enable livenessProbe
    ## @param arbiter.livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe
    ## @param arbiter.livenessProbe.periodSeconds Period seconds for livenessProbe
    ## @param arbiter.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
    ## @param arbiter.livenessProbe.failureThreshold Failure threshold for livenessProbe
    ## @param arbiter.livenessProbe.successThreshold Success threshold for livenessProbe
    ##
    livenessProbe:
      enabled: true
      initialDelaySeconds: 30
      periodSeconds: 20
      timeoutSeconds: 10
      failureThreshold: 6
      successThreshold: 1
    ## MongoDB(&reg;) Arbiter pods' readiness probe. Evaluated as a template.
    ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes
    ## @param arbiter.readinessProbe.enabled Enable readinessProbe
    ## @param arbiter.readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe
    ## @param arbiter.readinessProbe.periodSeconds Period seconds for readinessProbe
    ## @param arbiter.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
    ## @param arbiter.readinessProbe.failureThreshold Failure threshold for readinessProbe
    ## @param arbiter.readinessProbe.successThreshold Success threshold for readinessProbe
    ##
    readinessProbe:
      enabled: true
      initialDelaySeconds: 5
      periodSeconds: 20
      timeoutSeconds: 10
      failureThreshold: 6
      successThreshold: 1
    ## MongoDB(&reg;) Arbiter pods' startup probe. Evaluated as a template.
    ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes
    ## @param arbiter.startupProbe.enabled Enable startupProbe
    ## @param arbiter.startupProbe.initialDelaySeconds Initial delay seconds for startupProbe
    ## @param arbiter.startupProbe.periodSeconds Period seconds for startupProbe
    ## @param arbiter.startupProbe.timeoutSeconds Timeout seconds for startupProbe
    ## @param arbiter.startupProbe.failureThreshold Failure threshold for startupProbe
    ## @param arbiter.startupProbe.successThreshold Success threshold for startupProbe
    ##
    startupProbe:
      enabled: false
      initialDelaySeconds: 5
      periodSeconds: 10
      timeoutSeconds: 5
      successThreshold: 1
      failureThreshold: 30
    ## @param arbiter.customLivenessProbe Override default liveness probe for Arbiter containers
    ## Ignored when arbiter.livenessProbe.enabled=true
    ##
    customLivenessProbe: {}
    ## @param arbiter.customReadinessProbe Override default readiness probe for Arbiter containers
    ## Ignored when arbiter.readinessProbe.enabled=true
    ##
    customReadinessProbe: {}
    ## @param arbiter.customStartupProbe Override default startup probe for Arbiter containers
    ## Ignored when arbiter.startupProbe.enabled=true
    ##
    customStartupProbe: {}
    ## @param arbiter.initContainers Add additional init containers for the Arbiter pod(s)
    ## Example:
    ## initContainers:
    ##   - name: your-image-name
    ##     image: your-image
    ##     imagePullPolicy: Always
    ##     ports:
    ##       - name: portname
    ##         containerPort: 1234
    ##
    initContainers: []
    ## @param arbiter.sidecars Add additional sidecar containers for the Arbiter pod(s)
    ## Example:
    ## sidecars:
    ##   - name: your-image-name
    ##     image: your-image
    ##     imagePullPolicy: Always
    ##     ports:
    ##       - name: portname
    ##         containerPort: 1234
    ##
    sidecars: []
    ## @param arbiter.extraVolumeMounts Optionally specify extra list of additional volumeMounts for the Arbiter container(s)
    ## Examples:
    ## extraVolumeMounts:
    ##   - name: extras
    ##     mountPath: /usr/share/extras
    ##     readOnly: true
    ##
    extraVolumeMounts: []
    ## @param arbiter.extraVolumes Optionally specify extra list of additional volumes to the Arbiter statefulset
    ## extraVolumes:
    ##   - name: extras
    ##     emptyDir: {}
    ##
    extraVolumes: []
    ## MongoDB(&reg;) Arbiter Pod Disruption Budget configuration
    ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/
    ##
    pdb:
      ## @param arbiter.pdb.create Enable/disable a Pod Disruption Budget creation for Arbiter pod(s)
      ##
      create: true
      ## @param arbiter.pdb.minAvailable Minimum number/percentage of Arbiter pods that should remain scheduled
      ##
      minAvailable: ""
      ## @param arbiter.pdb.maxUnavailable Maximum number/percentage of Arbiter pods that may be made unavailable. Defaults to `1` if both `arbiter.pdb.minAvailable` and `arbiter.pdb.maxUnavailable` are empty.
      ##
      maxUnavailable: ""
    ## MongoDB(&reg;) Arbiter service parameters
    ##
    service:
      ## @param arbiter.service.nameOverride The arbiter service name
      ##
      nameOverride: ""
      ## @param arbiter.service.ports.mongodb MongoDB(&reg;) service port
      ##
      ports:
        mongodb: 27017
      ## @param arbiter.service.extraPorts Extra ports to expose (normally used with the `sidecar` value)
      ##
      extraPorts: []
      ## @param arbiter.service.annotations Provide any additional annotations that may be required
      ##
      annotations: {}
      ## Headless service properties
      ##
      headless:
        ## @param arbiter.service.headless.annotations Annotations for the headless service.
        ##
        annotations: {}
  ## @section Hidden Node parameters
  ##
  hidden:
    ## @param hidden.enabled Enable deploying the hidden nodes
    ##   https://docs.mongodb.com/manual/tutorial/configure-a-hidden-replica-set-member/
    ##
    enabled: false
    ## @param hidden.automountServiceAccountToken Mount Service Account token in pod
    ##
    automountServiceAccountToken: false
    ## @param hidden.hostAliases Add deployment host aliases
    ## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/
    ##
    hostAliases: []
    ## @param hidden.configuration Hidden node configuration file to be used
    ##   http://docs.mongodb.org/manual/reference/configuration-options/
    ##
    configuration: ""
    ## @param hidden.existingConfigmap Name of existing ConfigMap with Hidden node configuration
    ## NOTE: When it's set the hidden.configuration parameter is ignored
    ##
    existingConfigmap: ""
    ## Command and args for running the container (set to default if not set). Use array form
    ## @param hidden.command Override default container command (useful when using custom images)
    ## @param hidden.args Override default container args (useful when using custom images)
    ##
    command: []
    args: []
    ## @param hidden.extraFlags Hidden node additional command line flags
    ## Example:
    ## extraFlags:
    ##  - "--wiredTigerCacheSizeGB=2"
    ##
    extraFlags: []
    ## @param hidden.extraEnvVars Extra environment variables to add to Hidden node pods
    ## E.g:
    ## extraEnvVars:
    ##   - name: FOO
    ##     value: BAR
    ##
    extraEnvVars: []
    ## @param hidden.extraEnvVarsCM Name of existing ConfigMap containing extra env vars
    ##
    extraEnvVarsCM: ""
    ## @param hidden.extraEnvVarsSecret Name of existing Secret containing extra env vars (in case of sensitive data)
    ##
    extraEnvVarsSecret: ""
    ## @param hidden.annotations Additional labels to be added to thehidden node statefulset
    ##
    annotations: {}
    ## @param hidden.labels Annotations to be added to the hidden node statefulset
    ##
    labels: {}
    ## @param hidden.topologySpreadConstraints MongoDB(&reg;) Spread Constraints for hidden Pods
    ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/
    ##
    topologySpreadConstraints: []
    ## @param hidden.lifecycleHooks LifecycleHook for the Hidden container to automate configuration before or after startup
    ##
    lifecycleHooks: {}
    ## @param hidden.replicaCount Number of hidden nodes (only when `architecture=replicaset`)
    ## Ignored when mongodb.architecture=standalone
    ##
    replicaCount: 1
    ## @param hidden.terminationGracePeriodSeconds Hidden Termination Grace Period
    ##
    terminationGracePeriodSeconds: ""
    ## @param hidden.updateStrategy.type Strategy that will be employed to update Pods in the StatefulSet
    ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies
    ## updateStrategy:
    ##  type: RollingUpdate
    ##  rollingUpdate:
    ##    maxSurge: 25%
    ##    maxUnavailable: 25%
    ##
    updateStrategy:
      type: RollingUpdate
    ## @param hidden.podManagementPolicy Pod management policy for hidden node
    ##
    podManagementPolicy: OrderedReady
    ## @param hidden.schedulerName Name of the scheduler (other than default) to dispatch pods
    ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
    ##
    schedulerName: ""
    ## @param hidden.podAffinityPreset Hidden node Pod affinity preset. Ignored if `affinity` is set. Allowed values: `soft` or `hard`
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
    ##
    podAffinityPreset: ""
    ## @param hidden.podAntiAffinityPreset Hidden node Pod anti-affinity preset. Ignored if `affinity` is set. Allowed values: `soft` or `hard`
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
    ##
    podAntiAffinityPreset: soft
    ## Node affinity preset
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
    ## Allowed values: soft, hard
    ##
    nodeAffinityPreset:
      ## @param hidden.nodeAffinityPreset.type Hidden Node affinity preset type. Ignored if `affinity` is set. Allowed values: `soft` or `hard`
      ##
      type: ""
      ## @param hidden.nodeAffinityPreset.key Hidden Node label key to match Ignored if `affinity` is set.
      ## E.g.
      ## key: "kubernetes.io/e2e-az-name"
      ##
      key: ""
      ## @param hidden.nodeAffinityPreset.values Hidden Node label values to match. Ignored if `affinity` is set.
      ## E.g.
      ## values:
      ##   - e2e-az1
      ##   - e2e-az2
      ##
      values: []
    ## @param hidden.affinity Hidden node Affinity for pod assignment
    ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
    ## Note: podAffinityPreset, podAntiAffinityPreset, and nodeAffinityPreset will be ignored when it's set
    ##
    affinity: {}
    ## @param hidden.nodeSelector Hidden node Node labels for pod assignment
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/
    ##
    nodeSelector: {}
    ## @param hidden.tolerations Hidden node Tolerations for pod assignment
    ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
    ##
    tolerations: []
    ## @param hidden.podLabels Hidden node pod labels
    ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
    ##
    podLabels: {}
    ## @param hidden.podAnnotations Hidden node Pod annotations
    ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
    ##
    podAnnotations: {}
    ## @param hidden.priorityClassName Name of the existing priority class to be used by hidden node pod(s)
    ## ref: https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/
    ##
    priorityClassName: ""
    ## @param hidden.runtimeClassName Name of the runtime class to be used by hidden node pod(s)
    ## ref: https://kubernetes.io/docs/concepts/containers/runtime-class/
    ##
    runtimeClassName: ""
    ## MongoDB(&reg;) Hidden pods' Security Context.
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
    ## @param hidden.podSecurityContext.enabled Enable Hidden pod(s)' Security Context
    ## @param hidden.podSecurityContext.fsGroupChangePolicy Set filesystem group change policy
    ## @param hidden.podSecurityContext.supplementalGroups Set filesystem extra groups
    ## @param hidden.podSecurityContext.fsGroup Group ID for the volumes of the Hidden pod(s)
    ## @param hidden.podSecurityContext.sysctls sysctl settings of the Hidden pod(s)'
    ##
    podSecurityContext:
      enabled: true
      fsGroupChangePolicy: Always
      supplementalGroups: []
      fsGroup: 1001
      ## sysctl settings
      ## Example:
      ## sysctls:
      ## - name: net.core.somaxconn
      ##   value: "10000"
      ##
      sysctls: []
    ## MongoDB(&reg;) Hidden containers' Security Context (only main container).
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container
    ## @param hidden.containerSecurityContext.enabled Enabled containers' Security Context
    ## @param hidden.containerSecurityContext.seLinuxOptions [object,nullable] Set SELinux options in container
    ## @param hidden.containerSecurityContext.runAsUser Set containers' Security Context runAsUser
    ## @param hidden.containerSecurityContext.runAsGroup Set containers' Security Context runAsGroup
    ## @param hidden.containerSecurityContext.runAsNonRoot Set container's Security Context runAsNonRoot
    ## @param hidden.containerSecurityContext.privileged Set container's Security Context privileged
    ## @param hidden.containerSecurityContext.readOnlyRootFilesystem Set container's Security Context readOnlyRootFilesystem
    ## @param hidden.containerSecurityContext.allowPrivilegeEscalation Set container's Security Context allowPrivilegeEscalation
    ## @param hidden.containerSecurityContext.capabilities.drop List of capabilities to be dropped
    ## @param hidden.containerSecurityContext.seccompProfile.type Set container's Security Context seccomp profile
    ##
    containerSecurityContext:
      enabled: true
      seLinuxOptions: {}
      runAsUser: 1001
      runAsGroup: 1001
      runAsNonRoot: true
      privileged: false
      readOnlyRootFilesystem: true
      allowPrivilegeEscalation: false
      capabilities:
        drop: ["ALL"]
      seccompProfile:
        type: "RuntimeDefault"
    ## MongoDB(&reg;) Hidden containers' resource requests and limits.
    ## ref: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
    ## We usually recommend not to specify default resources and to leave this as a conscious
    ## choice for the user. This also increases chances charts run on environments with little
    ## resources, such as Minikube. If you do want to specify resources, uncomment the following
    ## lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    ## @param hidden.resourcesPreset Set container resources according to one common preset (allowed values: none, nano, micro, small, medium, large, xlarge, 2xlarge). This is ignored if hidden.resources is set (hidden.resources is recommended for production).
    ## More information: https://github.com/bitnami/charts/blob/main/bitnami/common/templates/_resources.tpl#L15
    ##
    resourcesPreset: "micro"
    ## @param hidden.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
    ## Example:
    ## resources:
    ##   requests:
    ##     cpu: 2
    ##     memory: 512Mi
    ##   limits:
    ##     cpu: 3
    ##     memory: 1024Mi
    ##
    resources: {}
    ## @param hidden.containerPorts.mongodb MongoDB(&reg;) hidden container port
    ##
    containerPorts:
      mongodb: 27017
    ## MongoDB(&reg;) Hidden pods' liveness probe. Evaluated as a template.
    ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes
    ## @param hidden.livenessProbe.enabled Enable livenessProbe
    ## @param hidden.livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe
    ## @param hidden.livenessProbe.periodSeconds Period seconds for livenessProbe
    ## @param hidden.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
    ## @param hidden.livenessProbe.failureThreshold Failure threshold for livenessProbe
    ## @param hidden.livenessProbe.successThreshold Success threshold for livenessProbe
    ##
    livenessProbe:
      enabled: true
      initialDelaySeconds: 30
      periodSeconds: 20
      timeoutSeconds: 10
      failureThreshold: 6
      successThreshold: 1
    ## MongoDB(&reg;) Hidden pods' readiness probe. Evaluated as a template.
    ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes
    ## @param hidden.readinessProbe.enabled Enable readinessProbe
    ## @param hidden.readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe
    ## @param hidden.readinessProbe.periodSeconds Period seconds for readinessProbe
    ## @param hidden.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
    ## @param hidden.readinessProbe.failureThreshold Failure threshold for readinessProbe
    ## @param hidden.readinessProbe.successThreshold Success threshold for readinessProbe
    ##
    readinessProbe:
      enabled: true
      initialDelaySeconds: 5
      periodSeconds: 20
      timeoutSeconds: 10
      failureThreshold: 6
      successThreshold: 1
    ## Slow starting containers can be protected through startup probes
    ## Startup probes are available in Kubernetes version 1.16 and above
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#define-startup-probes
    ## @param hidden.startupProbe.enabled Enable startupProbe
    ## @param hidden.startupProbe.initialDelaySeconds Initial delay seconds for startupProbe
    ## @param hidden.startupProbe.periodSeconds Period seconds for startupProbe
    ## @param hidden.startupProbe.timeoutSeconds Timeout seconds for startupProbe
    ## @param hidden.startupProbe.failureThreshold Failure threshold for startupProbe
    ## @param hidden.startupProbe.successThreshold Success threshold for startupProbe
    ##
    startupProbe:
      enabled: false
      initialDelaySeconds: 5
      periodSeconds: 10
      timeoutSeconds: 5
      successThreshold: 1
      failureThreshold: 30
    ## @param hidden.customLivenessProbe Override default liveness probe for hidden node containers
    ## Ignored when hidden.livenessProbe.enabled=true
    ##
    customLivenessProbe: {}
    ## @param hidden.customReadinessProbe Override default readiness probe for hidden node containers
    ## Ignored when hidden.readinessProbe.enabled=true
    ##
    customReadinessProbe: {}
    ## @param hidden.customStartupProbe Override default startup probe for MongoDB(&reg;) containers
    ## Ignored when hidden.startupProbe.enabled=true
    ##
    customStartupProbe: {}
    ## @param hidden.initContainers Add init containers to the MongoDB(&reg;) Hidden pods.
    ## Example:
    ## initContainers:
    ##   - name: your-image-name
    ##     image: your-image
    ##     imagePullPolicy: Always
    ##     ports:
    ##       - name: portname
    ##         containerPort: 1234
    ##
    initContainers: []
    ## @param hidden.sidecars Add additional sidecar containers for the hidden node pod(s)
    ## Example:
    ## sidecars:
    ##   - name: your-image-name
    ##     image: your-image
    ##     imagePullPolicy: Always
    ##     ports:
    ##       - name: portname
    ##         containerPort: 1234
    ##
    sidecars: []
    ## @param hidden.extraVolumeMounts Optionally specify extra list of additional volumeMounts for the hidden node container(s)
    ## Examples:
    ## extraVolumeMounts:
    ##   - name: extras
    ##     mountPath: /usr/share/extras
    ##     readOnly: true
    ##
    extraVolumeMounts: []
    ## @param hidden.extraVolumes Optionally specify extra list of additional volumes to the hidden node statefulset
    ## extraVolumes:
    ##   - name: extras
    ##     emptyDir: {}
    ##
    extraVolumes: []
    ## MongoDB(&reg;) Hidden Pod Disruption Budget configuration
    ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/
    ##
    pdb:
      ## @param hidden.pdb.create Enable/disable a Pod Disruption Budget creation for hidden node pod(s)
      ##
      create: true
      ## @param hidden.pdb.minAvailable Minimum number/percentage of hidden node pods that should remain scheduled
      ##
      minAvailable: ""
      ## @param hidden.pdb.maxUnavailable Maximum number/percentage of hidden node pods that may be made unavailable. Defaults to `1` if both `hidden.pdb.minAvailable` and `hidden.pdb.maxUnavailable` are empty.
      ##
      maxUnavailable: ""
    ## Enable persistence using Persistent Volume Claims
    ## ref: https://kubernetes.io/docs/concepts/storage/persistent-volumes/
    ##
    persistence:
      ## @param hidden.persistence.enabled Enable hidden node data persistence using PVC
      ##
      enabled: true
      ## @param hidden.persistence.medium Provide a medium for `emptyDir` volumes.
      ## Requires hidden.persistence.enabled: false
      ##
      medium: ""
      ## @param hidden.persistence.storageClass PVC Storage Class for hidden node data volume
      ## If defined, storageClassName: <storageClass>
      ## If set to "-", storageClassName: "", which disables dynamic provisioning
      ## If undefined (the default) or set to null, no storageClassName spec is
      ## set, choosing the default provisioner.
      ##
      storageClass: ""
      ## @param hidden.persistence.accessModes PV Access Mode
      ##
      accessModes:
        - ReadWriteOnce
      ## @param hidden.persistence.size PVC Storage Request for hidden node data volume
      ##
      size: 8Gi
      ## @param hidden.persistence.annotations PVC annotations
      ##
      annotations: {}
      ## @param hidden.persistence.mountPath The path the volume will be mounted at, useful when using different MongoDB(&reg;) images.
      ##
      mountPath: /bitnami/mongodb
      ## @param hidden.persistence.subPath The subdirectory of the volume to mount to, useful in dev environments
      ## and one PV for multiple services.
      ##
      subPath: ""
      ## Fine tuning for volumeClaimTemplates
      ##
      volumeClaimTemplates:
        ## @param hidden.persistence.volumeClaimTemplates.selector A label query over volumes to consider for binding (e.g. when using local volumes)
        ## See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.20/#labelselector-v1-meta for more details
        ##
        selector: {}
        ## @param hidden.persistence.volumeClaimTemplates.requests Custom PVC requests attributes
        ## Sometime cloud providers use additional requests attributes to provision custom storage instance
        ## See https://cloud.ibm.com/docs/containers?topic=containers-file_storage#file_dynamic_statefulset
        ##
        requests: {}
        ## @param hidden.persistence.volumeClaimTemplates.dataSource Set volumeClaimTemplate dataSource
        ##
        dataSource: {}
    service:
      ## @param hidden.service.portName MongoDB(&reg;) service port name
      ##
      portName: "mongodb"
      ## @param hidden.service.ports.mongodb MongoDB(&reg;) service port
      ##
      ports:
        mongodb: 27017
      ## @param hidden.service.extraPorts Extra ports to expose (normally used with the `sidecar` value)
      ##
      extraPorts: []
      ## @param hidden.service.annotations Provide any additional annotations that may be required
      ##
      annotations: {}
      ## Headless service properties
      ##
      headless:
        ## @param hidden.service.headless.annotations Annotations for the headless service.
        ##
        annotations: {}
  ## @section Metrics parameters
  ##
  metrics:
    ## @param metrics.enabled Enable using a sidecar Prometheus exporter
    ##
    enabled: false
    ## Bitnami MongoDB(&reg;) Promtheus Exporter image
    ## ref: https://hub.docker.com/r/bitnami/mongodb-exporter/tags/
    ## @param metrics.image.registry [default: REGISTRY_NAME] MongoDB(&reg;) Prometheus exporter image registry
    ## @param metrics.image.repository [default: REPOSITORY_NAME/mongodb-exporter] MongoDB(&reg;) Prometheus exporter image repository
    ## @skip metrics.image.tag MongoDB(&reg;) Prometheus exporter image tag (immutable tags are recommended)
    ## @param metrics.image.digest MongoDB(&reg;) image digest in the way sha256:aa.... Please note this parameter, if set, will override the tag
    ## @param metrics.image.pullPolicy MongoDB(&reg;) Prometheus exporter image pull policy
    ## @param metrics.image.pullSecrets Specify docker-registry secret names as an array
    ##
    image:
      registry: docker.io
      repository: bitnami/mongodb-exporter
      tag: 0.40.0-debian-12-r30
      digest: ""
      pullPolicy: IfNotPresent
      ## Optionally specify an array of imagePullSecrets.
      ## Secrets must be manually created in the namespace.
      ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
      ## e.g:
      ## pullSecrets:
      ##   - myRegistryKeySecretName
      ##
      pullSecrets: []
    ## @param metrics.username String with username for the metrics exporter
    ## If undefined the root user will be used for the metrics exporter
    ##
    username: ""
    ## @param metrics.password String with password for the metrics exporter
    ## If undefined but metrics.username is defined, a random password will be generated
    ##
    password: ""
    ## @param metrics.compatibleMode Enables old style mongodb-exporter metrics
    compatibleMode: true
    collector:
      ## @param metrics.collector.all Enable all collectors. Same as enabling all individual metrics
      ## Enabling all metrics will cause significant CPU load on mongod
      all: false
      ## @param metrics.collector.diagnosticdata Boolean Enable collecting metrics from getDiagnosticData
      diagnosticdata: true
      ## @param metrics.collector.replicasetstatus Boolean Enable collecting metrics from replSetGetStatus
      replicasetstatus: true
      ## @param metrics.collector.dbstats Boolean Enable collecting metrics from dbStats
      dbstats: false
      ## @param metrics.collector.topmetrics Boolean Enable collecting metrics from top admin command
      topmetrics: false
      ## @param metrics.collector.indexstats Boolean Enable collecting metrics from $indexStats
      indexstats: false
      ## @param metrics.collector.collstats Boolean Enable collecting metrics from $collStats
      collstats: false
      ## @param metrics.collector.collstatsColls List of \<databases\>.\<collections\> to get $collStats
      collstatsColls: []
      ## @param metrics.collector.indexstatsColls List - List of \<databases\>.\<collections\> to get $indexStats
      indexstatsColls: []
      ## @param metrics.collector.collstatsLimit Number - Disable collstats, dbstats, topmetrics and indexstats collector if there are more than \<n\> collections. 0=No limit
      collstatsLimit: 0
    ## @param metrics.extraFlags String with extra flags to the metrics exporter
    ## ref: https://github.com/percona/mongodb_exporter/blob/main/main.go
    ##
    extraFlags: ""
    ## Command and args for running the container (set to default if not set). Use array form
    ## @param metrics.command Override default container command (useful when using custom images)
    ## @param metrics.args Override default container args (useful when using custom images)
    ##
    command: []
    args: []
    ## Metrics exporter container resource requests and limits
    ## ref: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
    ## We usually recommend not to specify default resources and to leave this as a conscious
    ## choice for the user. This also increases chances charts run on environments with little
    ## resources, such as Minikube. If you do want to specify resources, uncomment the following
    ## lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    ## @param metrics.resourcesPreset Set container resources according to one common preset (allowed values: none, nano, micro, small, medium, large, xlarge, 2xlarge). This is ignored if metrics.resources is set (metrics.resources is recommended for production).
    ## More information: https://github.com/bitnami/charts/blob/main/bitnami/common/templates/_resources.tpl#L15
    ##
    resourcesPreset: "nano"
    ## @param metrics.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
    ## Example:
    ## resources:
    ##   requests:
    ##     cpu: 2
    ##     memory: 512Mi
    ##   limits:
    ##     cpu: 3
    ##     memory: 1024Mi
    ##
    resources: {}
    ## @param metrics.containerPort Port of the Prometheus metrics container
    ##
    containerPort: 9216
    ## Prometheus Exporter service configuration
    ##
    service:
      ## @param metrics.service.annotations [object] Annotations for Prometheus Exporter pods. Evaluated as a template.
      ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
      ##
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "{{ .Values.metrics.service.ports.metrics }}"
        prometheus.io/path: "/metrics"
      ## @param metrics.service.type Type of the Prometheus metrics service
      ##
      type: ClusterIP
      ## @param metrics.service.ports.metrics Port of the Prometheus metrics service
      ##
      ports:
        metrics: 9216
      ## @param metrics.service.extraPorts Extra ports to expose (normally used with the `sidecar` value)
      ##
      extraPorts: []
    ## Metrics exporter liveness probe
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes)
    ## @param metrics.livenessProbe.enabled Enable livenessProbe
    ## @param metrics.livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe
    ## @param metrics.livenessProbe.periodSeconds Period seconds for livenessProbe
    ## @param metrics.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
    ## @param metrics.livenessProbe.failureThreshold Failure threshold for livenessProbe
    ## @param metrics.livenessProbe.successThreshold Success threshold for livenessProbe
    ##
    livenessProbe:
      enabled: true
      initialDelaySeconds: 15
      periodSeconds: 5
      timeoutSeconds: 10
      failureThreshold: 3
      successThreshold: 1
    ## Metrics exporter readiness probe
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes)
    ## @param metrics.readinessProbe.enabled Enable readinessProbe
    ## @param metrics.readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe
    ## @param metrics.readinessProbe.periodSeconds Period seconds for readinessProbe
    ## @param metrics.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
    ## @param metrics.readinessProbe.failureThreshold Failure threshold for readinessProbe
    ## @param metrics.readinessProbe.successThreshold Success threshold for readinessProbe
    ##
    readinessProbe:
      enabled: true
      initialDelaySeconds: 5
      periodSeconds: 5
      timeoutSeconds: 10
      failureThreshold: 3
      successThreshold: 1
    ## Slow starting containers can be protected through startup probes
    ## Startup probes are available in Kubernetes version 1.16 and above
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#define-startup-probes
    ## @param metrics.startupProbe.enabled Enable startupProbe
    ## @param metrics.startupProbe.initialDelaySeconds Initial delay seconds for startupProbe
    ## @param metrics.startupProbe.periodSeconds Period seconds for startupProbe
    ## @param metrics.startupProbe.timeoutSeconds Timeout seconds for startupProbe
    ## @param metrics.startupProbe.failureThreshold Failure threshold for startupProbe
    ## @param metrics.startupProbe.successThreshold Success threshold for startupProbe
    ##
    startupProbe:
      enabled: false
      initialDelaySeconds: 5
      periodSeconds: 10
      timeoutSeconds: 5
      successThreshold: 1
      failureThreshold: 30
    ## @param metrics.customLivenessProbe Override default liveness probe for MongoDB(&reg;) containers
    ## Ignored when livenessProbe.enabled=true
    ##
    customLivenessProbe: {}
    ## @param metrics.customReadinessProbe Override default readiness probe for MongoDB(&reg;) containers
    ## Ignored when readinessProbe.enabled=true
    ##
    customReadinessProbe: {}
    ## @param metrics.customStartupProbe Override default startup probe for MongoDB(&reg;) containers
    ## Ignored when startupProbe.enabled=true
    ##
    customStartupProbe: {}
    ## @param metrics.extraVolumeMounts Optionally specify extra list of additional volumeMounts for the metrics container(s)
    ## Examples:
    ## extraVolumeMounts:
    ##   - name: extras
    ##     mountPath: /usr/share/extras
    ##     readOnly: true
    ##
    extraVolumeMounts: []
    ## Prometheus Service Monitor
    ## ref: https://github.com/coreos/prometheus-operator
    ##      https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md
    ##
    serviceMonitor:
      ## @param metrics.serviceMonitor.enabled Create ServiceMonitor Resource for scraping metrics using Prometheus Operator
      ##
      enabled: false
      ## @param metrics.serviceMonitor.namespace Namespace which Prometheus is running in
      ##
      namespace: ""
      ## @param metrics.serviceMonitor.interval Interval at which metrics should be scraped
      ##
      interval: 30s
      ## @param metrics.serviceMonitor.scrapeTimeout Specify the timeout after which the scrape is ended
      ## e.g:
      ## scrapeTimeout: 30s
      ##
      scrapeTimeout: ""
      ## @param metrics.serviceMonitor.relabelings RelabelConfigs to apply to samples before scraping.
      ##
      relabelings: []
      ## @param metrics.serviceMonitor.metricRelabelings MetricsRelabelConfigs to apply to samples before ingestion.
      ##
      metricRelabelings: []
      ## @param metrics.serviceMonitor.labels Used to pass Labels that are used by the Prometheus installed in your cluster to select Service Monitors to work with
      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#prometheusspec
      ##
      labels: {}
      ## @param metrics.serviceMonitor.selector Prometheus instance selector labels
      ## ref: https://github.com/bitnami/charts/tree/main/bitnami/prometheus-operator#prometheus-configuration
      ##
      selector: {}
      ## @param metrics.serviceMonitor.honorLabels Specify honorLabels parameter to add the scrape endpoint
      ##
      honorLabels: false
      ## @param metrics.serviceMonitor.jobLabel The name of the label on the target service to use as the job name in prometheus.
      ##
      jobLabel: ""
    ## Custom PrometheusRule to be defined
    ## ref: https://github.com/coreos/prometheus-operator#customresourcedefinitions
    ##
    prometheusRule:
      ## @param metrics.prometheusRule.enabled Set this to true to create prometheusRules for Prometheus operator
      ##
      enabled: false
      ## @param metrics.prometheusRule.additionalLabels Additional labels that can be used so prometheusRules will be discovered by Prometheus
      ##
      additionalLabels: {}
      ## @param metrics.prometheusRule.namespace Namespace where prometheusRules resource should be created
      ##
      namespace: ""
      ## @param metrics.prometheusRule.rules Rules to be created, check values for an example
      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#rulegroup
      ##      https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/
      ##
      ## This is an example of a rule, you should add the below code block under the "rules" param, removing the brackets
      ## rules:
      ## - alert: HighRequestLatency
      ##   expr: job:request_latency_seconds:mean5m{job="myjob"} > 0.5
      ##   for: 10m
      ##   labels:
      ##     severity: page
      ##   annotations:
      ##     summary: High request latency
      ##
      rules: []
